{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NFL Competition\n",
    "\n",
    "# Feature Engineering e Modelo de Machine Learning\n",
    "\n",
    "- Version: 1.0: usando padrao do fork: https://www.kaggle.com/bestpredict/location-eda-8eb410\n",
    "        *    Resultado: 0.012744\n",
    "        *    LB: 0.01363\n",
    "   \n",
    "- Version: 2.0: adicionado Feature Selection com LOFO Importance\n",
    "        *    Resultado: 0.012780\n",
    "        *    LB: 0.01365\n",
    "\n",
    "- Version: 3.0: adicionado novas features (apenas feature fxx + old_data + YardLine_std)\n",
    "        *    Resultado: 0.012614\n",
    "        *    LB: 0.01361\n",
    "        \n",
    "- Version: 4.0: adicionado novas features (turf)\n",
    "        *    Resultado: 0.012624\n",
    "        *    LB: 0.01361\n",
    "        \n",
    "- Version: 5.0: adicionado novas features (game_time)\n",
    "        *    Resultado: 0.012635\n",
    "        *    LB: 0.01362\n",
    "        \n",
    "- Version: 6.0: adicionado novas features (feat1, feat2, feat3, feat4) e removido (Turf + game_time)\n",
    "        *    Resultado: 0.012536\n",
    "        *    LB: Não é permitido\n",
    "        \n",
    "- Version: 7.0: alteração do modelo de bagging\n",
    "        *    Resultado: 0.012474\n",
    "        *    LB: 0.01362\n",
    "\n",
    "- Version: 8.0: adicionado novas features (accY, est_prev_yards)\n",
    "        *    Resultado: 0.012616\n",
    "        *    LB: 0.01362\n",
    "        \n",
    "- Version: 9.0: adicionado novas features (norm_quat,mod_quat,norm_X,norm_Y,norm_A,norm_S)\n",
    "    \n",
    "        *    Resultado: 0.012604\n",
    "        *    LB: 0.01361\n",
    "\n",
    "- Version: 12.0: realizando stacking dos modelos NN e RF\n",
    "    \n",
    "        *    Resultado: 0.012616\n",
    "        *    LB: 0.01361\n",
    "        \n",
    "- Version: 13.0: inclusao de otimização no modelo de NN\n",
    "    \n",
    "        *    Resultado: 0.012587\n",
    "        *    LB: 0.01361\n",
    "        \n",
    "- Version: 14.0: usando todos os dados do dataset para treino\n",
    "    \n",
    "        *    Resultado: 0.009x\n",
    "        *    LB: 0.01379\n",
    "        \n",
    "- Version: 15.0: adicionando novas features (seconds_need_to_first_down, seconds_need_to_YardsLine, DefendersInTheBox_vs_Distance) e alguns ajustes no modelo\n",
    "    \n",
    "        *    Resultado: 0.012646\n",
    "        *    LB: 0.01361\n",
    "        \n",
    "- Version: 16.1: fazendo blending de NN com LGB e adicionando features da Daniela\n",
    "    \n",
    "        *    Resultado: 0.12614\n",
    "        *    LB:         \n",
    "  \n",
    "- Version: 16.2: removendo alguns outliers Yards (frequencia < 5)\n",
    "    \n",
    "        *    Resultado: 0.011860\n",
    "        *    LB:       \n",
    "        \n",
    "- Version: 16.3: removendo alguns outliers Yards (frequencia <= 1)\n",
    "    \n",
    "        *    Resultado: 0.012417\n",
    "        *    LB:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importa os pacotes e o dataset de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Importar os principais pacotes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "import codecs\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "# Evitar que aparece os warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Seta algumas opções no Jupyter para exibição dos datasets\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# Variavel para controlar o treinamento no Kaggle\n",
    "TRAIN_OFFLINE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os pacotes de algoritmos\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Importa os pacotes de algoritmos de redes neurais (Keras)\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda,BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import Callback,EarlyStopping,ModelCheckpoint\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "\n",
    "# Importa pacotes do sklearn\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics as mtr\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, StandardScaler\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_OFFLINE:\n",
    "    train  = pd.read_csv('data/train.csv', dtype={'WindSpeed': 'object'})\n",
    "else:\n",
    "    train  = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = train[['GameId','PlayId','Yards']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strtoseconds(txt):\n",
    "    txt = txt.split(':')\n",
    "    ans = int(txt[0])*60 + int(txt[1]) + int(txt[2])/60\n",
    "    return ans\n",
    "\n",
    "def strtofloat(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "def get_time(x):\n",
    "    x = x.split(\":\")\n",
    "    return int(x[0])*60 + int(x[1])\n",
    "\n",
    "def map_weather(txt):\n",
    "    ans = 1\n",
    "    if pd.isna(txt):\n",
    "        return 0\n",
    "    if 'partly' in txt:\n",
    "        ans*=0.5\n",
    "    if 'climate controlled' in txt or 'indoor' in txt:\n",
    "        return ans*3\n",
    "    if 'sunny' in txt or 'sun' in txt:\n",
    "        return ans*2\n",
    "    if 'clear' in txt:\n",
    "        return ans\n",
    "    if 'cloudy' in txt:\n",
    "        return -ans\n",
    "    if 'rain' in txt or 'rainy' in txt:\n",
    "        return -2*ans\n",
    "    if 'snow' in txt:\n",
    "        return -3*ans\n",
    "    return 0\n",
    "\n",
    "def OffensePersonnelSplit(x):\n",
    "    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0, 'QB' : 0, 'RB' : 0, 'TE' : 0, 'WR' : 0}\n",
    "    for xx in x.split(\",\"):\n",
    "        xxs = xx.split(\" \")\n",
    "        dic[xxs[-1]] = int(xxs[-2])\n",
    "    return dic\n",
    "\n",
    "def DefensePersonnelSplit(x):\n",
    "    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0}\n",
    "    for xx in x.split(\",\"):\n",
    "        xxs = xx.split(\" \")\n",
    "        dic[xxs[-1]] = int(xxs[-2])\n",
    "    return dic\n",
    "\n",
    "def orientation_to_cat(x):\n",
    "    x = np.clip(x, 0, 360 - 1)\n",
    "    try:\n",
    "        return str(int(x/15))\n",
    "    except:\n",
    "        return \"nan\"    \n",
    "    \n",
    "def uid_aggregation(comb, main_columns, uids, aggregations):\n",
    "    X = pd.DataFrame()\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_'+agg_type\n",
    "                temp_df = comb[[col, main_column]]\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                X[new_col_name] = comb[col].map(temp_df)\n",
    "                del temp_df\n",
    "                gc.collect()\n",
    "    return X\n",
    "\n",
    "def transform_time_quarter(str1):\n",
    "    return int(str1[:2])*60 + int(str1[3:5])\n",
    "\n",
    "def transform_time_all(str1,quarter):\n",
    "    if quarter<=4:\n",
    "        return 15*60 - (int(str1[:2])*60 + int(str1[3:5])) + (quarter-1)*15*60\n",
    "    if quarter ==5:\n",
    "        return 10*60 - (int(str1[:2])*60 + int(str1[3:5])) + (quarter-1)*15*60\n",
    "    \n",
    "def new_coord_X(x_coordinate, play_direction):\n",
    "    if play_direction == 'left':\n",
    "        return 120.0 - x_coordinate\n",
    "    else:\n",
    "        return x_coordinate\n",
    "\n",
    "def new_line(rush_team, field_position, yardline):\n",
    "    if rush_team == field_position:\n",
    "        # offense starting at X = 0 plus the 10 yard endzone plus the line of scrimmage\n",
    "        return 10.0 + yardline\n",
    "    else:\n",
    "        # half the field plus the yards between midfield and the line of scrimmage\n",
    "        return 60.0 + (50 - yardline)\n",
    "\n",
    "def new_orientation(angle, play_direction):\n",
    "    if play_direction == 'left':\n",
    "        new_angle = 360.0 - angle\n",
    "        if new_angle == 360.0:\n",
    "            new_angle = 0.0\n",
    "        return new_angle\n",
    "    else:\n",
    "        return angle\n",
    "\n",
    "def euclidean_distance(x1,y1,x2,y2):\n",
    "    x_diff = (x1-x2)**2\n",
    "    y_diff = (y1-y2)**2\n",
    "    return np.sqrt(x_diff + y_diff)\n",
    "\n",
    "def back_direction(orientation):\n",
    "    if orientation > 180.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def velocity(x2, x1, sec) :\n",
    "    return (x2 - x1) / sec\n",
    "\n",
    "def diff_x(b, c, theta) :\n",
    "    if 90.0 < theta < 270.0 :\n",
    "        return np.sqrt(((b ** 2) + (c ** 2)) - 2 * b * c * np.cos(theta))\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "def diff_y(b, c, theta) :\n",
    "    if theta <= 90.0 and theta >= 270.0 :\n",
    "        return - np.sqrt(((b ** 2) + (c ** 2)) - 2 * b * c * np.cos(theta))\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "def stop_period(speed, acc) :\n",
    "    return speed / acc   \n",
    "\n",
    "def new_roll_velocity(x1, y1, x2, y2) :  \n",
    "    x_diff = np.sqrt((x1 - x2) ** 2)\n",
    "    y_diff = np.sqrt((y1 - y2) ** 2)\n",
    "    return np.sqrt(x_diff + y_diff) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_centroid(tmp):\n",
    "    '''\n",
    "    Calculates centroid coordinates of offense and defense teams\n",
    "    Takes into account rusher coordinates\n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for col in tmp.columns:\n",
    "        if col in ['X','Y']:\n",
    "            df[col+'_mean']= tmp.groupby(['GameId','PlayId','IsOnOffense'])[col].mean()\n",
    "            df[col+'_std']= tmp.groupby(['GameId','PlayId','IsOnOffense'])[col].std()\n",
    "    temp1= df.unstack()\n",
    "    temp1.columns = ['Xc_def_mean','Xc_off_mean', 'Xc_def_std','Xc_off_std',\n",
    "                    'Yc_def_mean','Yc_off_mean','Yc_def_std','Yc_off_std']\n",
    "    #col = ['Xc_def_std','Yc_def_std','Xc_off_std','Yc_off_std']\n",
    "    #temp1.drop(col, axis=1,inplace=True)\n",
    "    \n",
    "    temp2 = tmp.loc[tmp['IsRusher'] == True, ['GameId','PlayId','X','Y','Sx','Sy','Ax','Ay','F','Fx','Fy','p','px','py','pf_max','px_max','py_max']]\n",
    "    temp2.columns = ['GameId','PlayId','X_rusher','Y_rusher','Sx_rusher','Sy_rusher','Ax_rusher','Ay_rusher','F_rusher','Fx_rusher','Fy_rusher','p_rusher','px_rusher','py_rusher','p_max_rusher','px_max_rusher','py_max_rusher']\n",
    "    temp1 = temp1.merge(temp2, on=['GameId','PlayId'], how='left')\n",
    "    new_tmp = tmp.merge(temp1, on=['GameId','PlayId'])\n",
    "\n",
    "    return new_tmp\n",
    "\n",
    "def add_x_y_components(df):\n",
    "    '''\n",
    "    Splits S and A into their x and y components\n",
    "    Calculates max speed at given acceleration\n",
    "    Calculates F and p, absolute and relatives\n",
    "    '''\n",
    "    df['S_'] = df['S'].apply(convert_yard_to_m)\n",
    "    df['A_'] = df['A'].apply(convert_yard_to_m)\n",
    "    \n",
    "    df['Sx'] = df['S']*np.cos(df['Dir'])\n",
    "    \n",
    "    df['Sy'] = df['S']*np.sin(df['Dir'])\n",
    "    df['Sy_'] = np.abs(df['S']*np.sin(df['Dir'])) \n",
    "    \n",
    "    df['Ax'] = df['A']*np.cos(df['Dir'])\n",
    "    df['Ay'] = df['A']*np.sin(df['Dir'])\n",
    "    \n",
    "    df['Sfx'] = df['Sx']+df['Ax']*3.5\n",
    "    df['Sfy'] = df['Sy']+df['Ay']*3.5\n",
    "    \n",
    "    df['F'] = df['PlayerWeight_kg']*df['A_']\n",
    "    df['Fx'] = df['F']*np.cos(df['Dir'])\n",
    "    df['Fy'] = df['F']*np.sin(df['Dir'])\n",
    "\n",
    "    df['p'] = df['PlayerWeight_kg']*df['S_']\n",
    "    df['px'] = df['p']*np.cos(df['Dir'])\n",
    "    df['py'] = df['p']*np.sin(df['Dir'])\n",
    "    \n",
    "    df['pf_max'] = df['PlayerWeight_kg']*(np.sqrt(df['Sfx']**2+df['Sfy']**2))\n",
    "    #df['pf_max'] = df['PlayerWeight']*9\n",
    "    \n",
    "    df['px_max'] = df['pf_max']*np.cos(df['Dir'])\n",
    "    df['py_max'] = df['pf_max']*np.sin(df['Dir'])\n",
    "    \n",
    "    df['KE'] = (1/2) * df['PlayerWeight_kg'] * (df['S_'] ** 2)\n",
    "\n",
    "    \n",
    "    del df['A_']\n",
    "    del df['S_']\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def l2_norm_mat(mat_v, mat_u):\n",
    "    '''\n",
    "    Calculates euclidean distance\n",
    "    '''\n",
    "    return np.linalg.norm(mat_v - mat_u, axis=1)    \n",
    "\n",
    "def calculate_distance(tmp): \n",
    "    '''\n",
    "    Calculates euclidean distance for below components\n",
    "    '''\n",
    "\n",
    "    mat_u = np.array(tmp[['X','Y']])\n",
    "    mat_v = np.array(tmp[['Xc_def_mean','Yc_def_mean']])\n",
    "    mat_w = np.array(tmp[['Xc_off_mean','Yc_off_mean']])\n",
    "    mat_z = np.array(tmp[['X_rusher','Y_rusher']])\n",
    "    \n",
    "    tmp['dist_cdef_vs_coff'] = l2_norm_mat(mat_v, mat_w)\n",
    "    tmp['dist_cdef_vs_player'] = l2_norm_mat(mat_u, mat_v)\n",
    "    tmp['dist_coff_vs_player'] = l2_norm_mat(mat_u, mat_w)\n",
    "    tmp['dist_cdef_vs_rusher'] = l2_norm_mat(mat_z, mat_v)\n",
    "    tmp['dist_coff_vs_rusher'] = l2_norm_mat(mat_z, mat_w)\n",
    "    tmp['dist_player_vs_rusher'] = l2_norm_mat(mat_u, mat_z)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "def add_rushertimeto1stdown(data):\n",
    "    '''\n",
    "    Calculates time needed for 1st down (based on Distance)\n",
    "    '''\n",
    "    tmp = data.copy()\n",
    "    tmp = tmp.loc[tmp['IsRusher']==True,['GameId','PlayId','X','Sx_rusher','Ax_rusher','Dis','Distance','YardLine']]\n",
    "    # below corrected\n",
    "    tmp['S0x_rusher'] = np.sqrt(np.square(tmp['Sx_rusher'])-2*(tmp['Ax_rusher'])*(tmp['Dis']))\n",
    "    tmp['S0x_rusher'].fillna(np.sqrt(-np.square(tmp['Sx_rusher'])-2*(-tmp['Ax_rusher'])*(tmp['Dis'])), inplace = True)\n",
    "    tmp['rusher_dist_to_yardline'] = tmp['YardLine']-tmp['X']\n",
    "    tmp['rusher_dist_to_1stdown'] = tmp['rusher_dist_to_yardline'] + tmp['Distance']\n",
    "    # tmp['Disc_1'] = (tmp['Sx_rusher'])**2-(4*(tmp['Ax_rusher']/2)*(-tmp['Distance']))\n",
    "    tmp['Disc_1'] = (tmp['S0x_rusher'])**2-(4*(tmp['Ax_rusher']/2)*(-tmp['rusher_dist_to_1stdown']))\n",
    "    tmp['t_1stDown'] = (-tmp['S0x_rusher'] + np.sqrt(tmp['Disc_1'])/(tmp['Ax_rusher']))\n",
    "    S_max_rusher = tmp.loc[tmp['Sx_rusher']>0,'Sx_rusher'].median()\n",
    "    tmp['t_1stDown'].fillna(2*tmp['rusher_dist_to_1stdown']/S_max_rusher, inplace=True)\n",
    "    tmp.loc[np.isneginf(tmp['t_1stDown'])==True,['t_1stDown']] = -1.0\n",
    "    tmp.loc[np.isposinf(tmp['t_1stDown'])==True,['t_1stDown']] = -1.0\n",
    "\n",
    "\n",
    "    col = ['Sx_rusher','Ax_rusher','Dis','Distance','Disc_1','X','YardLine','rusher_dist_to_yardline']\n",
    "    tmp.drop(col,axis= 1, inplace= True)\n",
    "    \n",
    "    data = data.merge(tmp, on=['GameId','PlayId'], how='left')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def calc_timetotackle(tmp): # it doesnt get affected \n",
    "    '''\n",
    "    Calculates time needed to tackle rusher\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    tmp = tmp.loc[tmp['IsOnOffense']==False,['GameId','PlayId','Sx','Sx_rusher','X','X_rusher','Ax','Ax_rusher','Sy','Sy_rusher','Y','Y_rusher','Ay','Ay_rusher','IsOnOffense']]\n",
    "    tmp['Dx'] = np.square(tmp['Sx']-tmp['Sx_rusher'])-4*(tmp['X']-tmp['X_rusher'])*(tmp['Ax']-tmp['Ax_rusher'])/2 \n",
    "    tmp['Dy'] = np.square(tmp['Sy']-tmp['Sy_rusher'])-4*(tmp['Y']-tmp['Y_rusher'])*(tmp['Ay']-tmp['Ay_rusher'])/2 \n",
    "\n",
    "\n",
    "    #tmp['t1'] = (-(tmp['Sx']-tmp['Sx_rusher'])+np.sqrt(tmp['D']))/(2*(tmp['Ax']-tmp['Ax_rusher'])/2)\n",
    "    tmp['tx'] = (-(tmp['Sx']-tmp['Sx_rusher'])-np.sqrt(tmp['Dx']))/(2*(tmp['Ax']-tmp['Ax_rusher'])/2)\n",
    "    tmp['ty'] = (-(tmp['Sy']-tmp['Sy_rusher'])-np.sqrt(tmp['Dy']))/(2*(tmp['Ay']-tmp['Ay_rusher'])/2)\n",
    "    \n",
    "    tmp['tx'].fillna((-(tmp['Sx']-tmp['Sx_rusher']))/(2*(tmp['Ax']-tmp['Ax_rusher'])/2), inplace = True)\n",
    "    tmp['ty'].fillna((-(tmp['Sy']-tmp['Sy_rusher']))/(2*(tmp['Ay']-tmp['Ay_rusher'])/2), inplace = True)\n",
    "    \n",
    "    tmp['time_to_tackle'] = np.sqrt(np.square(tmp['tx']) + np.square(tmp['ty']))\n",
    "    \n",
    "    for col in tmp.columns:\n",
    "        if col in ['time_to_tackle']:\n",
    "            df[col+'_mean']= tmp.groupby(['GameId','PlayId','IsOnOffense'])[col].mean()\n",
    "            df[col+'_min']= tmp.groupby(['GameId','PlayId','IsOnOffense'])[col].min()\n",
    "            df[col+'_max']= tmp.groupby(['GameId','PlayId','IsOnOffense'])[col].max()\n",
    "            \n",
    "    tmp2 = df.unstack()\n",
    "    tmp2.columns = ['time_to_tackle_mean','time_to_tackle_min','time_to_tackle_max']#,'time_to_tackle_cnt']\n",
    "    \n",
    "    tmp2.loc[np.isposinf(tmp2['time_to_tackle_max'])==True,['time_to_tackle_mean','time_to_tackle_max']] = tmp2['time_to_tackle_min']\n",
    "    tmp2.loc[np.isposinf(tmp2['time_to_tackle_min'])==True,['time_to_tackle_min']] = 40\n",
    "    tmp2.loc[np.isposinf(tmp2['time_to_tackle_max'])==True,['time_to_tackle_max']] = 40\n",
    "    tmp2.loc[np.isposinf(tmp2['time_to_tackle_mean'])==True,['time_to_tackle_mean']] = 40\n",
    "    \n",
    "    return tmp2\n",
    "\n",
    "def add_distance_ratios(data, encoder, deploy=False):\n",
    "    \n",
    "    tmp = data.copy()\n",
    "    \n",
    "    tmp1 = tmp.loc[:,['GameId','PlayId','IsOnOffense','IsRusher','dist_player_vs_rusher']]\n",
    "    tmp1['dist_from_rusher_cat'] = tmp1['dist_player_vs_rusher'].apply(lambda x: map_distance_from_rusher(x))\n",
    "\n",
    "    #enc = OneHotEncoder(handle_unknown='ignore', sparse = False)\n",
    "    enc = encoder\n",
    "    \n",
    "    if not deploy:\n",
    "        new_column = pd.DataFrame(enc.fit_transform(np.array(tmp1['dist_from_rusher_cat']).reshape(-1,1)))\n",
    "    else:\n",
    "        new_column = pd.DataFrame(enc.transform(np.array(tmp1['dist_from_rusher_cat']).reshape(-1,1)))  \n",
    "    \n",
    "    new_column.columns = ['D1','D2','D3','D4','D5','D6','D7']\n",
    "\n",
    "    tmp = pd.merge(tmp, new_column, left_index=True, right_index=True,how ='left')\n",
    "\n",
    "    tmp2 = tmp.loc[(tmp1['IsRusher']!=1)&(tmp['IsOnOffense']==1),['GameId','PlayId','NflId','IsOnOffense','dist_player_vs_rusher','dist_from_rusher_cat','D1','D2','D3','D4','D5','D6','D7']]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for col in ['D1','D2','D3','D4','D5','D6','D7']:\n",
    "        df[col+'_cnt_off'] = tmp2.groupby(['GameId','PlayId'])[col].sum()\n",
    "\n",
    "    tmp2 = tmp.loc[(tmp1['IsOnOffense']==0),['GameId','PlayId','NflId','IsOnOffense','dist_player_vs_rusher','dist_from_rusher_cat','D1','D2','D3','D4','D5','D6','D7']]\n",
    "\n",
    "    for col in ['D1','D2','D3','D4','D5','D6','D7']:\n",
    "        df[col+'_cnt_def'] = tmp2.groupby(['GameId','PlayId'])[col].sum()\n",
    "\n",
    "\n",
    "    weights = [2, 0.9,0.8,0.7,0.6,0.5,0.4]\n",
    "    df['dist_from_rusher_DEF'] = np.dot(df[['D1_cnt_def','D2_cnt_def','D3_cnt_def','D4_cnt_def','D5_cnt_def','D6_cnt_def','D7_cnt_def']],weights)/np.sum(weights)\n",
    "    df['dist_from_rusher_OFF'] = np.dot(df[['D1_cnt_off','D2_cnt_off','D3_cnt_off','D4_cnt_off','D5_cnt_off','D6_cnt_off','D7_cnt_off']],weights)/np.sum(weights)\n",
    "    df['dist_from_rusher_ratio'] = df['dist_from_rusher_OFF']/df['dist_from_rusher_DEF']\n",
    "\n",
    "    \n",
    "    cols = ['D1_cnt_def','D2_cnt_def','D3_cnt_def','D4_cnt_def','D5_cnt_def','D6_cnt_def','D7_cnt_def','D1_cnt_off','D2_cnt_off','D3_cnt_off','D4_cnt_off','D5_cnt_off','D6_cnt_off','D7_cnt_off']\n",
    "    df.drop(cols, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    data = pd.merge(data, df, on=['GameId','PlayId'], how='left')\n",
    "    \n",
    "    return data, enc\n",
    "\n",
    "# Funcao para converter peso em lbs para kg\n",
    "def convert_to_kg(lbs):\n",
    "    kg = lbs * 0.45359237\n",
    "    #print(\"The weight is\", kg, \"in kilograms\")\n",
    "    \n",
    "    return kg\n",
    "\n",
    "# Funcao para converter yard to m\n",
    "def convert_yard_to_m(yard):\n",
    "    m = yard * 0.9144    \n",
    "    return m\n",
    "\n",
    "def map_distance_from_rusher(distance):\n",
    "    if np.square(distance)<= np.square(1):\n",
    "        return '1'\n",
    "    elif np.square(distance)<= np.square(2):\n",
    "        return '2'\n",
    "    elif np.square(distance)<= np.square(3):\n",
    "        return '3'\n",
    "    elif np.square(distance)<= np.square(4):\n",
    "        return '4'\n",
    "    elif np.square(distance)<= np.square(5):\n",
    "        return '5'\n",
    "    elif np.square(distance)<= np.square(10):\n",
    "        return '6'\n",
    "    else:\n",
    "        return '7'\n",
    "\n",
    "def s_ratio_runner_vs_1stdef(data):\n",
    "    tmp = data.loc[((data['dist_player_vs_rusher'])==(data['dist_player_vs_rusher_min_NOnOff']))&(data['IsOnOffense']==False),['GameId','PlayId','Sx','Sx_rusher']]\n",
    "    tmp['rusher_1stdef_sx_ratio'] = tmp['Sx_rusher']/tmp['Sx']\n",
    "    drop = ['Sx','Sx_rusher']\n",
    "    tmp.drop(drop, axis = 1, inplace = True)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "def add_players_data(tmp):\n",
    "    '''\n",
    "    Calculates mean and minimum distances\n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for col in tmp.columns:\n",
    "        if col in ['dist_cdef_vs_player','dist_coff_vs_player','dist_player_vs_rusher']:\n",
    "            df[col+'_mean']= tmp.groupby(['GameId','PlayId','IsOnOffense'])[col].mean()\n",
    "            df[col+'_min']= tmp.groupby(['GameId','PlayId','IsOnOffense'])[col].min()\n",
    "            \n",
    "    temp1= df.unstack()\n",
    "    temp1.columns = ['dist_cdef_vs_player_mean_NOnOff','dist_cdef_vs_player_mean_OnOff','dist_cdef_vs_player_min_NOnOff','dist_cdef_vs_player_min_OnOff',\n",
    "    'dist_coff_vs_player_mean_NOnOff','dist_coff_vs_player_mean_OnOff','dist_coff_vs_player_min_NOnOff','dist_coff_vs_player_min_OnOff',\n",
    "    'dist_player_vs_rusher_mean_NOnOff','dist_player_vs_rusher_mean_OnOff','dist_player_vs_rusher_min_NOnOff','dist_player_vs_rusher_min_OnOff']\n",
    "    #tmp = tmp.merge(temp1, on=['GameId','PlayId'], how='left')\n",
    "\n",
    "    return temp1\n",
    "\n",
    "def clf_yards(x):\n",
    "\n",
    "    y = 2\n",
    "    if x<= -7:\n",
    "        y=0\n",
    "    elif x<=-2:\n",
    "        y=1\n",
    "    elif x<=0:\n",
    "        y=2\n",
    "    elif x<=2:\n",
    "        y=3\n",
    "    elif x<=5:\n",
    "        y=4\n",
    "    elif x<=10:\n",
    "        y=5\n",
    "    elif x<=20:\n",
    "        y=6\n",
    "    elif x<=40:\n",
    "        y=7\n",
    "    else:\n",
    "        y=8\n",
    "    return y\n",
    "\n",
    "def fill_defendersinabox(x, y): # x=DefensePersonnel y=Defenderinthebox\n",
    "    \n",
    "    if x =='4 DL, 2 LB, 5 DB':\n",
    "        y = 6\n",
    "    elif x =='4 DL, 3 LB, 4 DB':\n",
    "        y = 7\n",
    "    elif x == '3 DL, 4 LB, 4 DB':\n",
    "        y = 7\n",
    "    elif x == '2 DL, 4 LB, 5 DB':\n",
    "        y = 6\n",
    "    elif x == '3 DL, 3 LB, 5 DB':\n",
    "        y = 6\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def map_DefensePersonnel(x):\n",
    "\n",
    "    if x =='4 DL, 2 LB, 5 DB':\n",
    "        y = 0\n",
    "    elif x =='4 DL, 3 LB, 4 DB':\n",
    "        y = 1\n",
    "    elif x == '3 DL, 4 LB, 4 DB':\n",
    "        y = 2\n",
    "    elif x == '2 DL, 4 LB, 5 DB':\n",
    "        y = 3\n",
    "    elif x == '3 DL, 3 LB, 5 DB':\n",
    "        y = 4\n",
    "    else:\n",
    "        y = 5\n",
    "    return y\n",
    "\n",
    "map_offense_formation = {'SINGLEBACK':0,\n",
    "                         'SHOTGUN':1,\n",
    "                         'I_FORM':2,\n",
    "                         'PISTOL':3,\n",
    "                         'JUMBO':4,\n",
    "                         'WILDCAT':5,\n",
    "                         'ACE':6,\n",
    "                         'EMPTY':7,\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_01(df, deploy=False):\n",
    "\n",
    "    \n",
    "    def update_yardline(df):\n",
    "        new_yardline = df[df['NflId'] == df['NflIdRusher']]\n",
    "        new_yardline['YardLine'] = new_yardline[['PossessionTeam','FieldPosition','YardLine']].apply(lambda x: new_line(x[0],x[1],x[2]), axis=1)\n",
    "        new_yardline = new_yardline[['GameId','PlayId','YardLine']]\n",
    "        return new_yardline\n",
    "\n",
    "    def update_orientation(df, yardline):\n",
    "        df['X'] = df[['X','PlayDirection']].apply(lambda x: new_coord_X(x[0],x[1]), axis=1)\n",
    "        df['Orientation'] = df[['Orientation','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n",
    "        df['Dir'] = df[['Dir','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n",
    "        df = df.drop('YardLine', axis=1)\n",
    "        df = pd.merge(df, yardline, on=['GameId','PlayId'], how='inner')\n",
    "        return df\n",
    "\n",
    "    def back_features(df):\n",
    "        carriers = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','NflIdRusher','X','Y','Orientation','Dir','YardLine']]\n",
    "        carriers['back_from_scrimmage'] = carriers['YardLine'] - carriers['X']\n",
    "        carriers['back_oriented_down_field'] = carriers['Orientation'].apply(lambda x: back_direction(x))\n",
    "        carriers['back_moving_down_field'] = carriers['Dir'].apply(lambda x: back_direction(x))\n",
    "        carriers = carriers.rename(columns={'X':'back_X','Y':'back_Y'})\n",
    "        carriers = carriers[['GameId','PlayId','NflIdRusher','back_X','back_Y','back_from_scrimmage','back_oriented_down_field','back_moving_down_field']]\n",
    "        return carriers\n",
    "\n",
    "    def features_relative_to_back(df, carriers):\n",
    "        player_distance = df[['GameId','PlayId','NflId','X','Y']]\n",
    "        player_distance = pd.merge(player_distance, carriers, on=['GameId','PlayId'], how='inner')\n",
    "        player_distance = player_distance[player_distance['NflId'] != player_distance['NflIdRusher']]\n",
    "        player_distance['dist_to_back'] = player_distance[['X','Y','back_X','back_Y']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n",
    "\n",
    "        player_distance = player_distance.groupby(['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field'])\\\n",
    "                                         .agg({'dist_to_back':['min','max','mean','std']})\\\n",
    "                                         .reset_index()\n",
    "        player_distance.columns = ['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field',\n",
    "                                   'min_dist','max_dist','mean_dist','std_dist']\n",
    "\n",
    "        return player_distance\n",
    "\n",
    "    def defense_features(df):\n",
    "        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','Y']]\n",
    "        rusher.columns = ['GameId','PlayId','RusherTeam','RusherX','RusherY']\n",
    "\n",
    "        defense = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n",
    "        defense = defense[defense['Team'] != defense['RusherTeam']][['GameId','PlayId','X','Y','RusherX','RusherY']]\n",
    "        defense['def_dist_to_back'] = defense[['X','Y','RusherX','RusherY']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n",
    "\n",
    "        defense = defense.groupby(['GameId','PlayId'])\\\n",
    "                         .agg({'def_dist_to_back':['min','max','mean','std']})\\\n",
    "                         .reset_index()\n",
    "        defense.columns = ['GameId','PlayId','def_min_dist','def_max_dist','def_mean_dist','def_std_dist']\n",
    "\n",
    "        return defense\n",
    "            \n",
    "    def rusher_features(df):\n",
    "        \n",
    "        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Dir', 'S', 'A', 'X', 'Y']]\n",
    "        rusher.columns = ['GameId','PlayId', 'RusherDir', 'RusherS', 'RusherA', 'RusherX', 'RusherY']\n",
    "       \n",
    "        radian_angle = (90 - rusher['RusherDir']) * np.pi / 180.0\n",
    "        v_horizontal = np.abs(rusher['RusherS'] * np.cos(radian_angle))\n",
    "        v_vertical = np.abs(rusher['RusherS'] * np.sin(radian_angle)) \n",
    "       \n",
    "        rusher['v_horizontal'] = v_horizontal\n",
    "        rusher['v_vertical'] = v_vertical\n",
    "        \n",
    "        rusher.columns = ['GameId','PlayId', 'RusherDir', 'RusherS','RusherA','RusherX', 'RusherY','v_horizontal', 'v_vertical']\n",
    "        \n",
    "        return rusher\n",
    "    \n",
    "    def static_features(df):\n",
    "        \n",
    "        \n",
    "        add_new_feas = []\n",
    "\n",
    "        ## Height\n",
    "        df['PlayerHeight_dense'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n",
    "        \n",
    "        \n",
    "        add_new_feas.append('PlayerHeight_dense')\n",
    "\n",
    "        ## Time\n",
    "        df['TimeHandoff'] = df['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "        df['TimeSnap'] = df['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "\n",
    "        df['TimeDelta'] = df.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n",
    "        df['PlayerBirthDate'] =df['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m/%d/%Y\"))\n",
    "\n",
    "        ## Age\n",
    "        seconds_in_year = 60*60*24*365.25\n",
    "        df['PlayerAge'] = df.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()/seconds_in_year, axis=1)\n",
    "        add_new_feas.append('PlayerAge')\n",
    "\n",
    "        ## WindSpeed\n",
    "        df['WindSpeed_ob'] = df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n",
    "        df['WindSpeed_ob'] = df['WindSpeed_ob'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n",
    "        df['WindSpeed_ob'] = df['WindSpeed_ob'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n",
    "        df['WindSpeed_dense'] = df['WindSpeed_ob'].apply(strtofloat)\n",
    "        add_new_feas.append('WindSpeed_dense')\n",
    "\n",
    "        ## Weather\n",
    "        df['GameWeather_process'] = df['GameWeather'].str.lower()\n",
    "        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: \"indoor\" if not pd.isna(x) and \"indoor\" in x else x)\n",
    "        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\n",
    "        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\n",
    "        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n",
    "        df['GameWeather_dense'] = df['GameWeather_process'].apply(map_weather)\n",
    "        add_new_feas.append('GameWeather_dense')\n",
    "\n",
    "        ## Orientation and Dir\n",
    "        df[\"Orientation_ob\"] = df[\"Orientation\"].apply(lambda x : orientation_to_cat(x)).astype(\"object\")\n",
    "        df[\"Dir_ob\"] = df[\"Dir\"].apply(lambda x : orientation_to_cat(x)).astype(\"object\")\n",
    "\n",
    "        df[\"Orientation_sin\"] = df[\"Orientation\"].apply(lambda x : np.sin(x/360 * 2 * np.pi))\n",
    "        df[\"Orientation_cos\"] = df[\"Orientation\"].apply(lambda x : np.cos(x/360 * 2 * np.pi))\n",
    "        \n",
    "        df[\"Dir_sin\"] = df[\"Dir\"].apply(lambda x : np.sin(x/360 * 2 * np.pi))\n",
    "        df[\"Dir_cos\"] = df[\"Dir\"].apply(lambda x : np.cos(x/360 * 2 * np.pi))\n",
    "        \n",
    "        add_new_feas.append(\"Dir_sin\")\n",
    "        add_new_feas.append(\"Dir_cos\")\n",
    "\n",
    "        ## diff Score\n",
    "        df[\"diffScoreBeforePlay\"] = df[\"HomeScoreBeforePlay\"] - df[\"VisitorScoreBeforePlay\"]\n",
    "        add_new_feas.append(\"diffScoreBeforePlay\")\n",
    "    \n",
    "        static_features = df[df['NflId'] == df['NflIdRusher']][add_new_feas+['GameId','PlayId','X','Y','S','A','Dis','Orientation','Dir',\n",
    "                                                                             'YardLine','Quarter','Down','Distance',\n",
    "                                                                             'NflId','NflIdRusher','PossessionTeam','HomeTeamAbbr','Turf',\n",
    "                                                                             'VisitorTeamAbbr','PlayDirection','GameClock','Season','Team',\n",
    "                                                                             'FieldPosition','DefendersInTheBox']].drop_duplicates()\n",
    "        #static_features.fillna(-999,inplace=True)\n",
    "\n",
    "        return static_features\n",
    "\n",
    "\n",
    "    def combine_features(relative_to_back, defense, rushing, static, deploy=deploy):\n",
    "        df = pd.merge(relative_to_back,defense,on=['GameId','PlayId'],how='inner')\n",
    "        df = pd.merge(df,rushing,on=['GameId','PlayId'],how='inner')\n",
    "        df = pd.merge(df,static,on=['GameId','PlayId'],how='inner')\n",
    "        \n",
    "        if not deploy:\n",
    "            df = pd.merge(df, outcomes, on=['GameId','PlayId'], how='inner')\n",
    "\n",
    "        return df\n",
    "    \n",
    "    yardline = update_yardline(df)\n",
    "    df = update_orientation(df, yardline)\n",
    "\n",
    "    back_feats = back_features(df)\n",
    "    rel_back = features_relative_to_back(df, back_feats)\n",
    "    def_feats = defense_features(df)\n",
    "    static_feats = static_features(df)\n",
    "    rush_feats = rusher_features(df)\n",
    "    basetable = combine_features(rel_back, def_feats, rush_feats, static_feats,deploy = deploy)\n",
    "    \n",
    "    return basetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_02(t_):\n",
    "    \n",
    "    t_['fe1'] = pd.Series(np.sqrt(np.absolute(np.square(t_.X.values) - np.square(t_.Y.values))))\n",
    "    t_['fe5'] = np.square(t_['S'].values) + 2 * t_['A'].values * t_['Dis'].values  # N\n",
    "    t_['fe7'] = np.arccos(np.clip(t_['X'].values / t_['Y'].values, -1, 1))  # N\n",
    "    t_['fe8'] = t_['S'].values / np.clip(t_['fe1'].values, 0.6, None)\n",
    "    radian_angle = (90 - t_['Dir']) * np.pi / 180.0\n",
    "    t_['fe10'] = np.abs(t_['S'] * np.cos(radian_angle))\n",
    "    t_['fe11'] = np.abs(t_['S'] * np.sin(radian_angle))\n",
    "\n",
    "    t_['IsRusher'] = (t_['NflId'] == t_['NflIdRusher'])\n",
    "    temp = t_[t_[\"IsRusher\"]][[\"Team\", \"PlayId\"]].rename(columns={\"Team\":\"RusherTeam\"})\n",
    "    t_ = t_.merge(temp, on = \"PlayId\")\n",
    "    t_[\"IsRusherTeam\"] = t_[\"Team\"] == t_[\"RusherTeam\"]    \n",
    "\n",
    "    t_[\"is_left\"]            = t_[\"PlayDirection\"] == \"left\"\n",
    "    t_[\"old_data\"]    = t_[\"Season\"] == 2017\n",
    "    t_['YardLine_std'] = 100 - t_['YardLine']\n",
    "    \n",
    "    t_['Orientation_rad'] = np.mod(t_.Orientation, 360) * np.pi/180.0\n",
    "    t_.loc[t_.Season >= 2018, 'Orientation_rad'] = np.mod(t_.loc[t_.Season >= 2018, 'Orientation'] - 90, 360) * np.pi/180.0\n",
    "    t_['Orientation_std'] = t_.Orientation_rad\n",
    "    t_.loc[t_.is_left, 'Orientation_std'] = np.mod(np.pi + t_.loc[t_.is_left, 'Orientation_rad'], 2*np.pi)\n",
    "    \n",
    "    t_['norm_quat'] = (t_['X']**2 + t_['Y']**2 + t_['A']**2 + t_['S']**2)\n",
    "    t_['mod_quat'] = (t_['norm_quat'])**0.5\n",
    "    t_['norm_X'] = t_['X'] / t_['mod_quat']\n",
    "    t_['norm_Y'] = t_['Y'] / t_['mod_quat']\n",
    "    t_['norm_A'] = t_['A'] / t_['mod_quat']\n",
    "    t_['norm_S'] = t_['S'] / t_['mod_quat']    \n",
    "\n",
    "    t_['seconds_need_to_first_down'] = (t_['Distance']*0.9144)/t_['Dis']\n",
    "    t_['seconds_need_to_YardsLine'] = (t_['YardLine']*0.9144)/t_['Dis']    \n",
    "    t_['DefendersInTheBox_vs_Distance'] = t_['DefendersInTheBox'] / t_['Distance']    \n",
    "\n",
    "    t_['time_quarter'] = t_.GameClock.map(lambda x:transform_time_quarter(x))\n",
    "    t_['time_end'] = t_.apply(lambda x:transform_time_all(x.loc['GameClock'],x.loc['Quarter']),axis=1)\n",
    "\n",
    "    Turf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n",
    "            'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n",
    "            'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n",
    "            'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n",
    "            'SISGrass':'Artificial', 'Twenty-Four/Seven Turf':'Artificial', 'natural grass':'Natural'} \n",
    "\n",
    "    t_['Turf'] = t_['Turf'].map(Turf)\n",
    "    t_['Turf'] = t_['Turf'] == 'Natural'\n",
    "    \n",
    "    t_ = t_.sort_values(by = ['X']).sort_values(by = ['Dis']).sort_values(by=['PlayId', 'IsRusherTeam', 'IsRusher']).reset_index(drop = True)\n",
    "\n",
    "    return t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_03(df, enc_in, deploy=False):\n",
    "\n",
    "    \n",
    "    df['IsRusher'] = df.NflId == df.NflIdRusher\n",
    "    df['TeamOnOffense'] = \"home\"\n",
    "    \n",
    "    df.loc[df.PossessionTeam != df.HomeTeamAbbr, 'TeamOnOffense'] = \"away\"\n",
    "    \n",
    "    df['IsOnOffense'] = df.Team == df.TeamOnOffense # Is player on offense?\n",
    "    df['PlayerWeight_kg'] = df['PlayerWeight'].apply(lambda x: convert_to_kg(x))\n",
    "    df['OffenseFormation'] = df['OffenseFormation'].map(map_offense_formation)\n",
    "    df['DefendersInTheBox'] = df[['DefensePersonnel','DefendersInTheBox']].apply(lambda x: fill_defendersinabox(x[0],x[1]), axis=1)\n",
    "    df['DefensePersonnel'] = df['DefensePersonnel'].apply(map_DefensePersonnel)\n",
    "    \n",
    "    \n",
    "    df = add_x_y_components(df)\n",
    "    df = add_centroid(df)\n",
    "    # Calculates time needed to make 1st down\n",
    "    df = add_rushertimeto1stdown(df)\n",
    "    \n",
    "    # Calculate distances between offense, defense, rusher and players\n",
    "    df = calculate_distance(df)\n",
    "    \n",
    "    df, enc_out = add_distance_ratios(df, encoder=enc_in, deploy=deploy)\n",
    "\n",
    "    #df = add_distance_ratios(df, deploy=deploy)\n",
    "\n",
    "    # Calculates time to tackle\n",
    "    timetotackle = calc_timetotackle(df)\n",
    "    df = pd.merge(df, timetotackle, on=['GameId','PlayId'], how= 'left')\n",
    "\n",
    "    play_data = add_players_data(df)\n",
    "    df = pd.merge(df, play_data, on=['GameId','PlayId'], how= 'left')\n",
    "    # Calculates ratio between speeds from rusher and closest defenser\n",
    "    to_merge_S = s_ratio_runner_vs_1stdef(df)\n",
    "    df = pd.merge(df, to_merge_S, on=['GameId','PlayId'], how='left')\n",
    "    \n",
    "    df = df.loc[df['IsRusher']==1,:]\n",
    "    cols = [ 'Team', 'X', 'Y', 'S', 'A', 'Dis', 'Orientation', 'Dir', 'NflId', 'DisplayName',\n",
    "            'JerseyNumber', 'Season', 'Quarter', 'GameClock', 'PossessionTeam', 'Down', 'Distance', \n",
    "            'FieldPosition', 'HomeScoreBeforePlay', 'VisitorScoreBeforePlay', 'NflIdRusher', \n",
    "            'OffensePersonnel', \n",
    "            'PlayDirection', 'TimeHandoff', 'TimeSnap', 'PlayerHeight', 'PlayerWeight', \n",
    "            'PlayerBirthDate', 'PlayerCollegeName', 'Position', 'HomeTeamAbbr', 'VisitorTeamAbbr', \n",
    "            'Week', 'Stadium', 'Location', 'StadiumType', 'Turf', 'GameWeather', 'Temperature',\n",
    "            'Humidity', 'WindSpeed', 'WindDirection', 'YardLine', 'IsRusher', 'TeamOnOffense', \n",
    "            'IsOnOffense', 'PlayerWeight_kg',\n",
    "            'X_rusher','Y_rusher','Sx_rusher','Sy_rusher','Ax_rusher','Ay_rusher','F_rusher','Fx_rusher',\n",
    "            'Fy_rusher','p_rusher','px_rusher','py_rusher','p_max_rusher','px_max_rusher','py_max_rusher',\n",
    "            'dist_player_vs_rusher']\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "    feat = [col for col in df.columns if col not in ['Yards']]\n",
    "    df = df[feat]\n",
    "    \n",
    "    return df, enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_basetable_01 = create_features_01(train, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basetable_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_basetable_02 = create_features_02(train_basetable_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basetable_02.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basetable_02.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove alguns outliers\n",
    "train_basetable_02['count'] = train_basetable_02.groupby('Yards')['Yards'].transform('count')\n",
    "train_basetable_02 = train_basetable_02[train_basetable_02['count'] > 1]\n",
    "train_basetable_02.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_basetable_03, enc_out = create_features_03(train, OneHotEncoder(handle_unknown='ignore', sparse = False), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basetable_03.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(train_basetable_02, train_basetable_03, on=['GameId','PlayId'], how='inner')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['count','GameId','PlayId','NflId','NflIdRusher','PossessionTeam',\n",
    "        'HomeTeamAbbr','VisitorTeamAbbr','PlayDirection','GameClock',\n",
    "        'Season','Team','FieldPosition','IsRusher','IsRusherTeam','RusherTeam'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing(dataset, display = 5):\n",
    "    temp_df = dataset.copy()\n",
    "    df_nan = (temp_df.isnull().sum() / len(temp_df)) * 100\n",
    "    missing_data = pd.DataFrame({'Missing n': temp_df.isnull().sum(),'% Missing' :df_nan})\n",
    "    if missing_data['Missing n'].sum() == 0:\n",
    "        return print('Great! There are no missing values in this dataset.')\n",
    "    else:\n",
    "        return missing_data.sort_values('% Missing', ascending = False).head(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(X, 5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X['OffenseFormation'].fillna(0, inplace=True)\n",
    "X['OffenseFormation'] = X['OffenseFormation'].astype(int)\n",
    "X['DefensePersonnel'] = X['DefensePersonnel'].astype(int)\n",
    "\n",
    "OHE = OneHotEncoder(handle_unknown='ignore', sparse = False)\n",
    "\n",
    "columns = ['OffenseFormation' ,'DefensePersonnel']\n",
    "new_column = pd.DataFrame(OHE.fit_transform(X[columns]))\n",
    "new_column.columns = ['OffenseFormation_0',\n",
    "                      'OffenseFormation_1',\n",
    "                      'OffenseFormation_2',\n",
    "                      'OffenseFormation_3',\n",
    "                      'OffenseFormation_4',\n",
    "                      'OffenseFormation_5',\n",
    "                      'OffenseFormation_6',\n",
    "                      'OffenseFormation_7',\n",
    "                      'DefensePersonnel_0',\n",
    "                      'DefensePersonnel_1',\n",
    "                      'DefensePersonnel_2',\n",
    "                      'DefensePersonnel_3',\n",
    "                      'DefensePersonnel_4',\n",
    "                      'DefensePersonnel_5']\n",
    "X = pd.concat([X, new_column], axis=1)\n",
    "delcol = ['OffenseFormation','DefensePersonnel']\n",
    "X.drop(delcol, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_features = ['Yards','A','S','back_from_scrimmage','back_oriented_down_field','back_moving_down_field','old_data',\n",
    "                 'def_mean_dist','def_std_dist','def_min_dist','def_max_dist','min_dist','mean_dist',\n",
    "                 'PlayerAge','PlayerHeight_dense','Dis',\n",
    "                 'Distance','Dir','Dir_sin','Dir_cos','YardLine_std','Orientation_std',\n",
    "                 'WindSpeed_dense','GameWeather_dense',\n",
    "                 'fe1_x','fe5_x','fe7_x','fe8_x','fe10_x','fe11_x',\n",
    "                 'norm_quat','mod_quat','norm_X','norm_Y','norm_A','norm_S',\n",
    "                 'RusherS','RusherA','RusherY', 'Turf',\n",
    "\n",
    "                 'fe1_y','fe5_y','fe7_y','fe8_y','fe10_y','fe11_y','Sx','Sy','Sy_','Ax','Ay','Sfx','Sfy','F','Fx',\n",
    "                 'Fy','p','px','py','pf_max','px_max','py_max','KE','Xc_def_mean','Xc_off_mean','Xc_def_std',\n",
    "                 'Xc_off_std','Yc_def_mean','Yc_off_mean','Yc_def_std','Yc_off_std','S0x_rusher',\n",
    "                 'rusher_dist_to_1stdown','t_1stDown','dist_cdef_vs_coff','dist_cdef_vs_player',\n",
    "                 'dist_coff_vs_player','dist_cdef_vs_rusher','dist_coff_vs_rusher','dist_from_rusher_DEF',\n",
    "                 'dist_from_rusher_OFF','dist_from_rusher_ratio','time_to_tackle_mean',\n",
    "                 'time_to_tackle_min','time_to_tackle_max','dist_cdef_vs_player_mean_NOnOff',\n",
    "                 'dist_cdef_vs_player_mean_OnOff','dist_cdef_vs_player_min_NOnOff',\n",
    "                 'dist_cdef_vs_player_min_OnOff','dist_coff_vs_player_mean_NOnOff',\n",
    "                 'dist_coff_vs_player_mean_OnOff','dist_coff_vs_player_min_NOnOff',\n",
    "                 'dist_coff_vs_player_min_OnOff','dist_player_vs_rusher_mean_NOnOff',\n",
    "                 'dist_player_vs_rusher_mean_OnOff','dist_player_vs_rusher_min_NOnOff',\n",
    "                 'dist_player_vs_rusher_min_OnOff','rusher_1stdef_sx_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = X.copy()\n",
    "new_X = X.loc[:,X.columns]\n",
    "new_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(30, 30)})\n",
    "corr = new_X.corr()\n",
    "plt.figure() \n",
    "ax = sns.heatmap(corr, linewidths=.5, annot=True, cmap=\"YlGnBu\", fmt='.1g')\n",
    "plt.savefig('corr_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated features (37->28)\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.99:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "\n",
    "best_features_revised = new_X.columns[columns].values\n",
    "drop_columns = new_X.columns[columns == False].values\n",
    "print(best_features_revised)\n",
    "print(drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = X.loc[:,best_features_revised]\n",
    "new_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importância do Atributo com o Extra Trees Classifier\n",
    "X_ = new_X.drop(['Yards'], axis=1)\n",
    "y_ = new_X['Yards']\n",
    "\n",
    "X_.replace(-np.inf,0,inplace=True)\n",
    "X_.replace(np.inf,0,inplace=True)\n",
    "X_.fillna(0,inplace=True)\n",
    "\n",
    "# Padronizando os dados (0 para a média, 1 para o desvio padrão)\n",
    "X_ = StandardScaler().fit_transform(X_)\n",
    "\n",
    "# Criação do Modelo - Feature Selection\n",
    "modeloRF = RandomForestRegressor(bootstrap=False, max_features=0.3, min_samples_leaf=15, \n",
    "                                  min_samples_split=8, n_estimators=50, n_jobs=-1, random_state=42)\n",
    "modeloRF.fit(X_, y_)\n",
    "\n",
    "# Convertendo o resultado em um dataframe\n",
    "feature_importance_df = pd.DataFrame(new_X.drop(['Yards'], axis=1).columns,columns=['Feature'])\n",
    "feature_importance_df['importance'] = pd.DataFrame(modeloRF.feature_importances_.astype(float))\n",
    "\n",
    "# Realizando a ordenacao por Importancia (Maior para Menor)\n",
    "result = feature_importance_df.sort_values('importance',ascending=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:50].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(18,16))\n",
    "sns.barplot(x=\"importance\",\n",
    "           y=\"Feature\",\n",
    "           data=best_features.sort_values(by=\"importance\",\n",
    "                                          ascending=False))\n",
    "plt.title('Importance Features')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Criar e avaliar alguns algoritmos de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Split Treino e Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um dataset somente com as colunas mais importantes conforme Feature Selection\n",
    "new_X = X.loc[:,best_features['Feature']]\n",
    "new_X.replace(-np.inf,0,inplace=True)\n",
    "new_X.replace(np.inf,0,inplace=True)\n",
    "new_X.fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "target = X.Yards\n",
    "\n",
    "y = np.zeros((target.shape[0], 199))\n",
    "for idx, target in enumerate(list(target)):\n",
    "    y[idx][99 + target] = 1\n",
    "    \n",
    "# Normalizando as variaveis do dataset de treino\n",
    "scaler = StandardScaler()\n",
    "new_X = scaler.fit_transform(new_X)\n",
    "new_X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Teste 3 modelos (LightGBM, RF e NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CRPS score\n",
    "def crps_score(y_prediction, y_valid, shape=X.shape[0]):\n",
    "    y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n",
    "    y_pred = np.clip(np.cumsum(y_prediction, axis=1), 0, 1)\n",
    "    val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * shape)\n",
    "    crps = np.round(val_s, 6)\n",
    "    \n",
    "    return crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRPSCallback(Callback):\n",
    "    \n",
    "    def __init__(self,validation, predict_batch_size=20, include_on_batch=False):\n",
    "        super(CRPSCallback, self).__init__()\n",
    "        self.validation = validation\n",
    "        self.predict_batch_size = predict_batch_size\n",
    "        self.include_on_batch = include_on_batch\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not ('CRPS_score_val' in self.params['metrics']):\n",
    "            self.params['metrics'].append('CRPS_score_val')\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if (self.include_on_batch):\n",
    "            logs['CRPS_score_val'] = float('-inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['CRPS_score_val'] = float('-inf')\n",
    "            \n",
    "        if (self.validation):\n",
    "            X_valid, y_valid = self.validation[0], self.validation[1]\n",
    "            y_pred = self.model.predict(X_valid)\n",
    "            y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n",
    "            y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
    "            val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * X_valid.shape[0])\n",
    "            val_s = np.round(val_s, 6)\n",
    "            logs['CRPS_score_val'] = val_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.regularizers import l2,l1, l1_l2\n",
    "\n",
    "def get_nn(x_tr,y_tr,x_val,y_val,shape):\n",
    "    K.clear_session()\n",
    "    inp = Input(shape = (x_tr.shape[1],))\n",
    "    \n",
    "    x = Dense(1024, input_dim=X.shape[1], activation='relu')(inp)# bias_regularizer=l1_l2(l1=0.05,l2=0.05))(inp)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    out = Dense(199, activation='softmax')(x)\n",
    "    model = Model(inp,out)\n",
    "    \n",
    "    model.compile(optimizer = optimizers.adam(lr = 0.05, decay = 0.05),\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=[])\n",
    "     \n",
    "    es = EarlyStopping(monitor='CRPS_score_val', \n",
    "                       mode='min',\n",
    "                       restore_best_weights=True, \n",
    "                       verbose=10, \n",
    "                       patience=15)\n",
    "\n",
    "    mc = ModelCheckpoint('best_model.h5',monitor='CRPS_score_val',mode='min',save_best_only=True, \n",
    "                         verbose=10, save_weights_only=True)\n",
    "    \n",
    "    bsz = 1024\n",
    "    model.fit(x_tr, y_tr,\n",
    "              callbacks=[CRPSCallback(validation = (x_val,y_val)),es,mc], \n",
    "              epochs=200, \n",
    "              batch_size=bsz,\n",
    "              verbose=10)\n",
    "    \n",
    "    model.load_weights(\"best_model.h5\")\n",
    "    \n",
    "    y_pred = model.predict(x_val)\n",
    "    y_valid = y_val\n",
    "    crps = crps_score(y_pred, y_valid, shape=shape)\n",
    "\n",
    "    return model,crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"multi_logloss\"\n",
    "param = {'num_leaves': 50,\n",
    "         'min_data_in_leaf': 30,\n",
    "         'objective':'multiclass',\n",
    "         'num_class': 199,\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.7,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": metric,\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"seed\":1234}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgbm(x_tr, y_tr, x_val, y_val, shape):\n",
    "    y_valid = y_val\n",
    "    y_tr = np.argmax(y_tr, axis=1)\n",
    "    y_val = np.argmax(y_val, axis=1)\n",
    "    trn_data = lgb.Dataset(x_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(x_val, label=y_val)\n",
    "    model = lgb.train(param, trn_data, 10000, valid_sets = [val_data], verbose_eval = 100, early_stopping_rounds = 200)\n",
    "    \n",
    "    y_pred = model.predict(x_val, num_iteration=model.best_iteration)\n",
    "    crps = crps_score(y_pred, y_valid, shape=shape)\n",
    "    \n",
    "    return model, crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rf(x_tr, y_tr, x_val, y_val, shape):\n",
    "    model = RandomForestRegressor(bootstrap=False, max_features=0.3, min_samples_leaf=15, \n",
    "                                  min_samples_split=8, n_estimators=50, n_jobs=-1, random_state=42)\n",
    "    model.fit(x_tr, y_tr)\n",
    "    \n",
    "    y_pred = model.predict(x_val)\n",
    "    y_valid = y_val\n",
    "    crps = crps_score(y_pred, y_valid, shape=shape)\n",
    "    \n",
    "    return model, crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "loop = 2\n",
    "fold = 5\n",
    "\n",
    "oof_nn = np.zeros([loop, y.shape[0], y.shape[1]])\n",
    "oof_lgbm = np.zeros([loop, y.shape[0], y.shape[1]])\n",
    "oof_rf = np.zeros([loop, y.shape[0], y.shape[1]])\n",
    "\n",
    "models_nn = []\n",
    "crps_csv_nn = []\n",
    "\n",
    "models_lgbm = []\n",
    "crps_csv_lgbm = []\n",
    "\n",
    "models_rf = []\n",
    "crps_csv_rf = []\n",
    "\n",
    "for k in range(loop):\n",
    "    kfold = KFold(fold, random_state = 42 + k, shuffle = True)\n",
    "    for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(y)):\n",
    "        print(\"-----------\")\n",
    "        print(f'Loop {k+1}/{loop}' + f' Fold {k_fold+1}/{fold}')\n",
    "        print(\"-----------\")\n",
    "        \n",
    "        tr_x, tr_y = new_X[tr_inds], y[tr_inds]\n",
    "        val_x, val_y = new_X[val_inds], y[val_inds]\n",
    "        \n",
    "        # Train NN\n",
    "        nn, crps_nn = get_nn(tr_x, tr_y, val_x, val_y, shape=val_x.shape[0])\n",
    "        models_nn.append(nn)\n",
    "        print(\"the %d fold crps (NN) is %f\"%((k_fold+1), crps_nn))\n",
    "        crps_csv_nn.append(crps_nn)\n",
    "        \n",
    "        # Train LGBM\n",
    "        #lgbm, crps_lgbm = get_lgbm(tr_x, tr_y, val_x, val_y, shape=val_x.shape[0])\n",
    "        #models_lgbm.append(lgbm)\n",
    "        #print(\"the %d fold crps (LGBM) is %f\"%((k_fold+1), crps_lgbm))\n",
    "        #crps_csv_lgbm.append(crps_lgbm)\n",
    "        \n",
    "        # Train RF\n",
    "        #rf, crps_rf = get_rf(tr_x, tr_y, val_x, val_y, shape=val_x.shape[0])\n",
    "        #models_rf.append(rf)\n",
    "        #print(\"the %d fold crps (RF) is %f\"%((k_fold+1), crps_rf))\n",
    "        #crps_csv_rf.append(crps_rf)\n",
    "        \n",
    "        #Predict OOF\n",
    "        oof_nn[k, val_inds, :] = nn.predict(val_x)\n",
    "        #oof_lgbm[k, val_inds, :] = lgbm.predict(val_x, num_iteration=lgbm.best_iteration)\n",
    "        #oof_rf[k, val_inds, :] = rf.predict(val_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_oof_nn = []\n",
    "#crps_oof_lgbm = []\n",
    "#crps_oof_rf = []\n",
    "\n",
    "for k in range(loop):\n",
    "    crps_oof_nn.append(crps_score(oof_nn[k,...], y))\n",
    "    #crps_oof_lgbm.append(crps_score(oof_lgbm[k,...], y))\n",
    "    #crps_oof_rf.append(crps_score(oof_rf[k,...], y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean crps (NN) is %f\"%np.mean(crps_csv_nn))\n",
    "#print(\"mean crps (LGBM) is %f\"%np.mean(crps_csv_lgbm))\n",
    "#print(\"mean crps (RF) is %f\"%np.mean(crps_csv_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean OOF crps (NN) is %f\"%np.mean(crps_oof_nn))\n",
    "#print(\"mean OOF crps (LGBM) is %f\"%np.mean(crps_oof_lgbm))\n",
    "#print(\"mean OOF crps (RF) is %f\"%np.mean(crps_oof_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v16.1: mean OOF crps (NN) is 0.012613\n",
    "#v16.2: mean OOF crps (NN) is 0.011860\n",
    "#v16.3: Best mean crps (Blend):  0.012417 (removendo apenas 13 linhas, com frequencia < 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Blending Weight Optimisation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def weight_opt(oof_nn, oof_rf, oof_lgbm, y_true):\n",
    "    weight_nn = np.inf\n",
    "    best_crps = np.inf\n",
    "    \n",
    "    for i in np.arange(0, 1.01, 0.05):\n",
    "        crps_blend = np.zeros(oof_nn.shape[0])\n",
    "        for k in range(oof_nn.shape[0]):\n",
    "            crps_blend[k] = crps_score(i * oof_nn[k,...] + (1-i) * oof_rf[k,...] + (1-i) * oof_lgbm[k,...], y_true)\n",
    "        if np.mean(crps_blend) < best_crps:\n",
    "            best_crps = np.mean(crps_blend)\n",
    "            weight_nn = round(i, 2)\n",
    "            \n",
    "        print(str(round(i, 2)) + ' : mean crps (Blend) is ', round(np.mean(crps_blend), 6))\n",
    "        \n",
    "    print('-'*36)\n",
    "    print('Best weight for NN: ', weight_nn)\n",
    "    print('Best weight for LGBM: ', round(1-weight_nn, 2))\n",
    "    print('Best weight for RF: ', round(1-weight_nn, 2))\n",
    "    print('Best mean crps (Blend): ', round(best_crps, 6))\n",
    "    \n",
    "    return weight_nn, round(1-weight_nn, 2), round(1-weight_nn, 2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weight_nn, weight_rf, weight_lgbm = weight_opt(oof_nn, oof_rf, oof_lgbm, y)\n",
    "# weight_nn, weight_rf = weight_opt(oof_nn, oof_rf, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Realizar a submissão para o Kaggle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def predict(x_te, models_nn, models_rf, weight_nn, weight_rf, iteration=False):\n",
    "    model_num_nn = len(models_nn)\n",
    "    model_num_rf = len(models_rf)\n",
    "    for k,m in enumerate(models_nn):\n",
    "        if k==0:\n",
    "            y_pred_nn = m.predict(x_te, batch_size=1024)\n",
    "            if iteration:\n",
    "                y_pred_rf = models_rf[k].predict(x_te, num_iteration=models_rf[k].best_iteration)\n",
    "            else:\n",
    "                y_pred_rf = models_rf[k].predict(x_te)\n",
    "        else:\n",
    "            y_pred_nn += m.predict(x_te, batch_size=1024)\n",
    "            if iteration:\n",
    "                y_pred_rf = models_rf[k].predict(x_te, num_iteration=models_rf[k].best_iteration)\n",
    "            else:\n",
    "                y_pred_rf = models_rf[k].predict(x_te)\n",
    "            \n",
    "    y_pred_nn = y_pred_nn / model_num_nn\n",
    "    y_pred_rf = y_pred_rf / model_num_rf\n",
    "    \n",
    "    return weight_nn * y_pred_nn + weight_rf * y_pred_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_te, models_nn):\n",
    "    \n",
    "    model_num_nn = len(models_nn)\n",
    "\n",
    "    for k,m in enumerate(models_nn):\n",
    "        if k==0:\n",
    "            y_pred_nn = m.predict(x_te, batch_size=1024)\n",
    "        else:\n",
    "            y_pred_nn += m.predict(x_te, batch_size=1024)\n",
    "            \n",
    "    y_pred_nn = y_pred_nn / model_num_nn\n",
    "    \n",
    "    return y_pred_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if  TRAIN_OFFLINE==False:\n",
    "    \n",
    "    from kaggle.competitions import nflrush\n",
    "    env = nflrush.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    df_prev = pd.DataFrame()\n",
    "    df_test = pd.DataFrame()\n",
    "\n",
    "    for (test_df, sample_prediction_df) in tqdm_notebook(iter_test):\n",
    "        \n",
    "        basetable_01 = create_features_01(test_df, deploy=True)\n",
    "        basetable_02 = create_features_02(basetable_01)\n",
    "        basetable_03, _ = create_features_03(test_df, enc_out, True)\n",
    "        X = pd.merge(basetable_02, basetable_03, on=['GameId','PlayId'], how='inner')\n",
    "\n",
    "        # Considerar somente as colunas do Feature Selection\n",
    "        X = X.loc[:,best_features['Feature']]\n",
    "        X.fillna(0,inplace=True)\n",
    "    \n",
    "        scaled_basetable = scaler.transform(X)\n",
    "        \n",
    "        y_pred = predict(scaled_basetable, models_nn)\n",
    "        y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1).tolist()[0]\n",
    "\n",
    "        preds_df = pd.DataFrame(data=[y_pred], columns=sample_prediction_df.columns)\n",
    "        \n",
    "        df_test = df_test.append(X)\n",
    "        df_prev = df_prev.append(preds_df)\n",
    "    \n",
    "        env.predict(preds_df)\n",
    "\n",
    "    env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2de19b2423534e9d9f3fbe051ff1ca39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d1f665fbad88453dbb7fcec9ddb54d21",
        "IPY_MODEL_afb33efa99d24435a0515b10b8e2c7b9"
       ],
       "layout": "IPY_MODEL_6c44e022da4246eab50db4d4e582e377"
      }
     },
     "3d0af63b4a4f4460af892e9c8b7a7792": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4c5bd2934baa429186673fb55e079d74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6c44e022da4246eab50db4d4e582e377": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76a9927a07d549ffb49219d82a0576ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "afb33efa99d24435a0515b10b8e2c7b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d0234a33663e4483b673d91a12b34924",
       "placeholder": "​",
       "style": "IPY_MODEL_4c5bd2934baa429186673fb55e079d74",
       "value": " 3438/? [17:44&lt;00:00,  3.23it/s]"
      }
     },
     "d0234a33663e4483b673d91a12b34924": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d1f665fbad88453dbb7fcec9ddb54d21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3d0af63b4a4f4460af892e9c8b7a7792",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_76a9927a07d549ffb49219d82a0576ac",
       "value": 1
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
