{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle\n",
    "## Competition NFL Big Data Bowl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os pacotes\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Statistic lib\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, norm\n",
    "\n",
    "# Sklearn lib\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Modelos de Regressao\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as XGB\n",
    "import tqdm\n",
    "\n",
    "# Model Keras (NN)\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization, Dropout, Activation, PReLU, Add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import metrics\n",
    "\n",
    "# Misc lib\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from functools import partial\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from IPython.display import Image\n",
    "\n",
    "# Utils\n",
    "import pandasql as ps\n",
    "import re \n",
    "import math, string, os\n",
    "import datetime\n",
    "\n",
    "# Options\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_seq_items = 8000\n",
    "pd.options.display.max_rows = 8000\n",
    "pd.set_option('display.max_columns', None)\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4f97b8fd-2dac-4fe0-aa0b-8c1d1cc7d66f",
    "_uuid": "72f9ce60-48f7-4666-80a4-c343e94bcc5a"
   },
   "outputs": [],
   "source": [
    "# Carregando os dados de treino\n",
    "train = pd.read_csv('../data/train_stage2.csv', low_memory=False)\n",
    "#train = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)\n",
    "print (\"Dataset carregado !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao para realizar feature engineering no dataset (treino ou teste)\n",
    "def feature_engineering(df): \n",
    "\n",
    "    # Nova feature para indicar se é o jogador que esta realizando a jogada (corredor)\n",
    "    df['IsRusher'] = df['NflId'] == df['NflIdRusher']\n",
    "    \n",
    "    # Tratando a feature Team\n",
    "    df['Team'] = df['Team'].apply(lambda x: x.strip()=='home')\n",
    "    \n",
    "    # Remove todas as colunas categoricas\n",
    "    cat_features = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype =='object':\n",
    "            cat_features.append(col)\n",
    "    df = df.drop(cat_features, axis=1) \n",
    "    \n",
    "    # Ordenacao do dataset e renovando o index\n",
    "    df = df.sort_values(by=['PlayId', 'Team', 'IsRusher']).reset_index()\n",
    "    \n",
    "    # Removendo colunas que não serão utilizadas\n",
    "    df.drop(['GameId', 'PlayId', 'index', 'IsRusher', 'Team', 'NflId', 'NflIdRusher'], axis=1, inplace=True)\n",
    "\n",
    "    df_median = df.median()\n",
    "    df.fillna(df_median, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um novo dataset aplicando Feature Engineering\n",
    "train_df = feature_engineering(train)\n",
    "ind_train = len(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "280b0090-ad48-4c21-91fb-4f762159b74e",
    "_uuid": "14db5230-0ae7-448c-b595-b9683230fe91"
   },
   "source": [
    "# Criação e Validação dos Modelos de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo uma limpeza na memoria\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma linha para cada jogada em que o rusher é o último\n",
    "players_col = []\n",
    "for col in train_df.columns:\n",
    "    if train_df[col][:22].std()!=0:\n",
    "        players_col.append(col)\n",
    "        \n",
    "X_train = np.array(train_df[players_col]).reshape(ind_train//22,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_col = train_df.drop(players_col+['Yards'], axis=1).columns\n",
    "X_play_col = np.zeros(shape=(X_train.shape[0], len(play_col)))\n",
    "for i, col in enumerate(play_col):\n",
    "    X_play_col[:, i] = train_df[col][::22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train, X_play_col], axis=1)\n",
    "y_train = np.zeros(shape=(X_train.shape[0], 199))\n",
    "\n",
    "for i,yard in enumerate(train_df['Yards'][::22]):\n",
    "    y_train[i, yard+99:] = np.ones(shape=(1, 100-yard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['RAdam']\n",
    "\n",
    "class RAdam(keras.optimizers.Optimizer):\n",
    "    \"\"\"RAdam optimizer.\n",
    "    # Arguments\n",
    "        learning_rate: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: float >= 0. Weight decay for each param.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n",
    "        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n",
    "        min_lr: float >= 0. Minimum learning rate after warmup.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "        - [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n",
    "                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n",
    "        learning_rate = kwargs.pop('lr', learning_rate)\n",
    "        super(RAdam, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n",
    "            self.total_steps = K.variable(total_steps, name='total_steps')\n",
    "            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n",
    "            self.min_lr = K.variable(min_lr, name='min_lr')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.initial_weight_decay = weight_decay\n",
    "        self.initial_total_steps = total_steps\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "        if self.initial_total_steps > 0:\n",
    "            warmup_steps = self.total_steps * self.warmup_proportion\n",
    "            decay_steps = K.maximum(self.total_steps - warmup_steps, 1)\n",
    "            decay_rate = (self.min_lr - lr) / decay_steps\n",
    "            lr = K.switch(\n",
    "                t <= warmup_steps,\n",
    "                lr * (t / warmup_steps),\n",
    "                lr + decay_rate * K.minimum(t - warmup_steps, decay_steps),\n",
    "            )\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        else:\n",
    "            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n",
    "\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        beta_1_t = K.pow(self.beta_1, t)\n",
    "        beta_2_t = K.pow(self.beta_2, t)\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - self.beta_2) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * t * beta_2_t / (1.0 - beta_2_t)\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            m_corr_t = m_t / (1.0 - beta_1_t)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                v_corr_t = K.sqrt(vhat_t / (1.0 - beta_2_t))\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                v_corr_t = K.sqrt(v_t / (1.0 - beta_2_t))\n",
    "\n",
    "            r_t = K.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                         (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                         sma_inf / sma_t)\n",
    "\n",
    "            p_t = K.switch(sma_t >= 5, r_t * m_corr_t / (v_corr_t + self.epsilon), m_corr_t)\n",
    "\n",
    "            if self.initial_weight_decay > 0:\n",
    "                p_t += self.weight_decay * p\n",
    "\n",
    "            p_t = p - lr * p_t\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self.learning_rate\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'learning_rate': float(K.get_value(self.learning_rate)),\n",
    "            'beta_1': float(K.get_value(self.beta_1)),\n",
    "            'beta_2': float(K.get_value(self.beta_2)),\n",
    "            'decay': float(K.get_value(self.decay)),\n",
    "            'weight_decay': float(K.get_value(self.weight_decay)),\n",
    "            'epsilon': self.epsilon,\n",
    "            'amsgrad': self.amsgrad,\n",
    "            'total_steps': float(K.get_value(self.total_steps)),\n",
    "            'warmup_proportion': float(K.get_value(self.warmup_proportion)),\n",
    "            'min_lr': float(K.get_value(self.min_lr)),\n",
    "        }\n",
    "        base_config = super(RAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar a estrutura da Rede Neural\n",
    "def make_nn_model():\n",
    "    numerical_inputs = Input(shape=(X_train.shape[1],)) \n",
    "    x = Dense(X_train.shape[1], activation='relu')(numerical_inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    logits = Dense(256,activation=None)(x)\n",
    "    logits = PReLU()(logits)\n",
    "    logits__ = logits\n",
    "    logits = Dropout(0.5)(logits)\n",
    "    \n",
    "    logits = Dense(256,activation=None)(logits)\n",
    "    logits = PReLU()(logits)\n",
    "    logits_ = logits\n",
    "    logits = Dropout(0.5)(logits)\n",
    "    \n",
    "    logits = Dense(256,activation=None)(logits)\n",
    "    logits = PReLU()(logits)\n",
    "    logits = Concatenate()([logits, logits_, logits__])\n",
    "    logits = Dropout(0.25)(logits)\n",
    "    \n",
    "    out = Dense(199, activation='sigmoid')(logits)\n",
    "    \n",
    "    model = Model(inputs = numerical_inputs, outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crps(labels,predictions) :\n",
    "    y_pred = np.zeros((len(labels),199))\n",
    "    y_ans = np.zeros((len(labels),199))\n",
    "    j = np.array(range(199))\n",
    "    for i,(p,t) in enumerate(zip(np.round(scaler.inverse_transform(predictions)),labels)) :\n",
    "        k2 = j[j>=p-10]\n",
    "        y_pred[i][k2]=(k2+10-p)*0.05\n",
    "        k1 = j[j>=p+10]\n",
    "        y_pred[i][k1]= 1.0\n",
    "        k3 = j[j>=t]\n",
    "        y_ans[i][k3]= 1.0\n",
    "                           \n",
    "    return 'CRPS: ', K.np.sum((y_pred-y_ans)**2)/(199*y_pred.shape[0]), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao de treinamento do modelo de Redes Neurais\n",
    "def train_model(x_tr, y_tr, x_vl, y_vl):\n",
    "    model = make_nn_model()\n",
    "    er = EarlyStopping(patience=10, min_delta=1e-4, restore_best_weights=True, monitor='val_loss')\n",
    "    model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-7), loss='mse', metrics=[crps])\n",
    "    model.fit(x_tr, y_tr, epochs=2, callbacks=[er], validation_data=[x_vl, y_vl])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cross validation folds\n",
    "kf = 2\n",
    "rkf = RepeatedKFold(n_splits=kf, n_repeats=kf)\n",
    "print(str(kf) + ' Folds para treino...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a mesma escala nos dados\n",
    "scaler = MinMaxScaler() \n",
    "X_train = scaler.fit_transform(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for fold_, (tr_idx, vl_idx) in enumerate(rkf.split(X_train, y_train)):\n",
    "    strLog = \"fold {}\".format(fold_)\n",
    "    print(strLog)\n",
    "    \n",
    "    x_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n",
    "    x_vl, y_vl = X_train[vl_idx], y_train[vl_idx]\n",
    "    \n",
    "    model = train_model(x_tr, y_tr, x_vl, y_vl)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REALIZANDO A SUBMISSAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao para realizar as previsoes no dataset de teste\n",
    "def make_pred(df, sample, env, models):\n",
    "    X = np.array(df[players_col]).reshape(ind_test//22, -1)\n",
    "    play_col = df.drop(players_col, axis=1).columns\n",
    "    X_play_col = np.zeros(shape=(X.shape[0], len(play_col)))\n",
    "    for i, col in enumerate(play_col):\n",
    "        X_play_col[:, i] = df[col][::22]\n",
    "    \n",
    "    X = np.concatenate([X, X_play_col], axis=1)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    y_pred = np.array([model.predict(X) for model in models]).mean(0)\n",
    "    \n",
    "    for pred in y_pred:\n",
    "        prev = 0\n",
    "        for i in range(len(pred)):\n",
    "            print(pred[i])\n",
    "            if pred[i]<prev:\n",
    "                pred[i]=prev\n",
    "            prev=pred[i]\n",
    "\n",
    "    env.predict(pd.DataFrame(data=y_pred,columns=sample.columns))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = nflrush.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test, sample in tqdm.tqdm(env.iter_test()):\n",
    "    make_pred(test, sample, env, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.write_submission_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
