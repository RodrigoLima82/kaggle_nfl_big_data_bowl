{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NFL Competition\n",
    "\n",
    "# Feature Engineering e Modelo de Machine Learning\n",
    "\n",
    "- Version: 1.0: usando padrao do fork: https://www.kaggle.com/bestpredict/location-eda-8eb410\n",
    "        *    Resultado: 0.012744\n",
    "        *    LB: 0.01363\n",
    "   \n",
    "- <font color='red'>Version: 2.0: adicionado Feature Selection com LOFO Importance\n",
    "        *    Resultado: 0.012780\n",
    "        *    LB: 0.01365\n",
    "        \n",
    "- <font color='blue'>Version: 3.0: adicionado novas features (apenas feature fxx + old_data + YardLine_std)\n",
    "        *    Resultado: 0.012614\n",
    "        *    LB: 0.01361\n",
    "        \n",
    "- <font color='blue'>Version: 4.0: adicionado novas features (turf)\n",
    "        *    Resultado: 0.012624\n",
    "        *    LB: 0.01361\n",
    "        \n",
    "- <font color='red'>Version: 5.0: adicionado novas features (game_time)\n",
    "        *    Resultado: 0.012635\n",
    "        *    LB: 0.01362\n",
    "\n",
    "- <font color='blue'>Version: 6.0: adicionado novas features (feat1, feat2, feat3, feat4) e removido (Turf + game_time)\n",
    "        *    Resultado: 0.012536\n",
    "        *    LB: Não é permitido\n",
    "    \n",
    "- <font color='red'>Version: 7.0: alteração do modelo de bagging\n",
    "        *    Resultado: 0.012474\n",
    "        *    LB: 0.01362\n",
    "\n",
    "    \n",
    "- <font color='blue'>Version: 8.0: adicionado novas features:\n",
    "    norm_quat,mod_quat,norm_X,norm_Y,norm_A,norm_S, X_YardLine_std,X_YardLine_median,X_YardLine_max,X_YardLine_min\n",
    "    \n",
    "        *    Resultado: 0.012604\n",
    "        *    LB:\n",
    "    \n",
    "- <font color='green'>Version: 9.0: adicionado novas features:\n",
    "    \n",
    "        *    Resultado:\n",
    "        *    LB:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importa os pacotes e o dataset de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Importar os principais pacotes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "import codecs\n",
    "import time\n",
    "import datetime\n",
    "import tsfresh\n",
    "import pandasql as ps\n",
    "import gc\n",
    "\n",
    "# Evitar que aparece os warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Seta algumas opções no Jupyter para exibição dos datasets\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "\n",
    "# Variavel para controlar o treinamento no Kaggle\n",
    "TRAIN_OFFLINE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os pacotes de algoritmos de regressão\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Importa os pacotes de algoritmos de redes neurais (Keras)\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda,BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import Callback,EarlyStopping,ModelCheckpoint\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam, Nadam, RMSprop\n",
    "from keras import optimizers\n",
    "#from keras_lookahead import Lookahead\n",
    "#from keras_radam import RAdam\n",
    "\n",
    "# Importa pacotes do sklearn\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics as mtr\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, StandardScaler\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})\n",
    "if TRAIN_OFFLINE:\n",
    "    train = pd.read_csv('../data/train.csv', dtype={'WindSpeed': 'object'})\n",
    "    #new_ft = pd.read_csv('../data/nfl-sample-features.csv')\n",
    "else:\n",
    "    train = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = train[['GameId','PlayId','Yards']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strtoseconds(txt):\n",
    "    txt = txt.split(':')\n",
    "    ans = int(txt[0])*60 + int(txt[1]) + int(txt[2])/60\n",
    "    return ans\n",
    "\n",
    "def strtofloat(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "def get_time(x):\n",
    "    x = x.split(\":\")\n",
    "    return int(x[0])*60 + int(x[1])\n",
    "\n",
    "def map_weather(txt):\n",
    "    ans = 1\n",
    "    if pd.isna(txt):\n",
    "        return 0\n",
    "    if 'partly' in txt:\n",
    "        ans*=0.5\n",
    "    if 'climate controlled' in txt or 'indoor' in txt:\n",
    "        return ans*3\n",
    "    if 'sunny' in txt or 'sun' in txt:\n",
    "        return ans*2\n",
    "    if 'clear' in txt:\n",
    "        return ans\n",
    "    if 'cloudy' in txt:\n",
    "        return -ans\n",
    "    if 'rain' in txt or 'rainy' in txt:\n",
    "        return -2*ans\n",
    "    if 'snow' in txt:\n",
    "        return -3*ans\n",
    "    return 0\n",
    "\n",
    "def OffensePersonnelSplit(x):\n",
    "    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0, 'QB' : 0, 'RB' : 0, 'TE' : 0, 'WR' : 0}\n",
    "    for xx in x.split(\",\"):\n",
    "        xxs = xx.split(\" \")\n",
    "        dic[xxs[-1]] = int(xxs[-2])\n",
    "    return dic\n",
    "\n",
    "def DefensePersonnelSplit(x):\n",
    "    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0}\n",
    "    for xx in x.split(\",\"):\n",
    "        xxs = xx.split(\" \")\n",
    "        dic[xxs[-1]] = int(xxs[-2])\n",
    "    return dic\n",
    "\n",
    "def orientation_to_cat(x):\n",
    "    x = np.clip(x, 0, 360 - 1)\n",
    "    try:\n",
    "        return str(int(x/15))\n",
    "    except:\n",
    "        return \"nan\"    \n",
    "    \n",
    "def split_personnel(s):\n",
    "    splits = s.split(',')\n",
    "    for i in range(len(splits)):\n",
    "        splits[i] = splits[i].strip()\n",
    "\n",
    "    return splits\n",
    "\n",
    "def defense_formation(l):\n",
    "    dl = 0\n",
    "    lb = 0\n",
    "    db = 0\n",
    "    other = 0\n",
    "\n",
    "    for position in l:\n",
    "        sub_string = position.split(' ')\n",
    "        if sub_string[1] == 'DL':\n",
    "            dl += int(sub_string[0])\n",
    "        elif sub_string[1] in ['LB','OL']:\n",
    "            lb += int(sub_string[0])\n",
    "        else:\n",
    "            db += int(sub_string[0])\n",
    "\n",
    "    counts = (dl,lb,db,other)\n",
    "\n",
    "    return counts\n",
    "\n",
    "def offense_formation(l):\n",
    "    qb = 0\n",
    "    rb = 0\n",
    "    wr = 0\n",
    "    te = 0\n",
    "    ol = 0\n",
    "\n",
    "    sub_total = 0\n",
    "    qb_listed = False\n",
    "    for position in l:\n",
    "        sub_string = position.split(' ')\n",
    "        pos = sub_string[1]\n",
    "        cnt = int(sub_string[0])\n",
    "\n",
    "        if pos == 'QB':\n",
    "            qb += cnt\n",
    "            sub_total += cnt\n",
    "            qb_listed = True\n",
    "        # Assuming LB is a line backer lined up as full back\n",
    "        elif pos in ['RB','LB']:\n",
    "            rb += cnt\n",
    "            sub_total += cnt\n",
    "        # Assuming DB is a defensive back and lined up as WR\n",
    "        elif pos in ['WR','DB']:\n",
    "            wr += cnt\n",
    "            sub_total += cnt\n",
    "        elif pos == 'TE':\n",
    "            te += cnt\n",
    "            sub_total += cnt\n",
    "        # Assuming DL is a defensive lineman lined up as an additional line man\n",
    "        else:\n",
    "            ol += cnt\n",
    "            sub_total += cnt\n",
    "\n",
    "    # If not all 11 players were noted at given positions we need to make some assumptions\n",
    "    # I will assume if a QB is not listed then there was 1 QB on the play\n",
    "    # If a QB is listed then I'm going to assume the rest of the positions are at OL\n",
    "    # This might be flawed but it looks like RB, TE and WR are always listed in the personnel\n",
    "    if sub_total < 11:\n",
    "        diff = 11 - sub_total\n",
    "        if not qb_listed:\n",
    "            qb += 1\n",
    "            diff -= 1\n",
    "        ol += diff\n",
    "\n",
    "    counts = (qb,rb,wr,te,ol)\n",
    "\n",
    "    return counts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_01(df, deploy=False):\n",
    "    def new_X(x_coordinate, play_direction):\n",
    "        if play_direction == 'left':\n",
    "            return 120.0 - x_coordinate\n",
    "        else:\n",
    "            return x_coordinate\n",
    "\n",
    "    def new_line(rush_team, field_position, yardline):\n",
    "        if rush_team == field_position:\n",
    "            # offense starting at X = 0 plus the 10 yard endzone plus the line of scrimmage\n",
    "            return 10.0 + yardline\n",
    "        else:\n",
    "            # half the field plus the yards between midfield and the line of scrimmage\n",
    "            return 60.0 + (50 - yardline)\n",
    "\n",
    "    def new_orientation(angle, play_direction):\n",
    "        if play_direction == 'left':\n",
    "            new_angle = 360.0 - angle\n",
    "            if new_angle == 360.0:\n",
    "                new_angle = 0.0\n",
    "            return new_angle\n",
    "        else:\n",
    "            return angle\n",
    "\n",
    "    def euclidean_distance(x1,y1,x2,y2):\n",
    "        x_diff = (x1-x2)**2\n",
    "        y_diff = (y1-y2)**2\n",
    "        return np.sqrt(x_diff + y_diff)\n",
    "\n",
    "    def back_direction(orientation):\n",
    "        if orientation > 180.0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def velocity(x2, x1, sec) :\n",
    "        return (x2 - x1) / sec\n",
    "    \n",
    "    def diff_x(b, c, theta) :\n",
    "        if 90.0 < theta < 270.0 :\n",
    "            return np.sqrt(((b ** 2) + (c ** 2)) - 2 * b * c * np.cos(theta))\n",
    "        else :\n",
    "            return 0\n",
    "        \n",
    "    def diff_y(b, c, theta) :\n",
    "        if theta <= 90.0 and theta >= 270.0 :\n",
    "            return - np.sqrt(((b ** 2) + (c ** 2)) - 2 * b * c * np.cos(theta))\n",
    "        else :\n",
    "            return 0\n",
    "        \n",
    "    def stop_period(speed, acc) :\n",
    "        return speed / acc   \n",
    "\n",
    "    def new_roll_velocity(x1, y1, x2, y2) :  \n",
    "        x_diff = np.sqrt((x1 - x2) ** 2)\n",
    "        y_diff = np.sqrt((y1 - y2) ** 2)\n",
    "        return np.sqrt(x_diff + y_diff) \n",
    "    \n",
    "    def update_yardline(df):\n",
    "        new_yardline = df[df['NflId'] == df['NflIdRusher']]\n",
    "        new_yardline['YardLine'] = new_yardline[['PossessionTeam','FieldPosition','YardLine']].apply(lambda x: new_line(x[0],x[1],x[2]), axis=1)\n",
    "        new_yardline = new_yardline[['GameId','PlayId','YardLine']]\n",
    "        return new_yardline\n",
    "\n",
    "    def update_orientation(df, yardline):\n",
    "        df['X'] = df[['X','PlayDirection']].apply(lambda x: new_X(x[0],x[1]), axis=1)\n",
    "        df['Orientation'] = df[['Orientation','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n",
    "        df['Dir'] = df[['Dir','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n",
    "        df = df.drop('YardLine', axis=1)\n",
    "        df = pd.merge(df, yardline, on=['GameId','PlayId'], how='inner')\n",
    "        return df\n",
    "\n",
    "    def back_features(df):\n",
    "        carriers = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','NflIdRusher','X','Y','Orientation','Dir','YardLine']]\n",
    "        carriers['back_from_scrimmage'] = carriers['YardLine'] - carriers['X']\n",
    "        carriers['back_oriented_down_field'] = carriers['Orientation'].apply(lambda x: back_direction(x))\n",
    "        carriers['back_moving_down_field'] = carriers['Dir'].apply(lambda x: back_direction(x))\n",
    "        carriers = carriers.rename(columns={'X':'back_X','Y':'back_Y'})\n",
    "        carriers = carriers[['GameId','PlayId','NflIdRusher','back_X','back_Y','back_from_scrimmage','back_oriented_down_field','back_moving_down_field']]\n",
    "        return carriers\n",
    "\n",
    "    def features_relative_to_back(df, carriers):\n",
    "        player_distance = df[['GameId','PlayId','NflId','X','Y']]\n",
    "        player_distance = pd.merge(player_distance, carriers, on=['GameId','PlayId'], how='inner')\n",
    "        player_distance = player_distance[player_distance['NflId'] != player_distance['NflIdRusher']]\n",
    "        player_distance['dist_to_back'] = player_distance[['X','Y','back_X','back_Y']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n",
    "\n",
    "        player_distance = player_distance.groupby(['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field'])\\\n",
    "                                         .agg({'dist_to_back':['min','max','mean','std']})\\\n",
    "                                         .reset_index()\n",
    "        player_distance.columns = ['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field',\n",
    "                                   'min_dist','max_dist','mean_dist','std_dist']\n",
    "\n",
    "        return player_distance\n",
    "\n",
    "    def defense_features(df):\n",
    "        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','Y']]\n",
    "        rusher.columns = ['GameId','PlayId','RusherTeam','RusherX','RusherY']\n",
    "\n",
    "        defense = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n",
    "        defense = defense[defense['Team'] != defense['RusherTeam']][['GameId','PlayId','X','Y','RusherX','RusherY']]\n",
    "        defense['def_dist_to_back'] = defense[['X','Y','RusherX','RusherY']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n",
    "\n",
    "        defense = defense.groupby(['GameId','PlayId'])\\\n",
    "                         .agg({'def_dist_to_back':['min','max','mean','std']})\\\n",
    "                         .reset_index()\n",
    "        defense.columns = ['GameId','PlayId','def_min_dist','def_max_dist','def_mean_dist','def_std_dist']\n",
    "\n",
    "        return defense\n",
    "\n",
    "    \n",
    "    def crawl_dist(df) :\n",
    "        crawl = df[df['NflId'] == df['NflIdRusher']][['X', 'Y', 'Dir', 'GameId', 'NflIdRusher', 'PlayId']]\n",
    "        crawl['Crawller_X'] = crawl[['X', 'Y', 'Dir']].apply(lambda x : diff_x(x[0], x[1], x[2]), axis = 1)\n",
    "        crawl['Crawller_y'] = crawl[['X', 'Y', 'Dir']].apply(lambda x : diff_y(x[0], x[1], x[2]), axis = 1)\n",
    "        crawl = crawl[['PlayId', 'GameId', 'Crawller_X', 'Crawller_y', 'NflIdRusher']]\n",
    "        return crawl\n",
    "    \n",
    "    def crawl_sec(df) :\n",
    "        crawls = df[df['NflId'] == df['NflIdRusher']][['A', 'S', 'PlayId', 'GameId', 'NflIdRusher']]\n",
    "        crawls['Crawller_second'] = crawls[['S', 'A']].apply(lambda x : stop_period(x[0], x[1]), axis = 1)\n",
    "        crawls = crawls[['GameId', 'NflIdRusher', 'PlayId', 'Crawller_second']]\n",
    "        return crawls\n",
    "        \n",
    "    def crawlling_velocity(df, crawl, crawls) :\n",
    "        player_on_pitch = df[df['NflId'] ==  df['NflIdRusher']][['X', 'Y', 'PlayId', 'GameId', 'NflIdRusher']]\n",
    "        player_on_pitch = pd.merge(player_on_pitch, crawl, on = ['GameId', 'PlayId'], how = 'inner')\n",
    "        player_on_pitch = pd.merge(player_on_pitch, crawls, on = ['GameId', 'PlayId'], how = 'inner')\n",
    "        player_on_pitch['Velocity_X'] = player_on_pitch[['X', 'Crawller_X', 'Crawller_second']].apply(lambda x : velocity(x[0], x[1], x[2]), axis = 1)\n",
    "        player_on_pitch['Velocity_y'] = player_on_pitch[['Y', 'Crawller_y', 'Crawller_second']].apply(lambda x : velocity(x[0], x[1], x[2]), axis = 1)        \n",
    "        player_on_pitch['Velocity_percent'] = player_on_pitch[['Velocity_X', 'Velocity_y', 'X', 'Y']].apply(lambda x : new_roll_velocity(x[0], x[1], x[2], x[3]), axis = 1)\n",
    "        \n",
    "        player_on_pitch = player_on_pitch.groupby(['GameId','PlayId', 'Velocity_X', 'Velocity_y', 'Crawller_X',\n",
    "                                                   'Crawller_y', 'Crawller_second'])\\\n",
    "                                         .agg({'Velocity_percent' : ['min','max','mean']})\\\n",
    "                                         .reset_index()\n",
    "        \n",
    "        player_on_pitch.columns = ['GameId', 'PlayId', 'min_velocity_percent', 'max_velocity_percrent', 'mean_velocity_percent',\n",
    "                                   'Velocity_X', 'Velocity_y', 'Crawller_X', 'Crawller_y', 'Crawller_second']\n",
    "        \n",
    "        return player_on_pitch\n",
    "    \n",
    "    \n",
    "    def static_features(df):\n",
    "        \n",
    "        \n",
    "        add_new_feas = []\n",
    "\n",
    "        ## Height\n",
    "        df['PlayerHeight_dense'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n",
    "        add_new_feas.append('PlayerHeight_dense')\n",
    "\n",
    "        ## Time\n",
    "        df['TimeHandoff'] = df['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "        df['TimeSnap'] = df['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "\n",
    "        df['TimeDelta'] = df.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n",
    "        df['PlayerBirthDate'] =df['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m/%d/%Y\"))\n",
    "\n",
    "        ## Age\n",
    "        seconds_in_year = 60*60*24*365.25\n",
    "        df['PlayerAge'] = df.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()/seconds_in_year, axis=1)\n",
    "        add_new_feas.append('PlayerAge')\n",
    "\n",
    "        ## BMI\n",
    "        df['BMI'] = df['PlayerWeight'] / df['PlayerHeight_dense']\n",
    "        add_new_feas.append('BMI')\n",
    "        \n",
    "        ## WindSpeed\n",
    "        df['WindSpeed_ob'] = df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n",
    "        df['WindSpeed_ob'] = df['WindSpeed_ob'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n",
    "        df['WindSpeed_ob'] = df['WindSpeed_ob'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n",
    "        df['WindSpeed_dense'] = df['WindSpeed_ob'].apply(strtofloat)\n",
    "        add_new_feas.append('WindSpeed_dense')\n",
    "\n",
    "        ## Weather\n",
    "        df['GameWeather_process'] = df['GameWeather'].str.lower()\n",
    "        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: \"indoor\" if not pd.isna(x) and \"indoor\" in x else x)\n",
    "        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\n",
    "        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\n",
    "        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n",
    "        df['GameWeather_dense'] = df['GameWeather_process'].apply(map_weather)\n",
    "        add_new_feas.append('GameWeather_dense')\n",
    "\n",
    "        ## Orientation and Dir\n",
    "        df[\"Orientation_ob\"] = df[\"Orientation\"].apply(lambda x : orientation_to_cat(x)).astype(\"object\")\n",
    "        df[\"Dir_ob\"] = df[\"Dir\"].apply(lambda x : orientation_to_cat(x)).astype(\"object\")\n",
    "\n",
    "        df[\"Orientation_sin\"] = df[\"Orientation\"].apply(lambda x : np.sin(x/360 * 2 * np.pi))\n",
    "        df[\"Orientation_cos\"] = df[\"Orientation\"].apply(lambda x : np.cos(x/360 * 2 * np.pi))\n",
    "        \n",
    "        df[\"Dir_sin\"] = df[\"Dir\"].apply(lambda x : np.sin(x/360 * 2 * np.pi))\n",
    "        df[\"Dir_cos\"] = df[\"Dir\"].apply(lambda x : np.cos(x/360 * 2 * np.pi))\n",
    "        \n",
    "        add_new_feas.append(\"Dir_sin\")\n",
    "        add_new_feas.append(\"Dir_cos\")\n",
    "\n",
    "        ## diff Score\n",
    "        df[\"diffScoreBeforePlay\"] = df[\"HomeScoreBeforePlay\"] - df[\"VisitorScoreBeforePlay\"]\n",
    "        add_new_feas.append(\"diffScoreBeforePlay\")\n",
    "    \n",
    "        static_features = df[df['NflId'] == df['NflIdRusher']][add_new_feas+['GameId','PlayId','X','Y','S','A','Dis','Orientation','Dir',\n",
    "                                                                             'YardLine','Quarter','Down','Distance','DefendersInTheBox',\n",
    "                                                                             'NflId','NflIdRusher','PossessionTeam','HomeTeamAbbr','Turf',\n",
    "                                                                             'VisitorTeamAbbr','PlayDirection','GameClock','Season','Team',\n",
    "                                                                             'FieldPosition']].drop_duplicates()\n",
    "        static_features.fillna(-999,inplace=True)\n",
    "\n",
    "        return static_features\n",
    "\n",
    "\n",
    "    def personnel_features(df):\n",
    "        personnel = df[['GameId','PlayId','OffensePersonnel','DefensePersonnel']].drop_duplicates()\n",
    "        personnel['DefensePersonnel'] = personnel['DefensePersonnel'].apply(lambda x: split_personnel(x))\n",
    "        personnel['DefensePersonnel'] = personnel['DefensePersonnel'].apply(lambda x: defense_formation(x))\n",
    "        personnel['num_DL'] = personnel['DefensePersonnel'].apply(lambda x: x[0])\n",
    "        personnel['num_LB'] = personnel['DefensePersonnel'].apply(lambda x: x[1])\n",
    "        personnel['num_DB'] = personnel['DefensePersonnel'].apply(lambda x: x[2])\n",
    "\n",
    "        personnel['OffensePersonnel'] = personnel['OffensePersonnel'].apply(lambda x: split_personnel(x))\n",
    "        personnel['OffensePersonnel'] = personnel['OffensePersonnel'].apply(lambda x: offense_formation(x))\n",
    "        personnel['num_QB'] = personnel['OffensePersonnel'].apply(lambda x: x[0])\n",
    "        personnel['num_RB'] = personnel['OffensePersonnel'].apply(lambda x: x[1])\n",
    "        personnel['num_WR'] = personnel['OffensePersonnel'].apply(lambda x: x[2])\n",
    "        personnel['num_TE'] = personnel['OffensePersonnel'].apply(lambda x: x[3])\n",
    "        personnel['num_OL'] = personnel['OffensePersonnel'].apply(lambda x: x[4])\n",
    "\n",
    "        # Let's create some features to specify if the OL is covered\n",
    "        personnel['OL_diff'] = personnel['num_OL'] - personnel['num_DL']\n",
    "        personnel['OL_TE_diff'] = (personnel['num_OL'] + personnel['num_TE']) - personnel['num_DL']\n",
    "        # Let's create a feature to specify if the defense is preventing the run\n",
    "        # Let's just assume 7 or more DL and LB is run prevention\n",
    "        personnel['run_def'] = (personnel['num_DL'] + personnel['num_LB'] > 6).astype(int)\n",
    "\n",
    "        personnel.drop(['OffensePersonnel','DefensePersonnel'], axis=1, inplace=True)\n",
    "        \n",
    "        return personnel\n",
    "    \n",
    "    def combine_features(relative_to_back, player_on_pitch, defense, static, personnel, deploy=deploy):\n",
    "        df = pd.merge(relative_to_back,defense,on=['GameId','PlayId'],how='inner')\n",
    "        df = pd.merge(df,static,on=['GameId','PlayId'],how='inner')\n",
    "        df = pd.merge(df, player_on_pitch, on = ['GameId', 'PlayId'], how = 'inner')\n",
    "        df = pd.merge(df,personnel,on=['GameId','PlayId'],how='inner')\n",
    "        \n",
    "        if not deploy:\n",
    "            df = pd.merge(df, outcomes, on=['GameId','PlayId'], how='inner')\n",
    "\n",
    "        return df\n",
    "    \n",
    "    yardline = update_yardline(df)\n",
    "    df = update_orientation(df, yardline)\n",
    "    \n",
    "    crawlling_dist = crawl_dist(df)\n",
    "    crawlling_second = crawl_sec(df)\n",
    "    crawlling_velocity_diff = crawlling_velocity(df, crawlling_dist, crawlling_second)\n",
    "    \n",
    "    \n",
    "    back_feats = back_features(df)\n",
    "    rel_back = features_relative_to_back(df, back_feats)\n",
    "    def_feats = defense_features(df)\n",
    "    static_feats = static_features(df)\n",
    "    personnel = personnel_features(df)\n",
    "    basetable = combine_features(rel_back, def_feats, crawlling_velocity_diff, static_feats, personnel, deploy = deploy)\n",
    "    \n",
    "    return basetable\n",
    "\n",
    "\n",
    "def uid_aggregation(comb, main_columns, uids, aggregations):\n",
    "    X = pd.DataFrame()\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_'+agg_type\n",
    "                temp_df = comb[[col, main_column]]\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                X[new_col_name] = comb[col].map(temp_df)\n",
    "                del temp_df\n",
    "                gc.collect()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_02(t_):\n",
    "    t_['fe1'] = pd.Series(np.sqrt(np.absolute(np.square(t_.X.values) - np.square(t_.Y.values))))\n",
    "    t_['fe5'] = np.square(t_['S'].values) + 2 * t_['A'].values * t_['Dis'].values  # N\n",
    "    t_['fe7'] = np.arccos(np.clip(t_['X'].values / t_['Y'].values, -1, 1))  # N\n",
    "    t_['fe8'] = t_['S'].values / np.clip(t_['fe1'].values, 0.6, None)\n",
    "    radian_angle = (90 - t_['Dir']) * np.pi / 180.0\n",
    "    t_['fe10'] = np.abs(t_['S'] * np.cos(radian_angle))\n",
    "    t_['fe11'] = np.abs(t_['S'] * np.sin(radian_angle))\n",
    "    \n",
    "    t_['IsRusher'] = (t_['NflId'] == t_['NflIdRusher'])\n",
    "    temp = t_[t_[\"IsRusher\"]][[\"Team\", \"PlayId\"]].rename(columns={\"Team\":\"RusherTeam\"})\n",
    "    t_ = t_.merge(temp, on = \"PlayId\")\n",
    "    t_[\"IsRusherTeam\"] = t_[\"Team\"] == t_[\"RusherTeam\"]\n",
    "    \n",
    "    #t_[\"is_rusher\"]          = 1.0*(t_[\"NflId\"] == t_[\"NflIdRusher\"])\n",
    "    #t_[\"is_home\"]            = t_[\"Team\"] == \"home\"\n",
    "    t_[\"is_possession_team\"] = 1.0*(t_[\"PossessionTeam\"] == t_[\"HomeTeamAbbr\"]) - 1.0*(t_[\"PossessionTeam\"] == t_[\"VisitorTeamAbbr\"])\n",
    "    t_[\"is_field_team\"]      = 1.0*(t_[\"FieldPosition\"] == t_[\"HomeTeamAbbr\"]) - 1.0*(t_[\"FieldPosition\"] == t_[\"VisitorTeamAbbr\"])\n",
    "    t_[\"is_left\"]            = t_[\"PlayDirection\"] == \"left\"\n",
    "    \n",
    "    t_[\"game_time\"]   = t_[\"GameClock\"].apply(get_time)\n",
    "    t_[\"old_data\"]    = t_[\"Season\"] == 2017\n",
    "    t_['YardLine_std'] = 100 - t_['YardLine']\n",
    "    \n",
    "    \n",
    "    Turf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n",
    "            'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n",
    "            'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n",
    "            'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n",
    "            'SISGrass':'Artificial', 'Twenty-Four/Seven Turf':'Artificial', 'natural grass':'Natural'} \n",
    "\n",
    "    t_['Turf'] = t_['Turf'].map(Turf)\n",
    "    t_['Turf'] = t_['Turf'] == 'Natural'\n",
    "    \n",
    "    t_['X_std'] = t_['X']\n",
    "    t_.loc[t_['is_left'], 'X_std'] = 120 - t_.loc[t_['is_left'], 'X'] \n",
    "    t_['Y_std'] = t_['Y']\n",
    "    t_.loc[t_['is_left'], 'Y_std'] = 160/3 - t_.loc[t_['is_left'], 'Y'] \n",
    "    \n",
    "    \n",
    "    t_['Orientation_rad'] = np.mod(t_.Orientation, 360) * np.pi/180.0\n",
    "    t_.loc[t_.Season >= 2018, 'Orientation_rad'] = np.mod(t_.loc[t_.Season >= 2018, 'Orientation'] - 90, 360) * np.pi/180.0\n",
    "    t_['Orientation_std'] = t_.Orientation_rad\n",
    "    t_.loc[t_.is_left, 'Orientation_std'] = np.mod(np.pi + t_.loc[t_.is_left, 'Orientation_rad'], 2*np.pi)\n",
    "\n",
    "    \n",
    "    \n",
    "    t_[\"Start\"] = t_[\"YardLine\"]\n",
    "    t_['PlayDirection_new'] = t_['PlayDirection'].map({'right': 1, 'left': -1})\n",
    "    t_['Orientation_new'] = 2 * np.pi * (90 - t_['Orientation']) / 360\n",
    "    t_['locX'] = (t_['X'].values - t_['Start'].values) * t_['PlayDirection_new'].values\n",
    "    t_['locY'] = t_['Y'].values - 53.3 / 2\n",
    "    t_['velX'] = t_['S'].values * np.cos(t_['Orientation_new'].values) * t_['PlayDirection_new'].values\n",
    "    t_['velY'] = t_['S'].values * np.sin(t_['Orientation_new'].values)\n",
    "    t_['accX'] = t_['A'].values * np.cos(t_['Orientation_new'].values) * t_['PlayDirection_new'].values\n",
    "    t_['accY'] = t_['A'].values * np.sin(t_['Orientation_new'].values)\n",
    "    \n",
    "    \n",
    "    t_['YardsFromOwnGoal'] = np.where(t_.FieldPosition == t_.PossessionTeam,t_.YardLine, 50 + (50-t_.YardLine))\n",
    "    t_[['prev_game', 'prev_play', 'prev_team', 'prev_yfog']] = t_[['GameId', 'PlayId', 'Team', 'YardsFromOwnGoal']].shift(1)\n",
    "\n",
    "    filt = (t_.GameId==t_.prev_game) & (t_.Team==t_.prev_team) & (t_.PlayId-t_.prev_play<30)\n",
    "    t_.loc[filt,'est_prev_yards'] = t_[filt]['YardsFromOwnGoal'] - t_[filt]['prev_yfog']\n",
    "\n",
    "    \n",
    "    t_['norm_quat'] = (t_['X']**2 + t_['Y']**2 + t_['A']**2 + t_['S']**2)\n",
    "    t_['mod_quat'] = (t_['norm_quat'])**0.5\n",
    "    t_['norm_X'] = t_['X'] / t_['mod_quat']\n",
    "    t_['norm_Y'] = t_['Y'] / t_['mod_quat']\n",
    "    t_['norm_A'] = t_['A'] / t_['mod_quat']\n",
    "    t_['norm_S'] = t_['S'] / t_['mod_quat']    \n",
    "\n",
    "    i_cols = ['YardLine']\n",
    "    uids = ['X']\n",
    "    aggregations = ['mean','std','median','max','min']\n",
    "    X_agg = uid_aggregation(t_, i_cols, uids, aggregations)\n",
    "    t_ = pd.concat([t_, X_agg], axis=1)\n",
    "\n",
    "    t_ = t_.sort_values(by = ['X']).sort_values(by = ['Dis']).sort_values(by=['PlayId', 'IsRusherTeam', 'IsRusher']).reset_index(drop = True)\n",
    "\n",
    "\n",
    "    return t_\n",
    "\n",
    "def logs(res, ls):\n",
    "    m = res.shape[1]\n",
    "    for l in ls:\n",
    "        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n",
    "        res.columns.values[m] = l + '_log'\n",
    "        m += 1\n",
    "    return res\n",
    "\n",
    "def squares(res, ls):\n",
    "    m = res.shape[1]\n",
    "    for l in ls:\n",
    "        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n",
    "        res.columns.values[m] = l + '_sq'\n",
    "        m += 1\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_basetable = create_features_01(train, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_basetable = create_features_02(train_basetable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basetable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria as novas features (etapa 03)\n",
    "log_features = ['back_from_scrimmage','X','Y','S','A','Dis','Orientation','Dir','YardLine','PlayerAge']\n",
    "train_basetable = logs(train_basetable, log_features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Cria as novas features (etapa 04)\n",
    "squared_features = ['X','Y','S','A','Dis','Orientation','Dir','YardLine','player_height','PlayerAge','game_time']\n",
    "train_basetable = squares(train_basetable, squared_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformação de variaveis categoricas para numericas usando LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le_dict = {}\n",
    "categoricals = ['Team_le','PossessionTeam_le','HomeTeamAbbr_le','VisitorTeamAbbr_le',\n",
    "                'FieldPosition_le','PlayDirection_le']\n",
    "\n",
    "for cat in categoricals:\n",
    "    le_dict[cat] = LabelEncoder()\n",
    "    train_basetable[cat] = le_dict[cat].fit_transform(train_basetable[cat[:-3]].apply(str))  \n",
    "\n",
    "# Remove as features originais que foram transformadas\n",
    "#X.drop(['TimeSnap','Team','PossessionTeam','HomeTeamAbbr','VisitorTeamAbbr','FieldPosition','PlayDirection'], axis=1, inplace=True)\n",
    "train_basetable.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma copia do dataset para backup\n",
    "X = train_basetable.copy()\n",
    "X.fillna(0,inplace=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['NflId','NflIdRusher','PossessionTeam','HomeTeamAbbr','Turf',\n",
    "                      'VisitorTeamAbbr','PlayDirection','GameClock','Season',\n",
    "                      'Team','FieldPosition','Crawller_X','Crawller_y','prev_game','prev_play','prev_team','RusherTeam'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Dataset for LOFO\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    target: string\n",
    "        Column name for target within df\n",
    "    features: list of strings\n",
    "        List of column names within df\n",
    "    feature_groups: dict, optional\n",
    "        Name, value dictionary of feature groups as numpy.darray or scipy.csr.scr_matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, target, features, feature_groups=None):\n",
    "        self.df = df.copy()\n",
    "        self.features = list(features)\n",
    "        self.feature_groups = feature_groups if feature_groups else dict()\n",
    "\n",
    "        self.num_rows = df.shape[0]\n",
    "        self.y = df[target].values\n",
    "\n",
    "        for feature_name, feature_matrix in self.feature_groups.items():\n",
    "            if not (isinstance(feature_matrix, np.ndarray) or isinstance(feature_matrix, ss.csr.csr_matrix)):\n",
    "                raise Exception(\"Data type {dtype} is not a valid type!\".format(dtype=type(feature_matrix)))\n",
    "\n",
    "            if feature_matrix.shape[0] != self.num_rows:\n",
    "                raise Exception(\"Expected {expected} rows but got {n} rows!\".format(expected=self.num_rows,\n",
    "                                                                                    n=feature_matrix.shape[0]))\n",
    "\n",
    "            if feature_name in self.features:\n",
    "                raise Exception(\"Feature group name '{name}' is the same with one of the features!\")\n",
    "\n",
    "    def getX(self, feature_to_remove, fit_params):\n",
    "        \"\"\"Get feature matrix and fit_params after removing a feature\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_to_remove : string\n",
    "            feature name to remove\n",
    "        fit_params : dict\n",
    "            fit parameters for the model\n",
    "        Returns\n",
    "        -------\n",
    "        X : numpy.darray or scipy.csr.scr_matrix\n",
    "            Feature matrix\n",
    "        fit_params: dict\n",
    "            Updated fit_params after feature removal\n",
    "        \"\"\"\n",
    "        feature_list = [feature for feature in self.features if feature != feature_to_remove]\n",
    "        concat_list = [self.df[feature_list].values]\n",
    "\n",
    "        for feature_name, feature_matrix in self.feature_groups.items():\n",
    "            if feature_name != feature_to_remove:\n",
    "                concat_list.append(feature_matrix)\n",
    "\n",
    "        fit_params = fit_params.copy()\n",
    "        if \"categorical_feature\" in fit_params:\n",
    "            cat_features = [f for f in fit_params[\"categorical_feature\"] if f != feature_to_remove]\n",
    "            fit_params[\"categorical_feature\"] = [ix for ix, f in enumerate(feature_list) if (f in cat_features)]\n",
    "\n",
    "        has_sparse = False\n",
    "        for feature_name, feature_matrix in self.feature_groups.items():\n",
    "            if feature_name != feature_to_remove and isinstance(feature_matrix, ss.csr.csr_matrix):\n",
    "                has_sparse = True\n",
    "\n",
    "        concat = np.hstack\n",
    "        if has_sparse:\n",
    "            concat = ss.hstack\n",
    "\n",
    "        return concat(concat_list), fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(importance_df, figsize=(8, 8)):\n",
    "    \"\"\"Plot feature importance\n",
    "    Parameters\n",
    "    ----------\n",
    "    importance_df : pandas dataframe\n",
    "        Output dataframe from LOFO/FLOFO get_importance\n",
    "    figsize : tuple\n",
    "    \"\"\"\n",
    "    importance_df = importance_df.copy()\n",
    "    importance_df[\"color\"] = (importance_df[\"importance_mean\"] > 0).map({True: 'g', False: 'r'})\n",
    "    importance_df.sort_values(\"importance_mean\", inplace=True)\n",
    "\n",
    "    importance_df.plot(x=\"feature\", y=\"importance_mean\", xerr=\"importance_std\",\n",
    "                       kind='barh', color=importance_df[\"color\"], figsize=figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import multiprocessing\n",
    "#from lofo.infer_defaults import infer_model\n",
    "\n",
    "\n",
    "class LOFOImportance:\n",
    "    \"\"\"\n",
    "    Leave One Feature Out Importance\n",
    "    Given a model and cross-validation scheme, calculates the feature importances.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: LOFO Dataset object\n",
    "    scoring: string or callable\n",
    "        Same as scoring in sklearn API\n",
    "    model: model (sklearn API), optional\n",
    "        Not trained model object\n",
    "    fit_params : dict, optional\n",
    "        fit parameters for the model\n",
    "    cv: int or iterable\n",
    "        Same as cv in sklearn API\n",
    "    n_jobs: int, optional\n",
    "        Number of jobs for parallel computation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, scoring, model=None, fit_params=None, cv=4, n_jobs=None):\n",
    "\n",
    "        self.fit_params = fit_params if fit_params else dict()\n",
    "        if model is None:\n",
    "            model, dataset.df, categoricals, dataset.y = infer_model(dataset.df, dataset.features, dataset.y, n_jobs)\n",
    "            self.fit_params[\"categorical_feature\"] = categoricals\n",
    "            n_jobs = 1\n",
    "\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.scoring = scoring\n",
    "        self.cv = cv\n",
    "        self.n_jobs = n_jobs\n",
    "        if self.n_jobs is not None and self.n_jobs > 1:\n",
    "            warning_str = (\"Warning: If your model is multithreaded, please initialise the number\"\n",
    "                           \"of jobs of LOFO to be equal to 1, otherwise you may experience performance issues.\")\n",
    "            warnings.warn(warning_str)\n",
    "\n",
    "    def _get_cv_score(self, feature_to_remove):\n",
    "        X, fit_params = self.dataset.getX(feature_to_remove=feature_to_remove, fit_params=self.fit_params)\n",
    "        y = self.dataset.y\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            cv_results = cross_validate(self.model, X, y, cv=self.cv, scoring=self.scoring, fit_params=fit_params)\n",
    "        return cv_results['test_score']\n",
    "\n",
    "    def _get_cv_score_parallel(self, feature, result_queue):\n",
    "        test_score = self._get_cv_score(feature_to_remove=feature)\n",
    "        result_queue.put((feature, test_score))\n",
    "        return test_score\n",
    "\n",
    "    def get_importance(self):\n",
    "        \"\"\"Run LOFO to get feature importances\n",
    "        Returns\n",
    "        -------\n",
    "        importance_df : pandas dataframe\n",
    "            Dataframe with feature names and corresponding importance mean and std (sorted by importance)\n",
    "        \"\"\"\n",
    "        base_cv_score = self._get_cv_score(feature_to_remove=None)\n",
    "        feature_list = self.dataset.features + list(self.dataset.feature_groups.keys())\n",
    "\n",
    "        if self.n_jobs is not None and self.n_jobs > 1:\n",
    "\n",
    "            pool = multiprocessing.Pool(self.n_jobs)\n",
    "            manager = multiprocessing.Manager()\n",
    "            result_queue = manager.Queue()\n",
    "\n",
    "            for f in feature_list:\n",
    "                pool.apply_async(self._get_cv_score_parallel, (f, result_queue))\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            lofo_cv_result = [result_queue.get() for _ in range(len(feature_list))]\n",
    "            lofo_cv_scores_normalized = np.array([base_cv_score - lofo_cv_score for _, lofo_cv_score in lofo_cv_result])\n",
    "            feature_list = [feature for feature, _ in lofo_cv_result]\n",
    "        else:\n",
    "            lofo_cv_scores = []\n",
    "            for f in tqdm_notebook(feature_list):\n",
    "                lofo_cv_scores.append(self._get_cv_score(feature_to_remove=f))\n",
    "\n",
    "            lofo_cv_scores_normalized = np.array([base_cv_score - lofo_cv_score for lofo_cv_score in lofo_cv_scores])\n",
    "\n",
    "        importance_df = pd.DataFrame()\n",
    "        importance_df[\"feature\"] = feature_list\n",
    "        importance_df[\"importance_mean\"] = lofo_cv_scores_normalized.mean(axis=1)\n",
    "        importance_df[\"importance_std\"] = lofo_cv_scores_normalized.std(axis=1)\n",
    "\n",
    "        return importance_df.sort_values(\"importance_mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedimento para verificar as features mais importantes\n",
    "# Usando LightGBM para treinamento\n",
    "#from lofo import LOFOImportance, Dataset, plot_importance\n",
    "\n",
    "n_folds = 3\n",
    "kfold_lgb = KFold(n_folds, shuffle=True)\n",
    "\n",
    "features = [x for x in X.columns if x not in ['Yards','GameId','PlayId']]\n",
    "\n",
    "params2 = {'num_leaves': 15,\n",
    "          'objective': 'mae',\n",
    "          #'learning_rate': 0.1,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"num_rounds\": 100\n",
    "          }\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(**params2)\n",
    "dataset = Dataset(df=X, target=\"Yards\", features=features)\n",
    "lofo_imp = LOFOImportance(dataset, model=model_lgb, cv=kfold_lgb, scoring=\"neg_mean_absolute_error\")\n",
    "\n",
    "importance_df = lofo_imp.get_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo grafico com as features\n",
    "plot_importance(importance_df, figsize=(12, 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = importance_df.loc[importance_df['importance_mean'] > 0].feature\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um dataset somente com as colunas mais importantes conforme visto anteriormente\n",
    "new_X = X.copy()\n",
    "#new_X = X.loc[:,best_features]\n",
    "new_X.replace(-np.inf,0,inplace=True)\n",
    "new_X.replace(np.inf,0,inplace=True)\n",
    "\n",
    "# Normalizando as variaveis do dataset de treino\n",
    "scaler = StandardScaler()\n",
    "new_X2 = scaler.fit_transform(new_X)\n",
    "\n",
    "\n",
    "#new_X = X.drop(['GameId','PlayId','Yards'], axis=1)\n",
    "target = X.Yards\n",
    "\n",
    "y = np.zeros((target.shape[0], 199))\n",
    "for idx, target in enumerate(list(target)):\n",
    "    y[idx][99 + target] = 1\n",
    "    \n",
    "\n",
    "new_X2.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_clf = RandomForestRegressor(bootstrap=False, max_features=0.3, min_samples_leaf=15,\n",
    "                              min_samples_split=7, n_estimators=1000, n_jobs=-1, random_state=42)\n",
    "rf_clf.fit(new_X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = pd.DataFrame(new_X2, index=new_X.index, columns=new_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_ = SelectFromModel(rf_clf, prefit=True, threshold='0.2*mean')\n",
    "feature_idx = fs_.get_support()\n",
    "feature_name = ttt.columns[feature_idx]\n",
    "\n",
    "print(len(feature_name))\n",
    "print(feature_name)\n",
    "\n",
    "with open('fe_imp.txt', 'w') as f:\n",
    "    for item in feature_name:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Criar e avaliar alguns algoritmos de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Split Treino e Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_basetable.copy()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = ['Velocity_y', 'A', 'YardLine', 'def_min_dist', 'fe5', 'fe10',\n",
    "                 'fe11', 'YardLine_std', 'Start', 'YardsFromOwnGoal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = ['A','S','back_from_scrimmage','back_oriented_down_field','back_moving_down_field','old_data',\n",
    "                 'def_mean_dist','def_std_dist','def_min_dist','def_max_dist','min_dist','max_dist','mean_dist',\n",
    "                 'PlayerAge','PlayerHeight_dense','Dis','DefendersInTheBox',\n",
    "                 'Distance','Dir','Dir_sin','Dir_cos','YardLine_std','Orientation_std',\n",
    "                 'WindSpeed_dense','GameWeather_dense',\n",
    "                 'fe1','fe5','fe8','fe10','fe11',\n",
    "                 'norm_quat','mod_quat','norm_X','norm_Y','norm_A','norm_S',\n",
    "                 'X_YardLine_std','X_YardLine_median','X_YardLine_max','X_YardLine_min',\n",
    "                 'num_DL','num_LB','num_DB','num_QB','num_RB','num_WR','num_TE','num_OL']\n",
    "\n",
    "#'A_log','Y_std','Quarter','Dis_log','Orientation','Down',\n",
    "#                 'min_velocity_percent','max_velocity_percrent','mean_velocity_percent','X_log',\n",
    "#                 'Crawller_second','Team_le','is_home','PlayDirection_le','VisitorTeamAbbr_le','Orientation_std',\n",
    "#                 'Velocity_X','YardLine_log','is_rusher'\n",
    "# 'locX','locY','velX','velY','accX','accY'            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NFL_validation_split(df):\n",
    "    games = df[['GameId', 'PossessionTeam']].drop_duplicates()\n",
    "\n",
    "    # Sort so the latest games are first and label the games with cumulative counter\n",
    "    games = games.sort_values(['PossessionTeam', 'GameId'], ascending=[True, False])\n",
    "    games['row_number'] = games.groupby(['PossessionTeam']).cumcount() + 1\n",
    "\n",
    "    # Use last 5 games for each team as validation. There will be overlap since two teams will have the same\n",
    "    # GameId\n",
    "    game_set = set([1, 2, 3, 4])\n",
    "\n",
    "    # Set of unique game ids\n",
    "    game_ids = set(games[games['row_number'].isin(game_set)]['GameId'].unique().tolist())\n",
    "\n",
    "    return game_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basetable.fillna(0,inplace=True)\n",
    "game_ids = NFL_validation_split(train_basetable)\n",
    "\n",
    "X_train = train_basetable[~train_basetable['GameId'].isin(game_ids)]\n",
    "X_test = train_basetable[train_basetable['GameId'].isin(game_ids)]\n",
    "\n",
    "X_train = X_train.loc[:,best_features]\n",
    "X_test  = X_test.loc[:,best_features]\n",
    "\n",
    "train_inds, test_inds = X_train.index, X_test.index\n",
    "\n",
    "# Normalizando as variaveis do dataset de treino\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um dataset somente com as colunas mais importantes conforme visto anteriormente\n",
    "new_X = X.loc[:,best_features]\n",
    "#new_X.replace(-np.inf,0,inplace=True)\n",
    "#new_X.replace(np.inf,0,inplace=True)\n",
    "\n",
    "# Normalizando as variaveis do dataset de treino\n",
    "scaler = StandardScaler()\n",
    "new_X = scaler.fit_transform(new_X)\n",
    "\n",
    "#new_X = X.drop(['GameId','PlayId','Yards'], axis=1)\n",
    "target = X.Yards\n",
    "\n",
    "y = np.zeros((target.shape[0], 199))\n",
    "for idx, target in enumerate(list(target)):\n",
    "    y[idx][99 + target] = 1\n",
    "    \n",
    "\n",
    "new_X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Teste com LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Teste com Keras (New NN Struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRPSCallback(Callback):\n",
    "    \n",
    "    def __init__(self,validation, predict_batch_size=20, include_on_batch=False):\n",
    "        super(CRPSCallback, self).__init__()\n",
    "        self.validation = validation\n",
    "        self.predict_batch_size = predict_batch_size\n",
    "        self.include_on_batch = include_on_batch\n",
    "        \n",
    "        #print('validation shape',len(self.validation))\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not ('CRPS_score_val' in self.params['metrics']):\n",
    "            self.params['metrics'].append('CRPS_score_val')\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if (self.include_on_batch):\n",
    "            logs['CRPS_score_val'] = float('-inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['CRPS_score_val'] = float('-inf')\n",
    "            \n",
    "        if (self.validation):\n",
    "            X_valid, y_valid = self.validation[0], self.validation[1]\n",
    "            y_pred = self.model.predict(X_valid)\n",
    "            y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n",
    "            y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
    "            val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * X_valid.shape[0])\n",
    "            val_s = np.round(val_s, 8)\n",
    "            logs['CRPS_score_val'] = val_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(x_tr,y_tr,x_val,y_val):\n",
    "    inp = Input(shape = (x_tr.shape[1],))\n",
    "    x = Dense(1024, input_dim=X.shape[1], activation='relu')(inp)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    out = Dense(199, activation='softmax')(x)\n",
    "    model = Model(inp,out)\n",
    "    \n",
    "    model.compile(optimizer = optimizers.adam(lr = 0.001, decay = 1e-06),\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=[])\n",
    "     \n",
    "    es = EarlyStopping(monitor='CRPS_score_val', \n",
    "                       mode='min',\n",
    "                       restore_best_weights=True, \n",
    "                       verbose=1, \n",
    "                       patience=10)\n",
    "\n",
    "    mc = ModelCheckpoint('best_model.h5',monitor='CRPS_score_val',mode='min',save_best_only=True, \n",
    "                         verbose=1, save_weights_only=True)\n",
    "    \n",
    "    bsz = 1024\n",
    "    steps = x_tr.shape[0]/bsz\n",
    "    \n",
    "    model.fit(x_tr, y_tr,\n",
    "              callbacks=[CRPSCallback(validation = (x_val,y_val)),es,mc], \n",
    "              epochs=100, \n",
    "              #steps_per_epoch = steps,\n",
    "              batch_size=bsz,\n",
    "              verbose=1)\n",
    "    \n",
    "    model.load_weights(\"best_model.h5\")\n",
    "    \n",
    "    y_pred = model.predict(x_val)\n",
    "    y_valid = y_val\n",
    "    y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n",
    "    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
    "    val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * x_val.shape[0])\n",
    "    crps = np.round(val_s, 8)\n",
    "\n",
    "    return model,crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_te):\n",
    "    model_num = len(models)\n",
    "    for k,m in enumerate(models):\n",
    "        if k==0:\n",
    "            y_pred = m.predict(x_te,batch_size=1024)\n",
    "        else:\n",
    "            y_pred+=m.predict(x_te,batch_size=1024)\n",
    "            \n",
    "    y_pred = y_pred / model_num\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rounds = 10\n",
    "splits = 10\n",
    "\n",
    "for i in range(rounds):\n",
    "    gc.collect()\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits = splits, shuffle = True)\n",
    "\n",
    "    for train_index, test_index in kf.split(new_X):\n",
    "        X_train, X_test = new_X[train_index], new_X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    y_train = np.uint8(y_train)\n",
    "    y_test = np.uint8(y_test)\n",
    "\n",
    "        \n",
    "    model,crps = get_model(X_train,y_train,X_test,y_test)\n",
    "    models.append(model)\n",
    "    print(\"the %d fold crps is %f\"%((i+1),crps))\n",
    "    crps_csv.append(crps)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Parametros\n",
    "nbags = 10\n",
    "nfolds = 10\n",
    "kfold = KFold(nfolds, random_state = 42, shuffle = True)\n",
    "\n",
    "for k_fold, (train_index, test_index) in enumerate(kfold.split(new_X)):\n",
    "    gc.collect()\n",
    "    \n",
    "    X_train, X_test = new_X[train_index], new_X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    for j in range(nbags):\n",
    "\n",
    "        y_train = np.uint8(y_train)\n",
    "        y_test = np.uint8(y_test)\n",
    "\n",
    "        model,crps = get_model(X_train,y_train,X_test,y_test)\n",
    "        models.append(model)\n",
    "        \n",
    "        i += 1\n",
    "        print(\"the %d fold crps is %f\"%((k_fold+1),crps))\n",
    "        crps_csv.append(crps)\n",
    "\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "models = []\n",
    "crps_csv = []\n",
    "\n",
    "s_time = time.time()\n",
    "\n",
    "for k in range(10):\n",
    "    gc.collect()\n",
    "    print(\"-----------\")\n",
    "    \n",
    "    y_train, y_test = y[train_inds], y[test_inds]\n",
    "    \n",
    "    model,crps = get_model(X_train,y_train,X_test,y_test)\n",
    "    models.append(model)\n",
    "\n",
    "    print(\"the k %d  crps is %f\"%((k),crps))\n",
    "    crps_csv.append(crps)\n",
    "    gc.collect()\n",
    "        \n",
    "print(\"mean crps is %f\"%np.mean(crps_csv))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "crps_csv = []\n",
    "\n",
    "s_time = time.time()\n",
    "\n",
    "for k in range(2):\n",
    "    kfold = KFold(10, random_state = 42 + k, shuffle = True)\n",
    "    for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(y)):\n",
    "        gc.collect()\n",
    "        print(\"-----------\")\n",
    "        tr_x,tr_y = new_X[tr_inds],y[tr_inds]\n",
    "        val_x,val_y = new_X[val_inds],y[val_inds]\n",
    "    \n",
    "        model,crps = get_model(tr_x,tr_y,val_x,val_y)\n",
    "        models.append(model)\n",
    "        \n",
    "        print(\"the %d fold crps is %f\"%((k_fold+1),crps))\n",
    "        crps_csv.append(crps)\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"mean crps is %f\"%np.mean(crps_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean crps is %f\"%np.mean(crps_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.013399, 0.013367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Usando Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports we know we'll need\n",
    "import skopt\n",
    "# !pip install scikit-optimize if  necessary\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer  \n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_learning_rate    = Real(low=1e-4, high=1e-2, prior='log-uniform',name='learning_rate')\n",
    "dim_num_dense_layers = Integer(low=1, high=5, name='num_dense_layers')\n",
    "dim_num_input_nodes  = Integer(low=256, high=1024, name='num_input_nodes')\n",
    "dim_num_dense_nodes  = Integer(low=256, high=1024, name='num_dense_nodes')\n",
    "dim_activation       = Categorical(categories=['relu'],name='activation')\n",
    "dim_batch_size       = Integer(low=128, high=1024, name='batch_size')\n",
    "dim_adam_decay       = Real(low=1e-6,high=1e-2,name=\"adam_decay\")\n",
    "\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_input_nodes,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation,\n",
    "              dim_batch_size,\n",
    "              dim_adam_decay\n",
    "             ]\n",
    "default_parameters = [1e-3, 1,512, 13, 'relu',64, 1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "def create_model(learning_rate, num_dense_layers,num_input_nodes,\n",
    "                 num_dense_nodes, activation, adam_decay):\n",
    "    #start the model making process and create our first layer\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_input_nodes, input_shape= input_shape, activation=activation\n",
    "                   ))\n",
    "    #create a loop making a new dense layer for the amount passed to this model.\n",
    "    #naming the layers helps avoid tensorflow error deep in the stack trace.\n",
    "    for i in range(num_dense_layers):\n",
    "        name = 'layer_dense_{0}'.format(i+1)\n",
    "        model.add(Dense(num_dense_nodes,\n",
    "                 activation=activation,\n",
    "                        name=name\n",
    "                 ))\n",
    "    #add our classification layer.\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    \n",
    "    #setup our optimizer and compile\n",
    "    adam = Adam(lr=learning_rate, decay= adam_decay)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy',\n",
    "                 metrics=[])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Usando TALOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrangle\n",
    "x_train, y_train, x_val, y_val = wrangle.array_split(new_X, y, .15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_model(x_tr,y_tr,x_val,y_val,params):\n",
    "    inp = Input(shape = (x_tr.shape[1],))\n",
    "    x = Dense(1024, input_dim=X.shape[1], activation=params['activation'], kernel_initializer='normal')(inp)\n",
    "    x = Dropout(params['dropout'])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation=params['activation'])(x)\n",
    "    x = Dropout(params['dropout'])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation=params['activation'])(x)\n",
    "    x = Dropout(params['dropout'])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    out = Dense(199, activation=params['last_activation'], kernel_initializer='normal')(x)\n",
    "    model = Model(inp,out)\n",
    "    \n",
    "    model.compile(optimizer=params['optimizer'],\n",
    "                  loss=params['losses'],\n",
    "                  metrics=[])\n",
    "     \n",
    "    es = EarlyStopping(monitor='CRPS_score_val', \n",
    "                       mode='min',\n",
    "                       restore_best_weights=True, \n",
    "                       verbose=1, \n",
    "                       patience=10)\n",
    "\n",
    "    mc = ModelCheckpoint('best_model.h5',monitor='CRPS_score_val',mode='min',save_best_only=True, \n",
    "                         verbose=1, save_weights_only=True)\n",
    "    \n",
    "    #bsz = 1024\n",
    "    #steps = x_tr.shape[0]/bsz\n",
    "    \n",
    "    out = model.fit(x_tr, y_tr,\n",
    "                    callbacks=[CRPSCallback(validation = (x_val,y_val)),es,mc], \n",
    "                    epochs=params['epochs'],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    verbose=1)\n",
    "    \n",
    "    #model.load_weights(\"best_model.h5\")\n",
    "    \n",
    "    #y_pred = model.predict(x_val)\n",
    "    #y_valid = y_val\n",
    "    #y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n",
    "    #y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
    "    #val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * x_val.shape[0])\n",
    "    #crps = np.round(val_s, 6)\n",
    "\n",
    "    return out,model #,crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameter space boundary\n",
    "p = {'activation':['relu', 'elu'],\n",
    "     'last_activation': ['softmax'],\n",
    "     'optimizer': ['Nadam', 'Adam'],\n",
    "     'losses': ['categorical_crossentropy'],\n",
    "     'shapes': ['brick'],\n",
    "     #'first_neuron': [16, 32, 64, 128],\n",
    "     #'hidden_layers':[0, 1, 2, 3],\n",
    "     'dropout': [.2, .3, .4, .5],\n",
    "     'batch_size': [128, 256, 512, 1024],\n",
    "     'epochs': [10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talos\n",
    "\n",
    "# start the experiment\n",
    "scan_object = talos.Scan(x=x_train,\n",
    "                         y=y_train,\n",
    "                         x_val=x_val,\n",
    "                         y_val=y_val,\n",
    "                         model=get_new_model,\n",
    "                         experiment_name='nfl_dsa_02',\n",
    "                         params=p,\n",
    "                         round_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use Scan object as input\n",
    "analyze_object = talos.Analyze(scan_object)\n",
    "\n",
    "# access the dataframe with the results\n",
    "analyze_object.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the highest result for any metric\n",
    "analyze_object.low('CRPS_score_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the round with the best result\n",
    "analyze_object.rounds2high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with the model\n",
    "models[5].predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Realizar a submissão para o Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if  TRAIN_OFFLINE==False:\n",
    "    \n",
    "    from kaggle.competitions import nflrush\n",
    "    env = nflrush.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    df_prev = pd.DataFrame()\n",
    "    df_test = pd.DataFrame()\n",
    "\n",
    "    for (test_df, sample_prediction_df) in tqdm_notebook(iter_test):\n",
    "        basetable = create_features(test_df, deploy=True)\n",
    "        basetable = create_features_02(basetable)\n",
    "        basetable.drop(['GameId','PlayId','NflId','NflIdRusher','PossessionTeam','HomeTeamAbbr','VisitorTeamAbbr','PlayDirection','GameClock','Season','Team','FieldPosition'], axis=1, inplace=True)\n",
    "        \n",
    "        # Considerar somente as colunas do Feature Selection\n",
    "        basetable = basetable.loc[:,best_features]\n",
    "    \n",
    "        scaled_basetable = scaler.transform(basetable)\n",
    "\n",
    "        y_pred = predict(scaled_basetable)\n",
    "        y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1).tolist()[0]\n",
    "\n",
    "        preds_df = pd.DataFrame(data=[y_pred], columns=sample_prediction_df.columns)\n",
    "        \n",
    "        df_test = df_test.append(basetable)\n",
    "        df_prev = df_prev.append(preds_df)\n",
    "    \n",
    "        env.predict(preds_df)\n",
    "\n",
    "    env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
