{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NFL Competition\n\n# Feature Engineering e Modelo de Machine Learning\n\n- **Version: 2.1.1:** included feature selection, loss validation on kFold and new struct for NN\n- **Version: 2.1.2:** included some new static features"},{"metadata":{},"cell_type":"markdown","source":"## 1. Importa os pacotes e o dataset de treino"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Importar os principais pacotes\nimport numpy as np\nimport pandas as pd\nimport sklearn.metrics as mtr\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nimport re\nimport codecs\nimport time\nimport datetime\nimport tsfresh\nimport pandasql as ps\n\n# Evitar que aparece os warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Seta algumas opções no Jupyter para exibição dos datasets\npd.set_option('display.max_columns', 150)\npd.set_option('display.max_rows', 150)","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"!pip install tsfresh\n!pip install keras-lookahead"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importa os pacotes de algoritmos de regressão\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n\n# Importa os pacotes de algoritmos de redes neurais (Keras)\nfrom keras.losses import binary_crossentropy\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda,BatchNormalization\nfrom keras.models import Sequential, Model\nfrom keras.callbacks import Callback,EarlyStopping,ModelCheckpoint\nimport keras.backend as K\n#from keras_lookahead import Lookahead\n#from keras_radam import RAdam\n\n# Importa pacotes do sklearn\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold, train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.preprocessing import scale, MinMaxScaler, StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define o caminho do arquivo\npath_local  = '../data/train.csv'\npath_kernel = '/kaggle/input/nfl-big-data-bowl-2020/train.csv'\n\n# Carrega o dataset de treino\ntrain = pd.read_csv(path_kernel, usecols =['GameId', 'PlayId', 'Team', 'X', 'Y', 'S', 'A', 'Dis',\n                                          'Orientation', 'Dir', 'NflId', 'DisplayName', 'YardLine',\n                                          'Quarter', 'GameClock', 'PossessionTeam', 'Down', 'Distance',\n                                          'FieldPosition', 'NflIdRusher', 'OffenseFormation', \n                                          'OffensePersonnel', 'DefendersInTheBox', 'DefensePersonnel', \n                                          'PlayDirection', 'TimeHandoff', 'TimeSnap', 'HomeTeamAbbr','VisitorTeamAbbr',\n                                          'PlayerHeight','WindSpeed','PlayerBirthDate','Season','Yards'])\n\n# Usado somente para teste (somente as primeiras 2200 linhas)\n#train = train[:2200]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_features_01(df, deploy=False):\n    def new_X(x_coordinate, play_direction):\n        if play_direction == 'left':\n            return 120.0 - x_coordinate\n        else:\n            return x_coordinate\n\n    def new_line(rush_team, field_position, yardline):\n        if rush_team == field_position:\n            # offense starting at X = 0 plus the 10 yard endzone plus the line of scrimmage\n            return 10.0 + yardline\n        else:\n            # half the field plus the yards between midfield and the line of scrimmage\n            return 60.0 + (50 - yardline)\n\n    def new_orientation(angle, play_direction):\n        if play_direction == 'left':\n            new_angle = 360.0 - angle\n            if new_angle == 360.0:\n                new_angle = 0.0\n            return new_angle\n        else:\n            return angle\n\n    def euclidean_distance(x1,y1,x2,y2):\n        x_diff = (x1-x2)**2\n        y_diff = (y1-y2)**2\n\n        return np.sqrt(x_diff + y_diff)\n\n    def back_direction(orientation):\n        if orientation > 180.0:\n            return 1\n        else:\n            return 0\n\n    def update_yardline(df):\n        new_yardline = df[df['NflId'] == df['NflIdRusher']]\n        new_yardline['YardLine'] = new_yardline[['PossessionTeam','FieldPosition','YardLine']].apply(lambda x: new_line(x[0],x[1],x[2]), axis=1)\n        new_yardline = new_yardline[['GameId','PlayId','YardLine']]\n\n        return new_yardline\n\n    def update_orientation(df, yardline):\n        df['X'] = df[['X','PlayDirection']].apply(lambda x: new_X(x[0],x[1]), axis=1)\n        df['Orientation'] = df[['Orientation','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n        df['Dir'] = df[['Dir','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n\n        df = df.drop('YardLine', axis=1)\n        df = pd.merge(df, yardline, on=['GameId','PlayId'], how='inner')\n\n        return df\n\n    def back_features(df):\n        carriers = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','NflIdRusher','X','Y','Orientation','Dir','YardLine']]\n        carriers['back_from_scrimmage'] = carriers['YardLine'] - carriers['X']\n        carriers['back_oriented_down_field'] = carriers['Orientation'].apply(lambda x: back_direction(x))\n        carriers['back_moving_down_field'] = carriers['Dir'].apply(lambda x: back_direction(x))\n        carriers = carriers.rename(columns={'X':'back_X',\n                                            'Y':'back_Y'})\n        carriers = carriers[['GameId','PlayId','NflIdRusher','back_X','back_Y','back_from_scrimmage','back_oriented_down_field','back_moving_down_field']]\n\n        return carriers\n\n    def features_relative_to_back(df, carriers):\n        player_distance = df[['GameId','PlayId','NflId','X','Y']]\n        player_distance = pd.merge(player_distance, carriers, on=['GameId','PlayId'], how='inner')\n        player_distance = player_distance[player_distance['NflId'] != player_distance['NflIdRusher']]\n        player_distance['dist_to_back'] = player_distance[['X','Y','back_X','back_Y']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n\n        player_distance = player_distance.groupby(['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field'])\\\n                                         .agg({'dist_to_back':['min','max','mean','std']})\\\n                                         .reset_index()\n        player_distance.columns = ['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field',\n                                   'min_dist','max_dist','mean_dist','std_dist']\n\n        return player_distance\n\n    def defense_features(df):\n        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','Y']]\n        rusher.columns = ['GameId','PlayId','RusherTeam','RusherX','RusherY']\n\n        defense = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n        defense = defense[defense['Team'] != defense['RusherTeam']][['GameId','PlayId','X','Y','RusherX','RusherY']]\n        defense['def_dist_to_back'] = defense[['X','Y','RusherX','RusherY']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n\n        defense = defense.groupby(['GameId','PlayId'])\\\n                         .agg({'def_dist_to_back':['min','max','mean','std']})\\\n                         .reset_index()\n        defense.columns = ['GameId','PlayId','def_min_dist','def_max_dist','def_mean_dist','def_std_dist']\n\n        return defense\n\n    def static_features(df):\n\n        add_new_feas = []\n\n        ## Height\n        df['PlayerHeight_dense'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n    \n        add_new_feas.append('PlayerHeight_dense')\n\n        ## Time\n        df['TimeHandoff'] = df['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n        df['TimeSnap'] = df['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n\n        df['TimeDelta'] = df.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n        df['PlayerBirthDate'] =df['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m/%d/%Y\"))\n\n        ## Age\n        seconds_in_year = 60*60*24*365.25\n        df['PlayerAge'] = df.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()/seconds_in_year, axis=1)\n        add_new_feas.append('PlayerAge')\n\n        ## WindSpeed\n        df['WindSpeed_ob'] = df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n        df['WindSpeed_ob'] = df['WindSpeed_ob'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n        df['WindSpeed_ob'] = df['WindSpeed_ob'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n        df['WindSpeed_dense'] = df['WindSpeed_ob'].apply(strtofloat)\n        add_new_feas.append('WindSpeed_dense')\n\n        ## Weather\n        df['GameWeather_process'] = df['GameWeather'].str.lower()\n        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: \"indoor\" if not pd.isna(x) and \"indoor\" in x else x)\n        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\n        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\n        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n        df['GameWeather_dense'] = df['GameWeather_process'].apply(map_weather)\n        add_new_feas.append('GameWeather_dense')\n\n        ## Orientation and Dir\n        df[\"Orientation_ob\"] = df[\"Orientation\"].apply(lambda x : orientation_to_cat(x)).astype(\"object\")\n        df[\"Dir_ob\"] = df[\"Dir\"].apply(lambda x : orientation_to_cat(x)).astype(\"object\")\n\n        df[\"Orientation_sin\"] = df[\"Orientation\"].apply(lambda x : np.sin(x/360 * 2 * np.pi))\n        df[\"Orientation_cos\"] = df[\"Orientation\"].apply(lambda x : np.cos(x/360 * 2 * np.pi))\n        df[\"Dir_sin\"] = df[\"Dir\"].apply(lambda x : np.sin(x/360 * 2 * np.pi))\n        df[\"Dir_cos\"] = df[\"Dir\"].apply(lambda x : np.cos(x/360 * 2 * np.pi))\n        add_new_feas.append(\"Dir_sin\")\n        add_new_feas.append(\"Dir_cos\")\n\n        ## diff Score\n        df[\"diffScoreBeforePlay\"] = df[\"HomeScoreBeforePlay\"] - df[\"VisitorScoreBeforePlay\"]\n        add_new_feas.append(\"diffScoreBeforePlay\")\n    \n        df['DefendersInTheBox'] = df['DefendersInTheBox'].fillna(np.mean(df['DefendersInTheBox']))\n        add_new_feas.append(\"DefendersInTheBox\")\n        \n        static_features = df[df['NflId'] == df['NflIdRusher']][add_new_feas+['GameId','PlayId','X','Y','S','A','Dis','Orientation','Dir',\n                                                               'YardLine','Quarter','Down','Distance','DefendersInTheBox','Team',\n                                                               'PossessionTeam','HomeTeamAbbr','VisitorTeamAbbr','FieldPosition',\n                                                               'PlayDirection','PlayerHeight','WindSpeed','NflId','NflIdRusher',\n                                                               'TimeHandoff','TimeSnap','PlayerBirthDate','GameClock','Season']].drop_duplicates()\n        static_features.fillna(-999,inplace=True)\n        \n        return static_features\n\n    def split_personnel(s):\n        splits = s.split(',')\n        for i in range(len(splits)):\n            splits[i] = splits[i].strip()\n\n        return splits\n\n    def defense_formation(l):\n        dl = 0\n        lb = 0\n        db = 0\n        other = 0\n\n        for position in l:\n            sub_string = position.split(' ')\n            if sub_string[1] == 'DL':\n                dl += int(sub_string[0])\n            elif sub_string[1] in ['LB','OL']:\n                lb += int(sub_string[0])\n            else:\n                db += int(sub_string[0])\n\n        counts = (dl,lb,db,other)\n\n        return counts\n\n    def offense_formation(l):\n        qb = 0\n        rb = 0\n        wr = 0\n        te = 0\n        ol = 0\n\n        sub_total = 0\n        qb_listed = False\n        for position in l:\n            sub_string = position.split(' ')\n            pos = sub_string[1]\n            cnt = int(sub_string[0])\n\n            if pos == 'QB':\n                qb += cnt\n                sub_total += cnt\n                qb_listed = True\n            # Assuming LB is a line backer lined up as full back\n            elif pos in ['RB','LB']:\n                rb += cnt\n                sub_total += cnt\n            # Assuming DB is a defensive back and lined up as WR\n            elif pos in ['WR','DB']:\n                wr += cnt\n                sub_total += cnt\n            elif pos == 'TE':\n                te += cnt\n                sub_total += cnt\n            # Assuming DL is a defensive lineman lined up as an additional line man\n            else:\n                ol += cnt\n                sub_total += cnt\n\n        # If not all 11 players were noted at given positions we need to make some assumptions\n        # I will assume if a QB is not listed then there was 1 QB on the play\n        # If a QB is listed then I'm going to assume the rest of the positions are at OL\n        # This might be flawed but it looks like RB, TE and WR are always listed in the personnel\n        if sub_total < 11:\n            diff = 11 - sub_total\n            if not qb_listed:\n                qb += 1\n                diff -= 1\n            ol += diff\n\n        counts = (qb,rb,wr,te,ol)\n\n        return counts    \n\n    def personnel_features(df):\n        personnel = df[['GameId','PlayId','OffensePersonnel','DefensePersonnel']].drop_duplicates()\n        personnel['DefensePersonnel'] = personnel['DefensePersonnel'].apply(lambda x: split_personnel(x))\n        personnel['DefensePersonnel'] = personnel['DefensePersonnel'].apply(lambda x: defense_formation(x))\n        personnel['num_DL'] = personnel['DefensePersonnel'].apply(lambda x: x[0])\n        personnel['num_LB'] = personnel['DefensePersonnel'].apply(lambda x: x[1])\n        personnel['num_DB'] = personnel['DefensePersonnel'].apply(lambda x: x[2])\n\n        personnel['OffensePersonnel'] = personnel['OffensePersonnel'].apply(lambda x: split_personnel(x))\n        personnel['OffensePersonnel'] = personnel['OffensePersonnel'].apply(lambda x: offense_formation(x))\n        personnel['num_QB'] = personnel['OffensePersonnel'].apply(lambda x: x[0])\n        personnel['num_RB'] = personnel['OffensePersonnel'].apply(lambda x: x[1])\n        personnel['num_WR'] = personnel['OffensePersonnel'].apply(lambda x: x[2])\n        personnel['num_TE'] = personnel['OffensePersonnel'].apply(lambda x: x[3])\n        personnel['num_OL'] = personnel['OffensePersonnel'].apply(lambda x: x[4])\n\n        # Let's create some features to specify if the OL is covered\n        personnel['OL_diff'] = personnel['num_OL'] - personnel['num_DL']\n        personnel['OL_TE_diff'] = (personnel['num_OL'] + personnel['num_TE']) - personnel['num_DL']\n        # Let's create a feature to specify if the defense is preventing the run\n        # Let's just assume 7 or more DL and LB is run prevention\n        personnel['run_def'] = (personnel['num_DL'] + personnel['num_LB'] > 6).astype(int)\n\n        personnel.drop(['OffensePersonnel','DefensePersonnel'], axis=1, inplace=True)\n        \n        return personnel\n\n    def combine_features(relative_to_back, defense, static, personnel, deploy=deploy):\n        df = pd.merge(relative_to_back,defense,on=['GameId','PlayId'],how='inner')\n        df = pd.merge(df,static,on=['GameId','PlayId'],how='inner')\n        df = pd.merge(df,personnel,on=['GameId','PlayId'],how='inner')\n\n        if not deploy:\n            df = pd.merge(df, outcomes, on=['GameId','PlayId'], how='inner')\n\n        return df\n\n    yardline = update_yardline(df)\n    df = update_orientation(df, yardline)\n    back_feats = back_features(df)\n    rel_back = features_relative_to_back(df, back_feats)\n    def_feats = defense_features(df)\n    static_feats = static_features(df)\n    personnel = personnel_features(df)\n    \n    basetable = combine_features(rel_back, def_feats, static_feats, personnel, deploy=deploy)\n    \n    return basetable\n\n\ndef get_time(x):\n    x = x.split(\":\")\n    return int(x[0])*60 + int(x[1])\n\ndef get_height(x):\n    x = x.split(\"-\")\n    return int(x[0])*12 + int(x[1])\n\ndef process_windspeed(txt):\n    txt = str(txt).lower().replace('mph', '').strip()\n    if '-' in txt:\n        txt = (int(txt.split('-')[0]) + int(txt.split('-')[1])) / 2\n    try:\n        return float(txt)\n    except:\n        return -1.0\n\ndef create_features_02(t_):\n    t_['fe1'] = pd.Series(np.sqrt(np.absolute(np.square(t_.X.values) - np.square(t_.Y.values))))\n    t_['fe5'] = np.square(t_['S'].values) + 2 * t_['A'].values * t_['Dis'].values  # N\n    t_['fe7'] = np.arccos(np.clip(t_['X'].values / t_['Y'].values, -1, 1))  # N\n    t_['fe8'] = t_['S'].values / np.clip(t_['fe1'].values, 0.6, None)\n    radian_angle = (90 - t_['Dir']) * np.pi / 180.0\n    t_['fe10'] = np.abs(t_['S'] * np.cos(radian_angle))\n    t_['fe11'] = np.abs(t_['S'] * np.sin(radian_angle))\n    \n    t_[\"is_rusher\"]          = 1.0*(t_[\"NflId\"] == t_[\"NflIdRusher\"])\n    t_[\"is_home\"]            = t_[\"Team\"] == \"home\"\n    t_[\"is_possession_team\"] = 1.0*(t_[\"PossessionTeam\"] == t_[\"HomeTeamAbbr\"]) - 1.0*(t_[\"PossessionTeam\"] == t_[\"VisitorTeamAbbr\"])\n    t_[\"is_field_team\"]      = 1.0*(t_[\"FieldPosition\"] == t_[\"HomeTeamAbbr\"]) - 1.0*(t_[\"FieldPosition\"] == t_[\"VisitorTeamAbbr\"])\n    t_[\"is_left\"]            = t_[\"PlayDirection\"] == \"left\"\n    \n    #t_[\"player_height\"]      = t_[\"PlayerHeight\"].apply(get_height)\n    #t_[\"WindSpeed\"]   = t_[\"WindSpeed\"].apply(process_windspeed)\n    #t_[\"TimeHandoff\"] = pd.to_datetime(t_[\"TimeHandoff\"])\n    #t_[\"TimeSnap\"]    = pd.to_datetime(t_[\"TimeSnap\"])\n    #t_[\"duration\"]    = (t_[\"TimeHandoff\"] - t_[\"TimeSnap\"]).dt.total_seconds()\n\n    #t_[\"player_age\"]  = (t_[\"TimeSnap\"].dt.date - pd.to_datetime(t_[\"PlayerBirthDate\"]).dt.date)/np.timedelta64(1, 'D') / 365\n\n    t_[\"game_time\"]   = t_[\"GameClock\"].apply(get_time)\n    t_[\"old_data\"]    = t_[\"Season\"] == 2017\n    return t_\n\n\ndef logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\ndef squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define as colunas ID que serão agrupadas e posteriormente removidas\noutcomes = train[['GameId','PlayId','Yards']].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cria as novas features (etapa 01)\n%time train_basetable = create_features_01(train, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cria as novas features (etapa 02)\n%time train_basetable = create_features_02(train_basetable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cria as novas features (etapa 03)\nlog_features = ['X','Y','S','A','Dis','Orientation','Dir','YardLine','player_height','player_age','game_time']\ntrain_basetable = logs(train_basetable, log_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cria as novas features (etapa 04)\nsquared_features = ['X','Y','S','A','Dis','Orientation','Dir','YardLine','player_height','player_age','game_time']\ntrain_basetable = squares(train_basetable, squared_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove algumas features do dataset e preenche valores NaN com 0 (zero)\ntrain_basetable.drop(['TimeHandoff','PlayerBirthDate','GameClock','PlayerHeight','NflId','NflIdRusher','Season'], axis=1, inplace=True)\ntrain_basetable = train_basetable.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cria uma copia do dataset para backup\nX = train_basetable.copy()\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transformação de variaveis categoricas para numericas usando LabelEncoder\nle = preprocessing.LabelEncoder()\n\nle_dict = {}\ncategoricals = ['Team_le','PossessionTeam_le','HomeTeamAbbr_le','VisitorTeamAbbr_le',\n                'FieldPosition_le','PlayDirection_le']\n\nfor cat in categoricals:\n    le_dict[cat] = LabelEncoder()\n    X[cat] = le_dict[cat].fit_transform(X[cat[:-3]].apply(str))  \n\n# Remove as features originais que foram transformadas\nX.drop(['TimeSnap','Team','PossessionTeam','HomeTeamAbbr','VisitorTeamAbbr','FieldPosition','PlayDirection'], axis=1, inplace=True)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.sparse as ss\n\nclass Dataset:\n    \"\"\"\n    Dataset for LOFO\n    Parameters\n    ----------\n    df: pandas dataframe\n    target: string\n        Column name for target within df\n    features: list of strings\n        List of column names within df\n    feature_groups: dict, optional\n        Name, value dictionary of feature groups as numpy.darray or scipy.csr.scr_matrix\n    \"\"\"\n\n    def __init__(self, df, target, features, feature_groups=None):\n        self.df = df.copy()\n        self.features = list(features)\n        self.feature_groups = feature_groups if feature_groups else dict()\n\n        self.num_rows = df.shape[0]\n        self.y = df[target].values\n\n        for feature_name, feature_matrix in self.feature_groups.items():\n            if not (isinstance(feature_matrix, np.ndarray) or isinstance(feature_matrix, ss.csr.csr_matrix)):\n                raise Exception(\"Data type {dtype} is not a valid type!\".format(dtype=type(feature_matrix)))\n\n            if feature_matrix.shape[0] != self.num_rows:\n                raise Exception(\"Expected {expected} rows but got {n} rows!\".format(expected=self.num_rows,\n                                                                                    n=feature_matrix.shape[0]))\n\n            if feature_name in self.features:\n                raise Exception(\"Feature group name '{name}' is the same with one of the features!\")\n\n    def getX(self, feature_to_remove, fit_params):\n        \"\"\"Get feature matrix and fit_params after removing a feature\n        Parameters\n        ----------\n        feature_to_remove : string\n            feature name to remove\n        fit_params : dict\n            fit parameters for the model\n        Returns\n        -------\n        X : numpy.darray or scipy.csr.scr_matrix\n            Feature matrix\n        fit_params: dict\n            Updated fit_params after feature removal\n        \"\"\"\n        feature_list = [feature for feature in self.features if feature != feature_to_remove]\n        concat_list = [self.df[feature_list].values]\n\n        for feature_name, feature_matrix in self.feature_groups.items():\n            if feature_name != feature_to_remove:\n                concat_list.append(feature_matrix)\n\n        fit_params = fit_params.copy()\n        if \"categorical_feature\" in fit_params:\n            cat_features = [f for f in fit_params[\"categorical_feature\"] if f != feature_to_remove]\n            fit_params[\"categorical_feature\"] = [ix for ix, f in enumerate(feature_list) if (f in cat_features)]\n\n        has_sparse = False\n        for feature_name, feature_matrix in self.feature_groups.items():\n            if feature_name != feature_to_remove and isinstance(feature_matrix, ss.csr.csr_matrix):\n                has_sparse = True\n\n        concat = np.hstack\n        if has_sparse:\n            concat = ss.hstack\n\n        return concat(concat_list), fit_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier, LGBMRegressor\n\ndef infer_model(df, features, y, n_jobs):\n    model_class = LGBMRegressor\n    if len(np.unique(y)) == 2:\n        y = LabelEncoder().fit_transform(y)\n        model_class = LGBMClassifier\n\n    categoricals = df[features].select_dtypes(exclude=[np.number]).columns.tolist()\n    for f in categoricals:\n        df[f] = LabelEncoder().fit_transform(df[f].apply(str))\n\n    min_child_samples = int(0.01*df.shape[0])\n\n    model = model_class(min_child_samples=min_child_samples, n_jobs=n_jobs)\n\n    return model, df, categoricals, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_importance(importance_df, figsize=(8, 8)):\n    \"\"\"Plot feature importance\n    Parameters\n    ----------\n    importance_df : pandas dataframe\n        Output dataframe from LOFO/FLOFO get_importance\n    figsize : tuple\n    \"\"\"\n    importance_df = importance_df.copy()\n    importance_df[\"color\"] = (importance_df[\"importance_mean\"] > 0).map({True: 'g', False: 'r'})\n    importance_df.sort_values(\"importance_mean\", inplace=True)\n\n    importance_df.plot(x=\"feature\", y=\"importance_mean\", xerr=\"importance_std\",\n                       kind='barh', color=importance_df[\"color\"], figsize=figsize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nimport multiprocessing\n#from lofo.infer_defaults import infer_model\n\n\nclass LOFOImportance:\n    \"\"\"\n    Leave One Feature Out Importance\n    Given a model and cross-validation scheme, calculates the feature importances.\n    Parameters\n    ----------\n    dataset: LOFO Dataset object\n    scoring: string or callable\n        Same as scoring in sklearn API\n    model: model (sklearn API), optional\n        Not trained model object\n    fit_params : dict, optional\n        fit parameters for the model\n    cv: int or iterable\n        Same as cv in sklearn API\n    n_jobs: int, optional\n        Number of jobs for parallel computation\n    \"\"\"\n\n    def __init__(self, dataset, scoring, model=None, fit_params=None, cv=4, n_jobs=None):\n\n        self.fit_params = fit_params if fit_params else dict()\n        if model is None:\n            model, dataset.df, categoricals, dataset.y = infer_model(dataset.df, dataset.features, dataset.y, n_jobs)\n            self.fit_params[\"categorical_feature\"] = categoricals\n            n_jobs = 1\n\n        self.model = model\n        self.dataset = dataset\n        self.scoring = scoring\n        self.cv = cv\n        self.n_jobs = n_jobs\n        if self.n_jobs is not None and self.n_jobs > 1:\n            warning_str = (\"Warning: If your model is multithreaded, please initialise the number\"\n                           \"of jobs of LOFO to be equal to 1, otherwise you may experience performance issues.\")\n            warnings.warn(warning_str)\n\n    def _get_cv_score(self, feature_to_remove):\n        X, fit_params = self.dataset.getX(feature_to_remove=feature_to_remove, fit_params=self.fit_params)\n        y = self.dataset.y\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            cv_results = cross_validate(self.model, X, y, cv=self.cv, scoring=self.scoring, fit_params=fit_params)\n        return cv_results['test_score']\n\n    def _get_cv_score_parallel(self, feature, result_queue):\n        test_score = self._get_cv_score(feature_to_remove=feature)\n        result_queue.put((feature, test_score))\n        return test_score\n\n    def get_importance(self):\n        \"\"\"Run LOFO to get feature importances\n        Returns\n        -------\n        importance_df : pandas dataframe\n            Dataframe with feature names and corresponding importance mean and std (sorted by importance)\n        \"\"\"\n        base_cv_score = self._get_cv_score(feature_to_remove=None)\n        feature_list = self.dataset.features + list(self.dataset.feature_groups.keys())\n\n        if self.n_jobs is not None and self.n_jobs > 1:\n\n            pool = multiprocessing.Pool(self.n_jobs)\n            manager = multiprocessing.Manager()\n            result_queue = manager.Queue()\n\n            for f in feature_list:\n                pool.apply_async(self._get_cv_score_parallel, (f, result_queue))\n\n            pool.close()\n            pool.join()\n\n            lofo_cv_result = [result_queue.get() for _ in range(len(feature_list))]\n            lofo_cv_scores_normalized = np.array([base_cv_score - lofo_cv_score for _, lofo_cv_score in lofo_cv_result])\n            feature_list = [feature for feature, _ in lofo_cv_result]\n        else:\n            lofo_cv_scores = []\n            for f in tqdm_notebook(feature_list):\n                lofo_cv_scores.append(self._get_cv_score(feature_to_remove=f))\n\n            lofo_cv_scores_normalized = np.array([base_cv_score - lofo_cv_score for lofo_cv_score in lofo_cv_scores])\n\n        importance_df = pd.DataFrame()\n        importance_df[\"feature\"] = feature_list\n        importance_df[\"importance_mean\"] = lofo_cv_scores_normalized.mean(axis=1)\n        importance_df[\"importance_std\"] = lofo_cv_scores_normalized.std(axis=1)\n\n        return importance_df.sort_values(\"importance_mean\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Procedimento para verificar as features mais importantes\n# Usando LightGBM para treinamento\n#from lofo import LOFOImportance, Dataset, plot_importance\n\nn_folds = 5\nkfold_lgb = KFold(n_folds, shuffle=True)\n\nfeatures = [x for x in X.columns if x not in ['Yards','GameId','PlayId']]\n\nparams2 = {'num_leaves': 15,\n          'objective': 'mae',\n          #'learning_rate': 0.1,\n          \"boosting\": \"gbdt\",\n          \"num_rounds\": 150\n          }\n\nmodel_lgb = lgb.LGBMRegressor(**params2)\ndataset = Dataset(df=X, target=\"Yards\", features=features)\nlofo_imp = LOFOImportance(dataset, model=model_lgb, cv=kfold_lgb, scoring=\"neg_mean_absolute_error\", fit_params={\"categorical_feature\": categoricals})\n\nimportance_df = lofo_imp.get_importance()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Exibindo grafico com as features\nplot_importance(importance_df, figsize=(12, 38))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_features = importance_df.loc[importance_df['importance_mean'] > 0].feature\nbest_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Criar e avaliar alguns algoritmos de Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criar um dataset somente com as colunas mais importantes conforme visto anteriormente\nnew_X = X.loc[:,best_features]\ntarget = X.Yards\n\ny = np.zeros((target.shape[0], 199))\nfor idx, target in enumerate(list(target)):\n    y[idx][99 + target] = 1\n    \n# Normalizando as variaveis do dataset de treino\nscaler = StandardScaler()\nnew_X = scaler.fit_transform(new_X)\nnew_X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3. Teste com Keras (New Struct)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CRPSCallback(Callback):\n    \n    def __init__(self,validation, predict_batch_size=20, include_on_batch=False):\n        super(CRPSCallback, self).__init__()\n        self.validation = validation\n        self.predict_batch_size = predict_batch_size\n        self.include_on_batch = include_on_batch\n        \n        print('validation shape',len(self.validation))\n\n    def on_batch_begin(self, batch, logs={}):\n        pass\n\n    def on_train_begin(self, logs={}):\n        if not ('CRPS_score_val' in self.params['metrics']):\n            self.params['metrics'].append('CRPS_score_val')\n\n    def on_batch_end(self, batch, logs={}):\n        if (self.include_on_batch):\n            logs['CRPS_score_val'] = float('-inf')\n\n    def on_epoch_end(self, epoch, logs={}):\n        logs['CRPS_score_val'] = float('-inf')\n            \n        if (self.validation):\n            X_valid, y_valid = self.validation[0], self.validation[1]\n            y_pred = self.model.predict(X_valid)\n            y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n            y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n            val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * X_valid.shape[0])\n            val_s = np.round(val_s, 6)\n            logs['CRPS_score_val'] = val_s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(x_tr,y_tr,x_val,y_val):\n    inp = Input(shape = (x_tr.shape[1],))\n    x = Dense(1024, input_dim=X.shape[1], activation='relu')(inp)\n    x = Dropout(0.6)(x)\n    x = BatchNormalization()(x)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = BatchNormalization()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    x = BatchNormalization()(x)\n    \n    out = Dense(199, activation='softmax')(x)\n    model = Model(inp,out)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[])\n    #add lookahead\n#     lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n#     lookahead.inject(model) # add into model\n\n    \n    es = EarlyStopping(monitor='CRPS_score_val', \n                       mode='min',\n                       restore_best_weights=True, \n                       verbose=1, \n                       patience=15)\n\n    mc = ModelCheckpoint('best_model.h5',monitor='CRPS_score_val',mode='min',\n                                   save_best_only=True, verbose=1, save_weights_only=True)\n    \n    bsz = 1024\n    steps = x_tr.shape[0]/bsz\n    \n\n\n    model.fit(x_tr, y_tr,callbacks=[CRPSCallback(validation = (x_val,y_val)),es,mc], epochs=250, batch_size=bsz,verbose=1)\n    model.load_weights(\"best_model.h5\")\n    \n    y_pred = model.predict(x_val)\n    y_valid = y_val\n    y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n    val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * x_val.shape[0])\n    crps = np.round(val_s, 6)\n\n    return model,crps","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"losses = []\nmodels = []\ncrps_csv = []\n\ns_time = time.time()\n\nfor k in range(2):\n    kfold = KFold(10, random_state = 42 + k, shuffle = True)\n    for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(y)):\n        print(\"-----------\")\n        print(\"-----------\")\n        tr_x,tr_y = new_X[tr_inds],y[tr_inds]\n        val_x,val_y = new_X[val_inds],y[val_inds]\n        model,crps = get_model(tr_x,tr_y,val_x,val_y)\n\n        if (crps <= 0.013):\n            print(\"the %d fold crps is %f\"%((k_fold),crps))\n            models.append(model)\n            crps_csv.append(crps)\n            if (len(crps_csv) > 10):\n                break\n        else:\n            print('Ignore KFold:',k_fold, '| CRPS:', crps)  \n\n        #models.append(model)\n        #print(\"the %d fold crps is %f\"%((k_fold+1),crps))\n        #crps_csv.append(crps)\n\nprint(\"mean crps is %f\"%np.mean(crps_csv))\n\n\ndef predict(x_te):\n    model_num = len(models)\n    for k,m in enumerate(models):\n        if k==0:\n            y_pred = m.predict(x_te,batch_size=1024)\n        else:\n            y_pred+=m.predict(x_te,batch_size=1024)\n            \n    y_pred = y_pred / model_num\n    \n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"mean crps is %f\"%np.mean(crps_csv))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Realizar a submissão para o Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"class GP:\n    def __init__(self):\n        self.classes = 20\n        self.class_names = [ 'class_0',\n                             'class_1',\n                             'class_2',\n                             'class_3',\n                             'class_4',\n                             'class_5',\n                             'class_6',\n                             'class_7',\n                             'class_8',\n                             'class_9',\n                             'class_10',\n                             'class_11',\n                             'class_12',\n                             'class_13',\n                            'class_14',\n                            'class_15',\n                            'class_16',\n                            'class_17',\n                            'class_18',\n                            'class_19',\n                           ]\n\n\n    def GrabPredictions(self, data):\n        oof_preds = np.zeros((len(data), len(self.class_names)))\n        oof_preds[:,0] = self.GP_class_0(data)\n        oof_preds[:,1] = self.GP_class_1(data)\n        oof_preds[:,2] = self.GP_class_2(data)\n        oof_preds[:,3] = self.GP_class_3(data)\n        oof_preds[:,4] = self.GP_class_4(data)\n        oof_preds[:,5] = self.GP_class_5(data)\n        oof_preds[:,6] = self.GP_class_6(data)\n        oof_preds[:,7] = self.GP_class_7(data)\n        oof_preds[:,8] = self.GP_class_8(data)\n        oof_preds[:,9] = self.GP_class_9(data)\n        oof_preds[:,10] = self.GP_class_10(data)\n        oof_preds[:,11] = self.GP_class_11(data)\n        oof_preds[:,12] = self.GP_class_12(data)\n        oof_preds[:,13] = self.GP_class_13(data)\n        oof_preds[:,14] = self.GP_class_14(data)\n        oof_preds[:,15] = self.GP_class_15(data)\n        oof_preds[:,16] = self.GP_class_16(data)\n        oof_preds[:,17] = self.GP_class_17(data)\n        oof_preds[:,18] = self.GP_class_18(data)\n        oof_preds[:,19] = self.GP_class_19(data)\n        oof_df = pd.DataFrame(np.exp(oof_preds), columns=self.class_names)\n        oof_df =oof_df.div(oof_df.sum(axis=1), axis=0)\n        \n        return oof_df.values\n\n\n    def GP_class_0(self,data):\n        return(0.250000*np.tanh(((((((data[:,0]) - (((data[:,0]) + (data[:,0]))))) + (data[:,0]))) + (((((data[:,0]) * 2.0)) * 2.0)))) +\n                0.250000*np.tanh(((((((((((((data[:,2]) - (data[:,7]))) * 2.0)) + (data[:,7]))) * 2.0)) + (data[:,0]))) + (((((data[:,2]) + (data[:,0]))) - (data[:,7]))))) +\n                0.250000*np.tanh(((((((((((((((((((data[:,3]) - (data[:,7]))) - (((data[:,0]) / 2.0)))) + (data[:,0]))) - (data[:,7]))) + (data[:,2]))) + (((data[:,7]) + (data[:,2]))))) + (data[:,7]))) - (data[:,7]))) + (data[:,0]))) +\n                0.250000*np.tanh(((((((((data[:,0]) - (data[:,7]))) - (data[:,14]))) + (((data[:,17]) - (data[:,14]))))) + (((((((data[:,0]) - ((1.0)))) - (data[:,0]))) + (data[:,20]))))) +\n                0.250000*np.tanh(((((data[:,18]) + (((((((data[:,0]) - (data[:,7]))) + (data[:,2]))) + (data[:,0]))))) - (((data[:,7]) + (data[:,2]))))) +\n                0.250000*np.tanh((((((((data[:,8]) + ((((((((data[:,13]) + (data[:,3]))) + (data[:,0]))/2.0)) + ((((((((data[:,0]) + (((((data[:,0]) - (data[:,0]))) / 2.0)))) + (data[:,17]))/2.0)) + (data[:,11]))))))/2.0)) + (data[:,0]))) - (data[:,13]))) +\n                0.250000*np.tanh(((((data[:,0]) * 2.0)) + (((((((((data[:,2]) + (data[:,2]))) * 2.0)) - (((data[:,14]) - (((np.tanh((data[:,2]))) * 2.0)))))) - (data[:,7]))))) +\n                0.250000*np.tanh(((((((((((((data[:,17]) - (((data[:,17]) * (((((data[:,17]) * (data[:,17]))) * 2.0)))))) + (((((((data[:,17]) * 2.0)) * 2.0)) * 2.0)))/2.0)) / 2.0)) + (((data[:,17]) - ((7.76236486434936523)))))/2.0)) + (((((((data[:,17]) * (data[:,17]))) * 2.0)) * 2.0)))) +\n                0.250000*np.tanh(((((((((((data[:,17]) + (data[:,17]))/2.0)) + ((((data[:,17]) + ((((data[:,17]) + (data[:,17]))/2.0)))/2.0)))/2.0)) * (data[:,17]))) - (data[:,13]))) +\n                0.250000*np.tanh((((((data[:,19]) + (((((((((data[:,19]) * 2.0)) + (data[:,19]))) - ((1.07645297050476074)))) + (((data[:,22]) + (data[:,22]))))))/2.0)) + (((data[:,22]) + (data[:,4]))))))\n    \n    def GP_class_1(self,data):\n        return(0.250000*np.tanh(((((((((((((data[:,3]) + (((np.tanh((data[:,2]))) - (data[:,14]))))) - ((10.0)))) - ((4.88989353179931641)))) - ((13.92175483703613281)))) / 2.0)) - (data[:,6]))) +\n                0.250000*np.tanh(data[:,3]) +\n                0.250000*np.tanh(((data[:,2]) + (((((np.tanh((((data[:,14]) * 2.0)))) - (((((((data[:,7]) * 2.0)) - (((((data[:,0]) - (data[:,14]))) - (data[:,7]))))) - (data[:,0]))))) + (data[:,0]))))) +\n                0.250000*np.tanh(((((((data[:,0]) - ((-1.0*((data[:,14])))))) + ((((((((((data[:,0]) - (data[:,7]))) - ((((data[:,14]) + (data[:,14]))/2.0)))) + (((data[:,2]) - (((data[:,0]) * (data[:,14]))))))/2.0)) - (data[:,14]))))) - (data[:,7]))) +\n                0.250000*np.tanh(((((data[:,0]) / 2.0)) + (data[:,0]))) +\n                0.250000*np.tanh(((((data[:,19]) - (data[:,19]))) / 2.0)) +\n                0.250000*np.tanh(((((np.tanh((((np.tanh((data[:,5]))) - ((((1.69341480731964111)) + ((((((np.tanh((data[:,14]))) - (data[:,14]))) + (data[:,14]))/2.0)))))))) - (data[:,2]))) - (((data[:,14]) * 2.0)))) +\n                0.250000*np.tanh((((((data[:,22]) + (((data[:,0]) + ((((((((data[:,22]) / 2.0)) + ((-1.0*((data[:,13])))))/2.0)) + (data[:,19]))))))/2.0)) - (data[:,13]))) +\n                0.250000*np.tanh(((((((data[:,17]) / 2.0)) * (data[:,17]))) * (((data[:,17]) + (((data[:,17]) * (((((((((data[:,17]) + (data[:,17]))) * (data[:,17]))) * (data[:,17]))) + (((np.tanh((data[:,17]))) / 2.0)))))))))) +\n                0.250000*np.tanh((((((data[:,14]) * (((data[:,13]) + (data[:,3]))))) + (((data[:,14]) * (((data[:,14]) - ((-1.0*((data[:,3])))))))))/2.0)))\n    \n    def GP_class_2(self,data):\n        return(0.250000*np.tanh(((data[:,0]) + (np.tanh((((((((((data[:,2]) + (((data[:,2]) / 2.0)))/2.0)) + (data[:,0]))/2.0)) * (data[:,7]))))))) +\n                0.250000*np.tanh((((data[:,22]) + (data[:,22]))/2.0)) +\n                0.250000*np.tanh((((((data[:,14]) + ((-1.0*((((((4.73206138610839844)) + (((((((((((np.tanh((data[:,7]))) + (data[:,1]))/2.0)) - (np.tanh((((((data[:,0]) - (data[:,20]))) - (data[:,14]))))))) * ((7.0)))) + (data[:,14]))/2.0)))/2.0))))))/2.0)) - (data[:,14]))) +\n                0.250000*np.tanh(((((data[:,1]) + ((4.15603733062744141)))) / 2.0)) +\n                0.250000*np.tanh(((((((((((((data[:,0]) - (data[:,7]))) - (data[:,7]))) + ((((data[:,0]) + (data[:,0]))/2.0)))) * 2.0)) + (data[:,0]))) * 2.0)) +\n                0.250000*np.tanh(((((data[:,11]) / 2.0)) / 2.0)) +\n                0.250000*np.tanh(((((((((((((((((((((((data[:,2]) / 2.0)) + (((data[:,2]) / 2.0)))/2.0)) / 2.0)) / 2.0)) / 2.0)) + ((((-1.0*((((data[:,0]) / 2.0))))) * (((data[:,0]) / 2.0)))))/2.0)) + (data[:,0]))/2.0)) + (np.tanh((data[:,0]))))/2.0)) / 2.0)) +\n                0.250000*np.tanh(((((data[:,0]) + (((data[:,0]) + (((((((data[:,14]) * (data[:,14]))) + (data[:,14]))) - (data[:,0]))))))) - (data[:,14]))) +\n                0.250000*np.tanh((((((np.tanh((np.tanh(((0.0)))))) * (data[:,15]))) + (data[:,20]))/2.0)) +\n                0.250000*np.tanh(((data[:,14]) * ((((data[:,13]) + ((((data[:,13]) + ((((data[:,14]) + ((((((data[:,14]) + ((((data[:,14]) + (((data[:,13]) - ((((-1.0*(((((data[:,14]) + (data[:,14]))/2.0))))) * 2.0)))))/2.0)))) + (data[:,14]))/2.0)))/2.0)))/2.0)))/2.0)))))\n    \n    def GP_class_3(self,data):\n        return(0.250000*np.tanh(((((((((8.0)) + (((((((5.33416414260864258)) + ((9.0)))/2.0)) + ((((8.0)) * 2.0)))))) * 2.0)) + ((5.33416414260864258)))/2.0)) +\n                0.250000*np.tanh((((9.48088836669921875)) + (((((10.56953334808349609)) + ((((4.48959350585937500)) + (np.tanh(((((3.0)) + ((9.0)))))))))/2.0)))) +\n                0.250000*np.tanh((((((((((data[:,22]) / 2.0)) + (data[:,22]))/2.0)) * 2.0)) / 2.0)) +\n                0.250000*np.tanh((10.44883441925048828)) +\n                0.250000*np.tanh(((((data[:,0]) + (((data[:,0]) + ((-1.0*((data[:,9])))))))) + (((((data[:,0]) - (data[:,7]))) - (data[:,9]))))) +\n                0.250000*np.tanh(((np.tanh((data[:,6]))) - (data[:,21]))) +\n                0.250000*np.tanh(((data[:,0]) - (((((data[:,14]) + (((((data[:,14]) + (((data[:,14]) + (((data[:,14]) - (data[:,0]))))))) - (((((data[:,0]) - (data[:,14]))) + (data[:,14]))))))) - (((((data[:,14]) * (data[:,14]))) + (data[:,18]))))))) +\n                0.250000*np.tanh(((data[:,14]) * ((((data[:,13]) + (data[:,14]))/2.0)))) +\n                0.250000*np.tanh((((((data[:,8]) * (((data[:,8]) / 2.0)))) + ((((((data[:,9]) * (data[:,8]))) + (data[:,8]))/2.0)))/2.0)) +\n                0.250000*np.tanh((((data[:,2]) + (((((((data[:,2]) + (data[:,20]))/2.0)) + ((((((((data[:,2]) + (data[:,8]))/2.0)) / 2.0)) - (data[:,8]))))/2.0)))/2.0)))\n    \n    def GP_class_4(self,data):\n        return(0.250000*np.tanh((((12.98819637298583984)) / 2.0)) +\n                0.250000*np.tanh(((((4.75780344009399414)) + (((((((8.66014671325683594)) + ((4.18107128143310547)))/2.0)) * ((8.97841739654541016)))))/2.0)) +\n                0.250000*np.tanh((((9.0)) / 2.0)) +\n                0.250000*np.tanh(((((((((((10.0)) + (np.tanh((((((6.0)) + ((((13.80149555206298828)) + ((7.0)))))/2.0)))))) + ((10.0)))) + (((((((data[:,0]) + ((8.0)))) / 2.0)) / 2.0)))/2.0)) + ((((-1.0*((((data[:,2]) * (data[:,9])))))) * 2.0)))) +\n                0.250000*np.tanh(((((data[:,0]) + (((((((data[:,22]) + (data[:,22]))/2.0)) + (data[:,22]))/2.0)))) + ((((((data[:,22]) + ((((((data[:,22]) * 2.0)) + (data[:,22]))/2.0)))) + (data[:,18]))/2.0)))) +\n                0.250000*np.tanh(((((((((((((np.tanh((data[:,18]))) + ((((data[:,1]) + (data[:,18]))/2.0)))/2.0)) * (data[:,18]))) * (data[:,18]))) + (((data[:,18]) * (data[:,18]))))/2.0)) - (((((((np.tanh((data[:,18]))) + (((data[:,18]) * (data[:,18]))))/2.0)) + (data[:,17]))/2.0)))) +\n                0.250000*np.tanh(((np.tanh(((((((np.tanh((data[:,6]))) - (((np.tanh((data[:,6]))) - (data[:,6]))))) + (np.tanh((data[:,6]))))/2.0)))) - ((((data[:,10]) + ((((data[:,2]) + (((np.tanh((data[:,6]))) - (((np.tanh((np.tanh((data[:,6]))))) - (data[:,10]))))))/2.0)))/2.0)))) +\n                0.250000*np.tanh((((((((((((data[:,10]) + (data[:,4]))) + (((data[:,10]) + (((data[:,0]) * (data[:,10]))))))/2.0)) + (((data[:,4]) * (((data[:,10]) + (data[:,4]))))))/2.0)) + (data[:,7]))/2.0)) +\n                0.250000*np.tanh((((((data[:,11]) + (((((data[:,18]) / 2.0)) * (((data[:,11]) * (data[:,18]))))))/2.0)) * (((data[:,18]) * (((((((((data[:,18]) * (((((((data[:,11]) / 2.0)) / 2.0)) * (data[:,18]))))) / 2.0)) / 2.0)) * ((-1.0*((data[:,21])))))))))) +\n                0.250000*np.tanh((((-1.0*((((((((((((data[:,2]) - (data[:,2]))) * (np.tanh((data[:,2]))))) + (np.tanh((((data[:,14]) * 2.0)))))/2.0)) + (data[:,2]))/2.0))))) - (data[:,2]))))\n    \n    def GP_class_5(self,data):\n        return(0.250000*np.tanh(((((((((((3.46574378013610840)) + ((((np.tanh(((3.46574378013610840)))) + ((8.46705245971679688)))/2.0)))/2.0)) * 2.0)) + ((((3.46574378013610840)) + ((4.0)))))) + ((((4.0)) * 2.0)))) +\n                0.250000*np.tanh((((10.55856513977050781)) / 2.0)) +\n                0.250000*np.tanh((((3.76695251464843750)) / 2.0)) +\n                0.250000*np.tanh((((((6.0)) / 2.0)) + ((((8.57984828948974609)) * 2.0)))) +\n                0.250000*np.tanh((((((((((((3.42168045043945312)) + (data[:,7]))) + (np.tanh((data[:,7]))))) + (np.tanh((np.tanh((np.tanh(((3.42168045043945312)))))))))/2.0)) + (data[:,7]))/2.0)) +\n                0.250000*np.tanh(((((((((((data[:,6]) - (data[:,9]))) + (data[:,7]))/2.0)) * 2.0)) + ((((((data[:,7]) + (data[:,9]))/2.0)) - (data[:,9]))))/2.0)) +\n                0.250000*np.tanh(((((((((((data[:,22]) + (data[:,22]))) + (data[:,11]))/2.0)) + (data[:,22]))/2.0)) / 2.0)) +\n                0.250000*np.tanh((((((((((0.76044219732284546)) / 2.0)) + ((0.76044219732284546)))/2.0)) + (np.tanh((np.tanh((((((((0.76043862104415894)) / 2.0)) + ((1.0)))/2.0)))))))/2.0)) +\n                0.250000*np.tanh(((((((data[:,12]) + (np.tanh((data[:,9]))))/2.0)) + (((((((((((data[:,12]) + (((((0.0)) + (np.tanh((((((((data[:,7]) + (data[:,6]))/2.0)) + (data[:,7]))/2.0)))))/2.0)))/2.0)) / 2.0)) / 2.0)) + (data[:,7]))/2.0)))/2.0)) +\n                0.250000*np.tanh(((((((((data[:,20]) / 2.0)) / 2.0)) * (((np.tanh((data[:,20]))) * (((data[:,20]) / 2.0)))))) * ((((((data[:,20]) / 2.0)) + (((data[:,20]) / 2.0)))/2.0)))))\n    \n    def GP_class_6(self,data):\n        return(0.250000*np.tanh(((((((11.68076229095458984)) + (((((11.68075847625732422)) + ((((11.47270107269287109)) + (((((11.47269725799560547)) + ((11.68076229095458984)))/2.0)))))/2.0)))/2.0)) + ((((4.0)) * 2.0)))) +\n                0.250000*np.tanh((((8.0)) + ((((10.0)) + ((6.59042119979858398)))))) +\n                0.250000*np.tanh(((((((((((13.33760547637939453)) + (((((3.86388397216796875)) + (((((((8.18321037292480469)) + (data[:,4]))) + (((((((5.95386552810668945)) + ((12.93883609771728516)))) + (((((10.73907089233398438)) + ((4.0)))/2.0)))/2.0)))/2.0)))/2.0)))/2.0)) + (np.tanh(((3.0)))))) - ((2.0)))) + (((((10.61559963226318359)) + (data[:,1]))/2.0)))) +\n                0.250000*np.tanh(((np.tanh(((((((((((7.13513946533203125)) + ((12.48778533935546875)))/2.0)) + (((((((7.13513565063476562)) - ((8.12031841278076172)))) + (((((((((7.13513565063476562)) + ((((10.69830417633056641)) + ((10.69830799102783203)))))) + ((10.69830799102783203)))) + ((7.83078956604003906)))/2.0)))/2.0)))/2.0)) * 2.0)))) + ((3.0)))) +\n                0.250000*np.tanh(((((((((((data[:,7]) - (data[:,0]))) * 2.0)) - (data[:,7]))) + (data[:,7]))) + (data[:,0]))) +\n                0.250000*np.tanh(((((data[:,7]) / 2.0)) / 2.0)) +\n                0.250000*np.tanh((((((((((((((((1.0)) + (((((((data[:,2]) + (((((((1.0)) * (data[:,2]))) + (data[:,14]))/2.0)))/2.0)) + ((1.0)))/2.0)))) + ((((1.0)) - (((data[:,2]) * (data[:,14]))))))/2.0)) - (data[:,2]))) * 2.0)) + (data[:,14]))/2.0)) + (data[:,14]))) +\n                0.250000*np.tanh(((np.tanh((np.tanh(((((((data[:,19]) * ((0.0)))) + ((0.0)))/2.0)))))) / 2.0)) +\n                0.250000*np.tanh((((((((0.0)) / 2.0)) / 2.0)) * (((((((((data[:,15]) / 2.0)) * ((0.0)))) / 2.0)) * ((((-1.0*(((0.0))))) / 2.0)))))) +\n                0.250000*np.tanh(((((data[:,15]) * (((((((((((((np.tanh(((((0.09032609313726425)) / 2.0)))) / 2.0)) / 2.0)) / 2.0)) / 2.0)) / 2.0)) / 2.0)))) / 2.0)))\n    \n    def GP_class_7(self,data):\n        return(0.250000*np.tanh((10.27210903167724609)) +\n                0.250000*np.tanh((((((((((10.0)) + (((((((((8.26972770690917969)) + ((11.06334972381591797)))/2.0)) * 2.0)) / 2.0)))) + ((((((5.0)) + ((((5.0)) + ((7.92727756500244141)))))) * 2.0)))) * ((7.92728137969970703)))) + ((10.0)))) +\n                0.250000*np.tanh((8.42519950866699219)) +\n                0.250000*np.tanh(((((((data[:,14]) + (data[:,14]))) + ((((((1.59392631053924561)) + (data[:,14]))) + (data[:,14]))))) + (np.tanh((((((data[:,14]) + (((((((3.0)) + (((data[:,14]) * 2.0)))/2.0)) + ((1.59391915798187256)))))) + ((1.59392631053924561)))))))) +\n                0.250000*np.tanh((((data[:,7]) + (((data[:,0]) + (((((((data[:,7]) + (data[:,4]))/2.0)) + (((data[:,7]) * 2.0)))/2.0)))))/2.0)) +\n                0.250000*np.tanh((((((((((((((data[:,9]) - (((data[:,0]) - (data[:,0]))))) - (((data[:,0]) / 2.0)))) / 2.0)) - (((data[:,21]) - (data[:,7]))))) + (data[:,21]))/2.0)) - (data[:,0]))) +\n                0.250000*np.tanh((((((((((2.0)) + ((((((data[:,21]) + ((((data[:,13]) + (((((((((((data[:,13]) * 2.0)) + (data[:,21]))/2.0)) + (data[:,13]))/2.0)) * 2.0)))/2.0)))/2.0)) - (np.tanh(((((0.0)) - (data[:,21]))))))))) + ((1.0)))/2.0)) + (data[:,13]))/2.0)) +\n                0.250000*np.tanh(((((data[:,14]) / 2.0)) / 2.0)) +\n                0.250000*np.tanh((((((((((((((((-1.0*(((((((data[:,21]) / 2.0)) + (data[:,8]))/2.0))))) / 2.0)) / 2.0)) / 2.0)) + (data[:,9]))/2.0)) / 2.0)) + ((((((((((((data[:,7]) / 2.0)) / 2.0)) / 2.0)) / 2.0)) + (data[:,7]))/2.0)))/2.0)) +\n                0.250000*np.tanh(((((((np.tanh(((1.0)))) / 2.0)) / 2.0)) / 2.0)))\n    \n    def GP_class_8(self,data):\n        return(0.250000*np.tanh(((((((data[:,0]) + (((((6.71804094314575195)) + (data[:,15]))/2.0)))) + ((((((11.56385326385498047)) * 2.0)) * ((((10.48986148834228516)) + ((11.56385326385498047)))))))) / 2.0)) +\n                0.250000*np.tanh((((((((((data[:,14]) / 2.0)) + ((2.83755254745483398)))/2.0)) + ((((((data[:,12]) + (data[:,14]))/2.0)) + (data[:,14]))))) + ((((data[:,9]) + (((data[:,12]) + (data[:,14]))))/2.0)))) +\n                0.250000*np.tanh(((((((((data[:,6]) + (((((((3.50821948051452637)) + ((((data[:,21]) + ((11.92932796478271484)))/2.0)))) + ((1.0)))/2.0)))) + (data[:,10]))/2.0)) + (data[:,21]))/2.0)) +\n                0.250000*np.tanh(((data[:,14]) + (np.tanh((np.tanh(((-1.0*((((data[:,22]) - (data[:,13])))))))))))) +\n                0.250000*np.tanh(((((1.0)) + ((((data[:,7]) + (data[:,7]))/2.0)))/2.0)) +\n                0.250000*np.tanh(((((((((data[:,21]) / 2.0)) / 2.0)) * (((((((np.tanh((((np.tanh((data[:,13]))) / 2.0)))) / 2.0)) + (((((((np.tanh(((((data[:,21]) + (data[:,21]))/2.0)))) / 2.0)) - (data[:,12]))) / 2.0)))) / 2.0)))) - (data[:,12]))) +\n                0.250000*np.tanh((((data[:,7]) + (np.tanh(((((data[:,12]) + (((data[:,7]) * ((((data[:,13]) + (data[:,13]))/2.0)))))/2.0)))))/2.0)) +\n                0.250000*np.tanh(((((((data[:,13]) / 2.0)) / 2.0)) / 2.0)) +\n                0.250000*np.tanh((((((((((np.tanh((data[:,9]))) + (data[:,13]))/2.0)) + ((((((((data[:,21]) / 2.0)) * 2.0)) + ((((3.94983267784118652)) / 2.0)))/2.0)))/2.0)) + ((((np.tanh(((3.94983267784118652)))) + (data[:,9]))/2.0)))/2.0)) +\n                0.250000*np.tanh(((((((((((((((data[:,14]) * (((((((((data[:,19]) / 2.0)) * (np.tanh((((data[:,2]) / 2.0)))))) / 2.0)) / 2.0)))) / 2.0)) / 2.0)) / 2.0)) / 2.0)) / 2.0)) * (((((data[:,11]) / 2.0)) / 2.0)))))\n    \n    def GP_class_9(self,data):\n        return(0.250000*np.tanh(((((((data[:,14]) * 2.0)) / 2.0)) + (((((data[:,14]) + (np.tanh((((data[:,14]) * 2.0)))))) + (data[:,14]))))) +\n                0.250000*np.tanh(((data[:,9]) - (((((-1.0*((data[:,11])))) + (data[:,9]))/2.0)))) +\n                0.250000*np.tanh(((data[:,21]) * 2.0)) +\n                0.250000*np.tanh((((((data[:,14]) + (data[:,14]))/2.0)) + ((((data[:,13]) + ((((np.tanh((((data[:,9]) + ((6.0)))))) + (data[:,9]))/2.0)))/2.0)))) +\n                0.250000*np.tanh(np.tanh(((1.0)))) +\n                0.250000*np.tanh(((np.tanh((np.tanh(((((np.tanh((((data[:,20]) * (((((((np.tanh((((data[:,20]) / 2.0)))) + (data[:,12]))/2.0)) + ((((data[:,15]) + ((((data[:,17]) + (data[:,9]))/2.0)))/2.0)))/2.0)))))) + (data[:,9]))/2.0)))))) / 2.0)) +\n                0.250000*np.tanh(((((data[:,2]) - (data[:,11]))) / 2.0)) +\n                0.250000*np.tanh((((((((data[:,13]) / 2.0)) + (((data[:,7]) + (data[:,7]))))/2.0)) - (((((data[:,22]) + (((((data[:,14]) * (((((data[:,7]) * (((data[:,13]) - ((((data[:,7]) + (((((data[:,13]) / 2.0)) + (data[:,7]))))/2.0)))))) / 2.0)))) * 2.0)))) / 2.0)))) +\n                0.250000*np.tanh(((((((((((((((data[:,1]) / 2.0)) + ((1.0)))/2.0)) + ((1.0)))/2.0)) + ((((((((((1.0)) / 2.0)) + (np.tanh(((1.0)))))/2.0)) + (np.tanh((((((1.0)) + ((1.0)))/2.0)))))/2.0)))/2.0)) + ((((data[:,20]) + ((1.0)))/2.0)))/2.0)) +\n                0.250000*np.tanh(((data[:,1]) * ((((data[:,14]) + (np.tanh(((((0.0)) / 2.0)))))/2.0)))))\n    \n    def GP_class_10(self,data):\n        return(0.250000*np.tanh((((((((((data[:,2]) + (((data[:,9]) * (np.tanh((data[:,14]))))))/2.0)) + ((((((data[:,14]) + (data[:,20]))/2.0)) + (data[:,0]))))/2.0)) + (data[:,14]))/2.0)) +\n                0.250000*np.tanh((((((data[:,9]) + ((-1.0*((((((((2.04309988021850586)) + (((data[:,11]) + (((np.tanh((data[:,0]))) / 2.0)))))/2.0)) * 2.0))))))/2.0)) + (data[:,2]))) +\n                0.250000*np.tanh(data[:,21]) +\n                0.250000*np.tanh((((((((data[:,14]) + (np.tanh((((data[:,6]) / 2.0)))))/2.0)) + (data[:,14]))) + (((((data[:,14]) * 2.0)) + ((((data[:,13]) + (data[:,14]))/2.0)))))) +\n                0.250000*np.tanh(data[:,9]) +\n                0.250000*np.tanh(((((((data[:,15]) * 2.0)) / 2.0)) / 2.0)) +\n                0.250000*np.tanh((((((((2.72836518287658691)) * ((((1.0)) / 2.0)))) / 2.0)) / 2.0)) +\n                0.250000*np.tanh(((((np.tanh((((data[:,7]) + (np.tanh(((((((((((0.0)) / 2.0)) / 2.0)) / 2.0)) / 2.0)))))))) / 2.0)) / 2.0)) +\n                0.250000*np.tanh(np.tanh((((np.tanh(((11.43236827850341797)))) / 2.0)))) +\n                0.250000*np.tanh(np.tanh((((data[:,7]) / 2.0)))))\n    \n    def GP_class_11(self,data):\n        return(0.250000*np.tanh((-1.0*((((((((((((((data[:,5]) - (((data[:,9]) - ((((-1.0*(((11.40053558349609375))))) / 2.0)))))) + ((((11.40053558349609375)) + ((((11.40053558349609375)) * 2.0)))))) / 2.0)) - (data[:,7]))) - ((-1.0*(((11.40053939819335938))))))) * 2.0))))) +\n                0.250000*np.tanh(((((data[:,14]) * 2.0)) + (data[:,14]))) +\n                0.250000*np.tanh(((data[:,2]) + (((((data[:,5]) + (data[:,3]))) + ((((data[:,2]) + (((data[:,3]) * 2.0)))/2.0)))))) +\n                0.250000*np.tanh(((((((((((data[:,21]) + (data[:,21]))) + ((-1.0*(((((np.tanh((np.tanh((((data[:,14]) / 2.0)))))) + (data[:,21]))/2.0))))))/2.0)) * 2.0)) + ((((data[:,14]) + (data[:,3]))/2.0)))/2.0)) +\n                0.250000*np.tanh(((((((data[:,0]) + ((((data[:,9]) + ((((data[:,9]) + (((((((((((((((data[:,9]) / 2.0)) - (data[:,10]))) + (data[:,5]))/2.0)) - (data[:,9]))) + (data[:,9]))/2.0)) / 2.0)))/2.0)))/2.0)))/2.0)) + (data[:,9]))/2.0)) +\n                0.250000*np.tanh((((((((data[:,13]) + ((((data[:,13]) + (data[:,13]))/2.0)))) + (data[:,1]))/2.0)) / 2.0)) +\n                0.250000*np.tanh(data[:,14]) +\n                0.250000*np.tanh(((((data[:,2]) / 2.0)) / 2.0)) +\n                0.250000*np.tanh((((data[:,9]) + (data[:,9]))/2.0)) +\n                0.250000*np.tanh(data[:,2]))\n    \n    def GP_class_12(self,data):\n        return(0.250000*np.tanh((((((((-1.0*((data[:,18])))) - (np.tanh((data[:,9]))))) + ((8.0)))) - ((14.74865913391113281)))) +\n                0.250000*np.tanh((((((data[:,21]) + (np.tanh((data[:,9]))))) + ((((((((data[:,5]) / 2.0)) * 2.0)) + (data[:,5]))/2.0)))/2.0)) +\n                0.250000*np.tanh(((((data[:,14]) + (data[:,14]))) + (((data[:,14]) * 2.0)))) +\n                0.250000*np.tanh(((data[:,2]) + (((((((data[:,14]) + ((((np.tanh((data[:,10]))) + (data[:,17]))/2.0)))/2.0)) + (data[:,21]))/2.0)))) +\n                0.250000*np.tanh((((data[:,9]) + ((((((((((((data[:,9]) + ((((((data[:,9]) / 2.0)) + (((data[:,9]) * 2.0)))/2.0)))/2.0)) + ((-1.0*((data[:,21])))))/2.0)) * (data[:,9]))) + (((((np.tanh((((data[:,21]) / 2.0)))) / 2.0)) / 2.0)))/2.0)))/2.0)) +\n                0.250000*np.tanh(((((0.0)) + (data[:,13]))/2.0)) +\n                0.250000*np.tanh((((((((data[:,2]) / 2.0)) * 2.0)) + (data[:,7]))/2.0)) +\n                0.250000*np.tanh((((((data[:,14]) * 2.0)) + (np.tanh((data[:,14]))))/2.0)) +\n                0.250000*np.tanh(np.tanh((((((((((-1.0*((((np.tanh((data[:,9]))) / 2.0))))) / 2.0)) / 2.0)) + (((data[:,3]) * (data[:,13]))))/2.0)))) +\n                0.250000*np.tanh((-1.0*((((((((-1.0*(((((((-1.0*((data[:,18])))) - ((((0.0)) / 2.0)))) / 2.0))))) + (np.tanh(((((-1.0*(((-1.0*((data[:,22]))))))) / 2.0)))))/2.0)) / 2.0))))))\n    \n    def GP_class_13(self,data):\n        return(0.250000*np.tanh(((((np.tanh((np.tanh((data[:,2]))))) - ((10.0)))) - (np.tanh((((((data[:,7]) - ((((((6.0)) - ((4.87243366241455078)))) - ((9.0)))))) - ((14.80352973937988281)))))))) +\n                0.250000*np.tanh(((np.tanh((((((3.0)) + (data[:,1]))/2.0)))) - ((7.0)))) +\n                0.250000*np.tanh(((((np.tanh((np.tanh((np.tanh((data[:,6]))))))) * 2.0)) - ((6.10337877273559570)))) +\n                0.250000*np.tanh(((((((data[:,9]) + (data[:,14]))) + (data[:,14]))) + (data[:,14]))) +\n                0.250000*np.tanh(((((((np.tanh(((((-1.0*(((((((data[:,2]) + ((10.0)))/2.0)) / 2.0))))) - ((13.28130435943603516)))))) + ((10.0)))) - ((13.28130435943603516)))) + ((-1.0*((data[:,5])))))) +\n                0.250000*np.tanh(((((((((((data[:,20]) + (((((((((data[:,20]) * 2.0)) * 2.0)) * 2.0)) + (data[:,13]))))/2.0)) + (((data[:,9]) + (((data[:,9]) + (((data[:,6]) + (data[:,9]))))))))/2.0)) + (data[:,20]))) + (data[:,15]))) +\n                0.250000*np.tanh(((((((((data[:,14]) / 2.0)) + ((((data[:,9]) + (((data[:,15]) + (((data[:,5]) + ((((((data[:,9]) + (((data[:,5]) + (data[:,14]))))) + (data[:,9]))/2.0)))))))/2.0)))) * 2.0)) + (data[:,14]))) +\n                0.250000*np.tanh((((data[:,13]) + (data[:,7]))/2.0)) +\n                0.250000*np.tanh(((data[:,9]) + ((((data[:,3]) + ((((data[:,21]) + (data[:,3]))/2.0)))/2.0)))) +\n                0.250000*np.tanh((((data[:,2]) + (((((((((((((((data[:,20]) + (data[:,1]))) + (((data[:,2]) + (data[:,14]))))) + (data[:,2]))) + (np.tanh((data[:,2]))))/2.0)) + (((data[:,14]) + (data[:,2]))))) + (data[:,14]))/2.0)))/2.0)))\n    \n    def GP_class_14(self,data):\n        return(0.250000*np.tanh((((((((((((5.54024744033813477)) - ((((((9.0)) * 2.0)) * 2.0)))) - (data[:,0]))) - ((((7.0)) + ((4.74105215072631836)))))) - (((((7.0)) + (((data[:,5]) * 2.0)))/2.0)))) - ((9.0)))) +\n                0.250000*np.tanh(((((((((9.0)) - ((14.27298927307128906)))) - (data[:,0]))) + (np.tanh(((((12.77708148956298828)) - ((14.80560111999511719)))))))/2.0)) +\n                0.250000*np.tanh(((((((0.0)) + ((-1.0*((((data[:,22]) + (((((((((13.30077362060546875)) - (np.tanh((data[:,4]))))) + (((data[:,22]) / 2.0)))/2.0)) / 2.0))))))))/2.0)) - (((np.tanh((((data[:,7]) - (data[:,7]))))) / 2.0)))) +\n                0.250000*np.tanh(((((((data[:,6]) + (data[:,14]))/2.0)) + (((np.tanh((((((data[:,14]) + (data[:,14]))) + (((data[:,6]) - (((data[:,5]) + (((np.tanh((data[:,21]))) + ((((data[:,14]) + ((((data[:,14]) + (data[:,14]))/2.0)))/2.0)))))))))))) + (data[:,5]))))/2.0)) +\n                0.250000*np.tanh((((((((((((((((((data[:,2]) + (data[:,16]))) + ((((((data[:,2]) / 2.0)) + (data[:,9]))/2.0)))/2.0)) + (np.tanh((data[:,10]))))/2.0)) + (data[:,9]))) + (data[:,9]))/2.0)) + (data[:,21]))) + (data[:,3]))) +\n                0.250000*np.tanh((((data[:,13]) + (((((((((((data[:,3]) / 2.0)) / 2.0)) / 2.0)) * (data[:,3]))) / 2.0)))/2.0)) +\n                0.250000*np.tanh(((data[:,14]) + (((((((data[:,14]) / 2.0)) + (data[:,14]))) * 2.0)))) +\n                0.250000*np.tanh(data[:,9]) +\n                0.250000*np.tanh(((data[:,1]) / 2.0)) +\n                0.250000*np.tanh((((((data[:,2]) + ((((((data[:,13]) + (data[:,14]))) + (data[:,13]))/2.0)))) + (np.tanh((data[:,2]))))/2.0)))\n    \n    def GP_class_15(self,data):\n        return(0.250000*np.tanh((((((((4.0)) + ((((8.0)) / 2.0)))) - ((9.56193733215332031)))) - (((((np.tanh(((13.93556308746337891)))) - (data[:,18]))) + ((13.93556308746337891)))))) +\n                0.250000*np.tanh(((data[:,4]) - ((((12.76094818115234375)) - (data[:,8]))))) +\n                0.250000*np.tanh((((-1.0*(((12.35446166992187500))))) - (((((12.35446166992187500)) + ((((((12.35446166992187500)) - ((-1.0*(((0.0))))))) - (data[:,0]))))/2.0)))) +\n                0.250000*np.tanh(((data[:,21]) - ((14.46367263793945312)))) +\n                0.250000*np.tanh(((((data[:,2]) + ((((data[:,9]) + ((-1.0*(((-1.0*(((-1.0*((((data[:,7]) * (((data[:,21]) * 2.0))))))))))))))/2.0)))) + (data[:,9]))) +\n                0.250000*np.tanh(((((((((((data[:,7]) + (data[:,3]))/2.0)) + (data[:,20]))/2.0)) - ((((2.33354020118713379)) / 2.0)))) - ((((2.0)) + ((8.78930377960205078)))))) +\n                0.250000*np.tanh(((((((data[:,14]) + (((((((data[:,14]) + (((data[:,15]) + (data[:,14]))))) + (((data[:,15]) + (data[:,14]))))) + (((data[:,14]) / 2.0)))))) + (data[:,14]))) + (data[:,14]))) +\n                0.250000*np.tanh(((((((((data[:,13]) + (data[:,13]))) + (((data[:,13]) + (data[:,12]))))) + (data[:,9]))) + ((((((data[:,9]) + (((data[:,13]) + ((((data[:,9]) + (((((data[:,13]) * 2.0)) / 2.0)))/2.0)))))/2.0)) * 2.0)))) +\n                0.250000*np.tanh((((data[:,9]) + ((((((data[:,9]) + (data[:,9]))/2.0)) / 2.0)))/2.0)) +\n                0.250000*np.tanh((((((data[:,14]) + (data[:,8]))) + (data[:,14]))/2.0)))\n    \n    def GP_class_16(self,data):\n        return(0.250000*np.tanh(((((((((((((9.37592792510986328)) - ((13.36316204071044922)))) + ((11.89122962951660156)))/2.0)) - ((((9.0)) - (((((((data[:,13]) / 2.0)) / 2.0)) + ((((-1.0*((((data[:,4]) / 2.0))))) * 2.0)))))))) - ((12.24639320373535156)))) - ((11.89122581481933594)))) +\n                0.250000*np.tanh(((((np.tanh(((4.51821422576904297)))) - ((13.23712635040283203)))) - ((((((13.23712635040283203)) + (((data[:,22]) - ((((11.40539550781250000)) - (((((9.0)) + ((((((((data[:,7]) + (((data[:,17]) / 2.0)))/2.0)) * 2.0)) / 2.0)))/2.0)))))))) - (data[:,5]))))) +\n                0.250000*np.tanh(((data[:,14]) - ((((4.0)) + (((((13.41770648956298828)) + ((12.42538261413574219)))/2.0)))))) +\n                0.250000*np.tanh((((5.0)) - ((9.27778816223144531)))) +\n                0.250000*np.tanh(((((data[:,16]) - (((((((data[:,1]) - (((np.tanh((((((((9.29507255554199219)) - (((((((14.34461498260498047)) * 2.0)) + (((((7.0)) + ((9.10683441162109375)))/2.0)))/2.0)))) + ((4.21145153045654297)))/2.0)))) * 2.0)))) - ((9.10683441162109375)))) * 2.0)))) - ((((14.34461498260498047)) * 2.0)))) +\n                0.250000*np.tanh((((((((((data[:,17]) / 2.0)) + (((data[:,13]) + (data[:,9]))))/2.0)) + (((data[:,17]) + (data[:,9]))))) + (data[:,0]))) +\n                0.250000*np.tanh(((data[:,13]) + (((data[:,14]) - (data[:,21]))))) +\n                0.250000*np.tanh((((((data[:,14]) + (data[:,5]))/2.0)) + (((((((data[:,9]) / 2.0)) + (data[:,14]))) / 2.0)))) +\n                0.250000*np.tanh((((((data[:,17]) + (data[:,9]))) + (((((data[:,17]) / 2.0)) + ((((((((((data[:,9]) + (data[:,9]))) - ((3.0)))) + (data[:,7]))) + (data[:,9]))/2.0)))))/2.0)) +\n                0.250000*np.tanh(np.tanh(((((((((((((((data[:,2]) + (((((((((((data[:,22]) + (data[:,21]))/2.0)) + (data[:,2]))/2.0)) * 2.0)) + ((((((data[:,21]) * 2.0)) + (data[:,21]))/2.0)))))) + (data[:,2]))/2.0)) + (data[:,17]))/2.0)) + (np.tanh((data[:,8]))))/2.0)) + (data[:,2]))))))\n    \n    def GP_class_17(self,data):\n        return(0.250000*np.tanh(((((data[:,14]) - ((((data[:,5]) + (((((14.43326377868652344)) + ((6.0)))/2.0)))/2.0)))) - (((((((10.0)) + ((14.84462165832519531)))/2.0)) - ((((7.0)) + (data[:,13]))))))) +\n                0.250000*np.tanh((((((((((data[:,14]) + (((((((((data[:,11]) - ((9.71331787109375000)))) / 2.0)) + ((9.0)))) - ((3.0)))))/2.0)) - ((10.0)))) - ((((10.0)) + ((((9.0)) - ((((((10.0)) - ((9.0)))) / 2.0)))))))) - ((10.0)))) +\n                0.250000*np.tanh((((((((((8.0)) - ((12.78277492523193359)))) - (data[:,10]))) - (np.tanh(((((((7.0)) - ((14.95321178436279297)))) - ((8.0)))))))) - ((14.95321178436279297)))) +\n                0.250000*np.tanh((((7.45682907104492188)) - ((14.98023796081542969)))) +\n                0.250000*np.tanh(((((((((((7.0)) + (((((np.tanh((((((((8.0)) * (data[:,8]))) + (data[:,11]))/2.0)))) / 2.0)) - ((8.0)))))/2.0)) - (data[:,20]))) - ((((8.0)) * 2.0)))) + ((8.0)))) +\n                0.250000*np.tanh(((data[:,8]) + (((data[:,4]) + (((np.tanh((data[:,4]))) + (((data[:,13]) + (data[:,13]))))))))) +\n                0.250000*np.tanh(data[:,14]) +\n                0.250000*np.tanh(((((((((((data[:,14]) + (np.tanh((((data[:,3]) * (data[:,13]))))))) + (data[:,14]))) + (data[:,0]))) + (data[:,8]))) / 2.0)) +\n                0.250000*np.tanh((((((data[:,9]) + (np.tanh(((((((data[:,19]) * (((((((data[:,1]) + (((((((data[:,21]) + (data[:,9]))/2.0)) + (data[:,6]))/2.0)))/2.0)) + (data[:,17]))/2.0)))) + (((data[:,15]) / 2.0)))/2.0)))))/2.0)) * 2.0)) +\n                0.250000*np.tanh(((data[:,1]) + (((((data[:,3]) + (data[:,15]))) + (np.tanh(((((data[:,20]) + (((data[:,1]) * 2.0)))/2.0)))))))))\n    \n    def GP_class_18(self,data):\n        return(0.250000*np.tanh((((((((((12.59485149383544922)) - ((12.59485149383544922)))) - ((12.59485149383544922)))) - ((((11.47756862640380859)) / 2.0)))) - ((12.59485149383544922)))) +\n                0.250000*np.tanh((((((-1.0*(((11.26121807098388672))))) - (((((((((11.33140659332275391)) - ((-1.0*(((9.76293182373046875))))))) - (((((11.33140659332275391)) + ((8.0)))/2.0)))) + ((9.76293182373046875)))/2.0)))) - (data[:,11]))) +\n                0.250000*np.tanh(((((((((((3.40250444412231445)) - (np.tanh(((14.86789989471435547)))))) - ((14.86789989471435547)))) + ((((((data[:,5]) + ((-1.0*(((((14.86789989471435547)) * 2.0))))))/2.0)) - ((14.86789989471435547)))))/2.0)) - (((((14.86789989471435547)) + (((((7.0)) + ((-1.0*((data[:,16])))))/2.0)))/2.0)))) +\n                0.250000*np.tanh(((data[:,18]) - ((11.84332561492919922)))) +\n                0.250000*np.tanh(((((np.tanh(((((8.96548557281494141)) * (data[:,12]))))) - ((-1.0*((((data[:,2]) * (((((data[:,4]) - ((-1.0*((data[:,9])))))) - ((9.0))))))))))) - ((8.96548557281494141)))) +\n                0.250000*np.tanh(((((((10.0)) - (data[:,12]))) + (((((((((data[:,13]) - ((12.43707370758056641)))) - ((((12.43707370758056641)) - (((data[:,12]) / 2.0)))))) - ((((12.43707370758056641)) - ((((data[:,13]) + ((12.43707370758056641)))/2.0)))))) - ((12.43707370758056641)))))/2.0)) +\n                0.250000*np.tanh(((((((np.tanh((((np.tanh((data[:,14]))) + (data[:,13]))))) * 2.0)) + (data[:,14]))) + (data[:,13]))) +\n                0.250000*np.tanh(((np.tanh((data[:,14]))) + (data[:,14]))) +\n                0.250000*np.tanh(((((data[:,9]) + (((((data[:,13]) + (data[:,4]))) + (data[:,13]))))) / 2.0)) +\n                0.250000*np.tanh(((data[:,15]) - ((((data[:,3]) + ((5.0)))/2.0)))))\n    \n    def GP_class_19(self,data):\n        return(0.250000*np.tanh((((((data[:,13]) + (data[:,13]))/2.0)) + ((((((data[:,8]) + ((((data[:,14]) + (data[:,14]))/2.0)))/2.0)) * 2.0)))) +\n                0.250000*np.tanh((((data[:,3]) + (((data[:,8]) + (np.tanh(((((((-1.0*((((((9.05535793304443359)) + ((((data[:,14]) + (((((((data[:,1]) - (((data[:,12]) * 2.0)))) * ((-1.0*((data[:,17])))))) / 2.0)))/2.0)))/2.0))))) + (data[:,2]))) * 2.0)))))))/2.0)) +\n                0.250000*np.tanh(((((((((data[:,3]) + (((((data[:,14]) + (data[:,14]))) * 2.0)))) + ((((data[:,14]) + (np.tanh((((data[:,14]) + (np.tanh((data[:,14]))))))))/2.0)))) * 2.0)) + ((((data[:,14]) + (data[:,2]))/2.0)))) +\n                0.250000*np.tanh(((((data[:,8]) + (((data[:,4]) + (((data[:,13]) + (data[:,10]))))))) + (((((data[:,13]) + (((((data[:,13]) / 2.0)) + (data[:,9]))))) + (np.tanh(((((data[:,8]) + (data[:,8]))/2.0)))))))) +\n                0.250000*np.tanh((((((data[:,19]) + (((((data[:,10]) + ((((data[:,11]) + (data[:,19]))/2.0)))) + (data[:,10]))))/2.0)) + (data[:,2]))) +\n                0.250000*np.tanh((((((data[:,0]) + (((((((((data[:,0]) + (((((data[:,0]) - (data[:,11]))) / 2.0)))/2.0)) * 2.0)) + ((((((np.tanh((data[:,13]))) + (((data[:,0]) / 2.0)))/2.0)) / 2.0)))/2.0)))/2.0)) * 2.0)) +\n                0.250000*np.tanh(((((-1.0*((data[:,15])))) + (((data[:,14]) + (data[:,14]))))/2.0)) +\n                0.250000*np.tanh(((((((((data[:,0]) + (data[:,15]))/2.0)) + (((data[:,15]) * (data[:,15]))))/2.0)) - (data[:,11]))) +\n                0.250000*np.tanh(data[:,13]) +\n                0.250000*np.tanh(((((((((data[:,18]) + ((((data[:,4]) + (data[:,18]))/2.0)))) / 2.0)) + ((((data[:,18]) + (data[:,4]))/2.0)))) * (data[:,4])))    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle.competitions import nflrush\npd.options.mode.chained_assignment = None\n\nenv = nflrush.make_env()\niter_test = env.iter_test()\n\n#df_prev = pd.DataFrame()\n#df_test = pd.DataFrame()\n\n#gp = GP()\n\nfor (test_df, sample_prediction_df) in tqdm_notebook(iter_test):\n    # Feature Engineering\n    basetable = create_features_01(test_df, True)\n    basetable = create_features_02(basetable)\n    basetable = logs(basetable, log_features)\n    basetable = squares(basetable, squared_features)\n    \n    # Remove algumas colunas\n    basetable.drop(['TimeHandoff','PlayerBirthDate','GameClock','PlayerHeight','NflId','NflIdRusher','Season'], axis=1, inplace=True)\n    \n    # Label Encoder para variaveis categoricas\n    for cat in categoricals:\n        le_dict[cat] = LabelEncoder()\n        basetable[cat] = le_dict[cat].fit_transform(basetable[cat[:-3]].apply(str))  \n        \n    # Remove as colunas categoricas originais\n    basetable.drop(['GameId','TimeSnap','Team','PossessionTeam','HomeTeamAbbr','VisitorTeamAbbr','FieldPosition','PlayDirection'], axis=1, inplace=True)\n    \n    # Considerar somente as colunas do Feature Selection\n    basetable = basetable.loc[:,best_features]\n    \n    # Normalizacao\n    scaled_basetable = scaler.transform(basetable)\n    \n    # Make predictions\n    y_pred = predict(scaled_basetable)\n    \n    #y_pred_gp = np.zeros((test_df.shape[0],199))\n    #ans = gp.GrabPredictions(scaled_basetable)\n    #y_pred_gp[:,96:96+20] = ans   \n    #y_pred = (.6*y_pred+.4*y_pred_gp)\n    \n    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1).tolist()[0]\n    preds_df = pd.DataFrame(data=[y_pred], columns=sample_prediction_df.columns)\n    \n    #df_test = df_test.append(basetable)\n    #df_prev = df_prev.append(preds_df)\n    \n    env.predict(preds_df)\n    \nenv.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}