{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A previous version of this code got a public LB of 0.01299 and is described in https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/119430\n",
    "\n",
    "Since competition end I have implemented few improvements, and in particular all piece of the transformer architecture.  The code uses keras functional model api, which makes it way ore compact than the transformer implementations one can find online.  \n",
    "\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F75976%2Fe50d024a978c324b67182a554a2b66bd%2Ftransformer.png?generation=1575305725966574&amp;alt=media)\n",
    "\n",
    "I also included some data cleaning that was shared by top teams, namely using `S = 10 * Dis`, and averaging `A` in 2017.  The CV is improved by about 0.00025, which could bring LB around 0.01275, not top, but still of interest for a model without any feature engineering besides distance matrix computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import datetime\n",
    "#from kaggle.competitions import nflrush\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import re\n",
    "import keras\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "tqdm.pandas()\n",
    "\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from kaggle.competitions import nflrush"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code can be used to decrease non determinism by setting a number of random seeds.  It also creates a new Tensorflow session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "def init_seeds(seed):\n",
    "\n",
    "    # The below is necessary for starting Numpy generated random numbers\n",
    "    # in a well-defined initial state.\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # The below is necessary for starting core Python generated random numbers\n",
    "    # in a well-defined state.\n",
    "\n",
    "    rn.seed(seed)\n",
    "\n",
    "    # The below tf.set_random_seed() will make random number generation\n",
    "    # in the TensorFlow backend have a well-defined initial state.\n",
    "    # For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "    sess = tf.Session(graph=tf.get_default_graph())\n",
    "    K.set_session(sess)\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading training data.[](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "input_folder = '../input/nfl-big-data-bowl-2020'\n",
    "train = pd.read_csv(os.path.join(input_folder, 'train.csv'), dtype={'WindSpeed': 'object'})\n",
    "train.shape\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the list of features used by the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_cols = ['X', 'Y', 'X_dir', 'Y_dir', 'X_S', 'Y_S', 'S', 'A', 'IsRusher', 'IsOnOffense']\n",
    "play_cols = ['X_rusher', 'Y_rusher', 'YardLine_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean and standardize data as in my [initial wrangling & Voronoi areas in Python](https://www.kaggle.com/cpmpml/initial-wrangling-voronoi-areas-in-python) notebook. I also include some data cleaning that was shared by top teams, namely using `S = 10 * Dis`, and averaging `A` in 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorient(train, flip_left):\n",
    "    train['ToLeft'] = train.PlayDirection == \"left\"\n",
    "    #train['IsBallCarrier'] = train.NflId == train.NflIdRusher\n",
    "    \n",
    "    train.loc[train.VisitorTeamAbbr == \"ARI\", 'VisitorTeamAbbr'] = \"ARZ\"\n",
    "    train.loc[train.HomeTeamAbbr == \"ARI\", 'HomeTeamAbbr'] = \"ARZ\"\n",
    "    \n",
    "    train.loc[train.VisitorTeamAbbr == \"BAL\", 'VisitorTeamAbbr'] = \"BLT\"\n",
    "    train.loc[train.HomeTeamAbbr == \"BAL\", 'HomeTeamAbbr'] = \"BLT\"\n",
    "    \n",
    "    train.loc[train.VisitorTeamAbbr == \"CLE\", 'VisitorTeamAbbr'] = \"CLV\"\n",
    "    train.loc[train.HomeTeamAbbr == \"CLE\", 'HomeTeamAbbr'] = \"CLV\"\n",
    "    \n",
    "    train.loc[train.VisitorTeamAbbr == \"HOU\", 'VisitorTeamAbbr'] = \"HST\"\n",
    "    train.loc[train.HomeTeamAbbr == \"HOU\", 'HomeTeamAbbr'] = \"HST\"\n",
    "    \n",
    "    train['TeamOnOffense'] = \"home\"\n",
    "    train.loc[train.PossessionTeam != train.HomeTeamAbbr, 'TeamOnOffense'] = \"away\"\n",
    "    train['IsOnOffense'] = train.Team == train.TeamOnOffense # Is player on offense?\n",
    "    train['YardLine_std'] = 100 - train.YardLine\n",
    "    train.loc[train.FieldPosition.fillna('') == train.PossessionTeam,  \n",
    "          'YardLine_std'\n",
    "         ] = train.loc[train.FieldPosition.fillna('') == train.PossessionTeam,  \n",
    "          'YardLine']\n",
    "    train.loc[train.ToLeft, 'X'] = 120 - train.loc[train.ToLeft, 'X'] \n",
    "    train.loc[train.ToLeft, 'Y'] = 160/3 - train.loc[train.ToLeft, 'Y'] \n",
    "    train.loc[train.ToLeft, 'Orientation'] = np.mod(180 + train.loc[train.ToLeft, 'Orientation'], 360)\n",
    "    train['Dir'] = 90 - train.Dir\n",
    "    train.loc[train.ToLeft, 'Dir'] = np.mod(180 + train.loc[train.ToLeft, 'Dir'], 360)\n",
    "    train.loc[train.IsOnOffense, 'Dir'] = train.loc[train.IsOnOffense, 'Dir'].fillna(0).values\n",
    "    train.loc[~train.IsOnOffense, 'Dir'] = train.loc[~train.IsOnOffense, 'Dir'].fillna(180).values\n",
    "    \n",
    "    train['S'] = 10 * train['Dis']\n",
    "    train.loc[train.Season == 2017, 'A'] = train.loc[train.Season == 2017, 'A'].mean()\n",
    "    \n",
    "    train['IsRusher'] = train['NflId'] == train['NflIdRusher']\n",
    "    if flip_left:\n",
    "        df = train[train['IsRusher']].copy()\n",
    "        #df['left'] = df.Y < 160/6\n",
    "        df['left'] = df.Dir < 0\n",
    "        train = train.merge(df[['PlayId', 'left']], how='left', on='PlayId')\n",
    "        train['Y'] = train.Y\n",
    "        train.loc[train.left, 'Y'] = 160/3 - train.loc[train.left, 'Y']\n",
    "        train['Dir'] = train.Dir\n",
    "        train.loc[train.left, 'Dir'] = np.mod( - train.loc[train.left, 'Dir'], 360)\n",
    "        train.drop('left', axis=1, inplace=True)\n",
    "        \n",
    "    \n",
    "    train['X_dir'] = np.cos( (np.pi / 180) * train .Dir)\n",
    "    train['Y_dir'] = np.sin( (np.pi / 180) * train.Dir)\n",
    "    train['X_S'] = train.X_dir * train.S\n",
    "    train['Y_S'] = train.Y_dir * train.S\n",
    "    train['X_A'] = train.X_dir * train.A\n",
    "    train['Y_A'] = train.Y_dir * train.A\n",
    "    train['PlayerHeight'] = train['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n",
    "    train['BMI'] = (train['PlayerWeight'] * 703) / (train['PlayerHeight'] ** 2)\n",
    "    train['Energy'] = train['PlayerWeight'] * (train['S'] ** 2) \n",
    "#    train.loc[train['Season'] == 2017, 'S'] = (train['S'][train['Season'] == 2017] - 2.4355) / 1.2930 * 1.4551 + 2.7570\n",
    "    train['time_step'] = 0.0\n",
    "    train = train.sort_values(by=['PlayId', 'IsOnOffense', 'IsRusher', 'Y']).reset_index(drop=True)\n",
    "    return  train\n",
    "\n",
    "def add_features(train):\n",
    "        \n",
    "    df = train[train.IsRusher][['PlayId', 'time_step', 'X', 'Y']].copy()\n",
    "    df.columns = ['PlayId', 'time_step', 'X_rusher', 'Y_rusher']\n",
    "    train = train.merge(df, how='left', on=['PlayId', 'time_step'])\n",
    "    #train.loc[~train.IsRusher, 'X'] = train.loc[~train.IsRusher, 'X'] - train.loc[~train.IsRusher, 'X_rusher']\n",
    "    #train.loc[~train.IsRusher, 'Y'] = train.loc[~train.IsRusher, 'Y'] - train.loc[~train.IsRusher, 'Y_rusher']\n",
    "    train.X -= train.X_rusher\n",
    "    train.Y -= train.Y_rusher\n",
    "    \n",
    "    #train.drop(['Orientation', 'Dir', 'TeamOnOffense', 'YardLine', 'left'], axis=1, inplace=True)\n",
    "    return  train\n",
    "\n",
    "def time_forward(train, time_step):\n",
    "    train = train.copy()\n",
    "    train['X'] = train['X'] + time_step * train['X_S'] + 0.5 * time_step**2 * train['X_A']\n",
    "    train['Y'] = train['Y'] + time_step * train['Y_S'] + 0.5 * time_step**2 * train['Y_A']\n",
    "    x_s = np.clip(train['X_S'] + time_step * train['X_A'], -14, 14)\n",
    "    y_s = np.clip(train['Y_S'] + time_step * train['Y_A'], -14, 14)\n",
    "    if 1:\n",
    "        train['X_A'] = np.clip(train['X_A'] * train['X_S'] / (x_s), -10, 10)\n",
    "        train['Y_A'] = np.clip(train['Y_A'] * train['Y_S'] / (y_s), -10, 10)\n",
    "    train['X_S'] = x_s\n",
    "    train['Y_S'] = y_s\n",
    "    train['time_step'] = time_step\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reorient = reorient(train.copy(), flip_left=True)\n",
    "len_train_reorient = len(train_reorient) // 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to improve deep learning mdoels is to augment data.  Here I augment it by computing positions of players after a given time step, then apply the same data cleaning to the new positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(train_reorient, time_steps):\n",
    "    train_reorient = pd.concat(time_forward(train_reorient, time_step) for time_step in time_steps)\n",
    "    train_reorient = add_features(train_reorient)\n",
    "    return train_reorient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little tuning suggests that using two time steps of 0.3 and 0.6 seconds are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = [0, 0.3, 0.6]\n",
    "train_reorient = augment_data(train_reorient, time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the data for first play looks like.  One thing the data cleaning code does it to sort players, so that defense team is on first 11 rows, the offense team, ending with the rusher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>X_dir</th>\n",
       "      <th>Y_dir</th>\n",
       "      <th>X_S</th>\n",
       "      <th>Y_S</th>\n",
       "      <th>S</th>\n",
       "      <th>A</th>\n",
       "      <th>IsRusher</th>\n",
       "      <th>IsOnOffense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.40</td>\n",
       "      <td>-8.30</td>\n",
       "      <td>0.188067</td>\n",
       "      <td>0.982156</td>\n",
       "      <td>0.959142</td>\n",
       "      <td>5.008997</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.43</td>\n",
       "      <td>-4.89</td>\n",
       "      <td>-0.270432</td>\n",
       "      <td>0.962739</td>\n",
       "      <td>-0.432692</td>\n",
       "      <td>1.540382</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.84</td>\n",
       "      <td>-4.31</td>\n",
       "      <td>-0.049198</td>\n",
       "      <td>0.998789</td>\n",
       "      <td>-0.196794</td>\n",
       "      <td>3.995156</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.75</td>\n",
       "      <td>-2.67</td>\n",
       "      <td>0.386389</td>\n",
       "      <td>0.922336</td>\n",
       "      <td>1.197806</td>\n",
       "      <td>2.859241</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.08</td>\n",
       "      <td>-2.11</td>\n",
       "      <td>0.320613</td>\n",
       "      <td>0.947210</td>\n",
       "      <td>0.032061</td>\n",
       "      <td>0.094721</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.997391</td>\n",
       "      <td>-0.072194</td>\n",
       "      <td>0.099739</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7.29</td>\n",
       "      <td>2.83</td>\n",
       "      <td>-0.962975</td>\n",
       "      <td>0.269592</td>\n",
       "      <td>-0.192595</td>\n",
       "      <td>0.053918</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>22.12</td>\n",
       "      <td>3.63</td>\n",
       "      <td>-0.822243</td>\n",
       "      <td>-0.569136</td>\n",
       "      <td>-2.302281</td>\n",
       "      <td>-1.593581</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.69</td>\n",
       "      <td>6.53</td>\n",
       "      <td>-0.996179</td>\n",
       "      <td>0.087330</td>\n",
       "      <td>-1.793123</td>\n",
       "      <td>0.157193</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.38</td>\n",
       "      <td>11.80</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>-0.000908</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.64</td>\n",
       "      <td>13.89</td>\n",
       "      <td>0.607514</td>\n",
       "      <td>-0.794309</td>\n",
       "      <td>0.121503</td>\n",
       "      <td>-0.158862</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-6.38</td>\n",
       "      <td>0.260336</td>\n",
       "      <td>0.965518</td>\n",
       "      <td>1.535982</td>\n",
       "      <td>5.696557</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.97</td>\n",
       "      <td>-2.68</td>\n",
       "      <td>0.586372</td>\n",
       "      <td>0.810042</td>\n",
       "      <td>1.114107</td>\n",
       "      <td>1.539079</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.32</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>0.678929</td>\n",
       "      <td>0.734204</td>\n",
       "      <td>2.172572</td>\n",
       "      <td>2.349453</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.15</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>0.998906</td>\n",
       "      <td>0.046758</td>\n",
       "      <td>1.698141</td>\n",
       "      <td>0.079488</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>1.04</td>\n",
       "      <td>-0.934453</td>\n",
       "      <td>0.356086</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.17</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.883357</td>\n",
       "      <td>0.468701</td>\n",
       "      <td>1.943385</td>\n",
       "      <td>1.031142</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.17</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.968886</td>\n",
       "      <td>0.247506</td>\n",
       "      <td>2.325327</td>\n",
       "      <td>0.594015</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.85</td>\n",
       "      <td>5.41</td>\n",
       "      <td>0.318132</td>\n",
       "      <td>0.948046</td>\n",
       "      <td>0.604451</td>\n",
       "      <td>1.801288</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.05</td>\n",
       "      <td>11.34</td>\n",
       "      <td>0.988964</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>5.043717</td>\n",
       "      <td>-0.755589</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.93</td>\n",
       "      <td>12.97</td>\n",
       "      <td>0.969659</td>\n",
       "      <td>-0.244461</td>\n",
       "      <td>5.333125</td>\n",
       "      <td>-1.344537</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.911690</td>\n",
       "      <td>0.410878</td>\n",
       "      <td>3.464423</td>\n",
       "      <td>1.561336</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.589579</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        X      Y     X_dir     Y_dir       X_S       Y_S    S         A  \\\n",
       "0    5.40  -8.30  0.188067  0.982156  0.959142  5.008997  5.1  1.589579   \n",
       "1    9.43  -4.89 -0.270432  0.962739 -0.432692  1.540382  1.6  1.589579   \n",
       "2    4.84  -4.31 -0.049198  0.998789 -0.196794  3.995156  4.0  1.589579   \n",
       "3    4.75  -2.67  0.386389  0.922336  1.197806  2.859241  3.1  1.589579   \n",
       "4    4.08  -2.11  0.320613  0.947210  0.032061  0.094721  0.1  1.589579   \n",
       "5    4.60   1.63  0.997391 -0.072194  0.099739 -0.007219  0.1  1.589579   \n",
       "6    7.29   2.83 -0.962975  0.269592 -0.192595  0.053918  0.2  1.589579   \n",
       "7   22.12   3.63 -0.822243 -0.569136 -2.302281 -1.593581  2.8  1.589579   \n",
       "8    3.69   6.53 -0.996179  0.087330 -1.793123  0.157193  1.8  1.589579   \n",
       "9    5.38  11.80  1.000000 -0.000698  1.300000 -0.000908  1.3  1.589579   \n",
       "10   4.64  13.89  0.607514 -0.794309  0.121503 -0.158862  0.2  1.589579   \n",
       "11   2.28  -6.38  0.260336  0.965518  1.535982  5.696557  5.9  1.589579   \n",
       "12   3.97  -2.68  0.586372  0.810042  1.114107  1.539079  1.9  1.589579   \n",
       "13   3.32  -1.88  0.678929  0.734204  2.172572  2.349453  3.2  1.589579   \n",
       "14   4.15  -1.35  0.998906  0.046758  1.698141  0.079488  1.7  1.589579   \n",
       "15  -1.01   1.04 -0.934453  0.356086 -0.000000  0.000000  0.0  1.589579   \n",
       "16   4.17   1.41  0.883357  0.468701  1.943385  1.031142  2.2  1.589579   \n",
       "17   4.17   2.06  0.968886  0.247506  2.325327  0.594015  2.4  1.589579   \n",
       "18   2.85   5.41  0.318132  0.948046  0.604451  1.801288  1.9  1.589579   \n",
       "19   4.05  11.34  0.988964 -0.148155  5.043717 -0.755589  5.1  1.589579   \n",
       "20   2.93  12.97  0.969659 -0.244461  5.333125 -1.344537  5.5  1.589579   \n",
       "21   0.00   0.00  0.911690  0.410878  3.464423  1.561336  3.8  1.589579   \n",
       "\n",
       "    IsRusher  IsOnOffense  \n",
       "0      False        False  \n",
       "1      False        False  \n",
       "2      False        False  \n",
       "3      False        False  \n",
       "4      False        False  \n",
       "5      False        False  \n",
       "6      False        False  \n",
       "7      False        False  \n",
       "8      False        False  \n",
       "9      False        False  \n",
       "10     False        False  \n",
       "11     False         True  \n",
       "12     False         True  \n",
       "13     False         True  \n",
       "14     False         True  \n",
       "15     False         True  \n",
       "16     False         True  \n",
       "17     False         True  \n",
       "18     False         True  \n",
       "19     False         True  \n",
       "20     False         True  \n",
       "21      True         True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_reorient[player_cols]\n",
    "df.head(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a cross validation such that 2017 data is not used for validation.  In order to do it we keep track of where 2017 ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11900"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_2017 = train[train.Season == 2017].shape[0] // 22\n",
    "len_2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute our training data.  We have three inputs to the model: players data, play data, and distance matrix.  Each data input is scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69513, 22, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_players = StandardScaler()\n",
    "X_players = df.values.astype('float32') \n",
    "X_players = ss_players.fit_transform(df)\n",
    "X_players = X_players.reshape((-1, 22, len(player_cols)))\n",
    "X_players.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_play = train_reorient[play_cols].values[::22]\n",
    "ss_play = StandardScaler()\n",
    "X_play = ss_play.fit_transform(X_play)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last input is the distance matrix for each play.  In order to compute it efficiently we use the numba compiler. This is way faster than using predefined dist function or numpy operations. More deatails can be found in my [Ultra Fast Distance Matrix Computation](https://www.kaggle.com/cpmpml/ultra-fast-distance-matrix-computation) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_dmat(X, Y):\n",
    "    dmat = np.zeros((22, 22))\n",
    "    for i in range(22):\n",
    "        for j in range(i+1, 22):\n",
    "            d = np.sqrt((X[i] - X[j])**2 + (Y[i] - Y[j])**2)\n",
    "            dmat[i, j] = d\n",
    "            dmat[j, i] = d\n",
    "    dmat = dmat.reshape((1, 22, 22))\n",
    "    return dmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69513, 22, 22)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y = train_reorient.X.values, train_reorient.Y.values\n",
    "dmats = [get_dmat(X[i:i+22], Y[i:i+22]) for i in range(0, train_reorient.shape[0], 22)]\n",
    "dmats = np.vstack(dmats).reshape((-1, 22, 22))\n",
    "dmats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use distance as an input on how each player influences other players.  Given the influence decreases with distance, we tried various decreasing functions of distance, starting with its inverse.  The squared inverse seemed to work better, which is why we're using it now.  Data is also sclaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_dmats = 1. / (1e-2 + dmats)**2\n",
    "inv_dmats /= inv_dmats.sum(axis=2, keepdims=True)\n",
    "_ = plt.hist(inv_dmats.ravel(), bins=100, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete the raw train data so that we don't use it by mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer relies on layer normalization rather than batch normalization.  I borrowed an implemententation from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "class LayerNormalization(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 epsilon=None,\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_constraint=None,\n",
    "                 beta_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"Layer normalization layer\n",
    "        See: [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)\n",
    "        :param center: Add an offset parameter if it is True.\n",
    "        :param scale: Add a scale parameter if it is True.\n",
    "        :param epsilon: Epsilon for calculating variance.\n",
    "        :param gamma_initializer: Initializer for the gamma weight.\n",
    "        :param beta_initializer: Initializer for the beta weight.\n",
    "        :param gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        :param beta_regularizer: Optional regularizer for the beta weight.\n",
    "        :param gamma_constraint: Optional constraint for the gamma weight.\n",
    "        :param beta_constraint: Optional constraint for the beta weight.\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon() * K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma_initializer = keras.initializers.get(gamma_initializer)\n",
    "        self.beta_initializer = keras.initializers.get(beta_initializer)\n",
    "        self.gamma_regularizer = keras.regularizers.get(gamma_regularizer)\n",
    "        self.beta_regularizer = keras.regularizers.get(beta_regularizer)\n",
    "        self.gamma_constraint = keras.constraints.get(gamma_constraint)\n",
    "        self.beta_constraint = keras.constraints.get(beta_constraint)\n",
    "        self.gamma, self.beta = None, None\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'center': self.center,\n",
    "            'scale': self.scale,\n",
    "            'epsilon': self.epsilon,\n",
    "            'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),\n",
    "            'beta_initializer': keras.initializers.serialize(self.beta_initializer),\n",
    "            'gamma_regularizer': keras.regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_regularizer': keras.regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_constraint': keras.constraints.serialize(self.gamma_constraint),\n",
    "            'beta_constraint': keras.constraints.serialize(self.beta_constraint),\n",
    "        }\n",
    "        base_config = super(LayerNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def compute_mask(self, inputs, input_mask=None):\n",
    "        return input_mask\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        shape = input_shape[-1:]\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(\n",
    "                shape=shape,\n",
    "                initializer=self.gamma_initializer,\n",
    "                regularizer=self.gamma_regularizer,\n",
    "                constraint=self.gamma_constraint,\n",
    "                name='gamma',\n",
    "            )\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(\n",
    "                shape=shape,\n",
    "                initializer=self.beta_initializer,\n",
    "                regularizer=self.beta_regularizer,\n",
    "                constraint=self.beta_constraint,\n",
    "                name='beta',\n",
    "            )\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        mean = K.mean(inputs, axis=-1, keepdims=True)\n",
    "        variance = K.mean(K.square(inputs - mean), axis=-1, keepdims=True)\n",
    "        std = K.sqrt(variance + self.epsilon)\n",
    "        outputs = (inputs - mean) / std\n",
    "        if self.scale:\n",
    "            outputs *= self.gamma\n",
    "        if self.center:\n",
    "            outputs += self.beta\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the model, let's define the target and the metric.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mae = train_reorient['Yards'].values[::22]\n",
    "y_mean = np.median(y_mae)\n",
    "y_crps = np.zeros(shape=(y_mae.shape[0], 199))\n",
    "for i,yard in enumerate(y_mae):\n",
    "    y_crps[i, yard+99:] = 1\n",
    "    \n",
    "yardline = train_reorient['YardLine_std'].values[::22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crps(y_true, y_pred):\n",
    "    stops = np.arange(-99, 100)\n",
    "    unit_steps = stops >= y_true.reshape(-1, 1)\n",
    "    crps = np.mean((y_pred - unit_steps)**2)\n",
    "    return crps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of our model must be non decreasing.  One way to achieve this is to run an [isotonic regression](https://scikit-learn.org/stable/auto_examples/plot_isotonic_regression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nondecreasing(x):\n",
    "    X_ir = np.arange(199).astype('float64')\n",
    "    ir = IsotonicRegression(0, 1)\n",
    "    x = ir.fit_transform(X_ir, x.astype('float64'))        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement our model.  Its high level architecture is inspired by the transformer architecture:\n",
    "\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F75976%2Fe50d024a978c324b67182a554a2b66bd%2Ftransformer.png?generation=1575305725966574&amp;alt=media)\n",
    "\n",
    "We start wiht embeddings, then a distance attention bloc, then a transformer encoder bloc with multi head self attention followed by a feed forward bloc. Then we plug the output into an encoder/decoder attention on the play embeddings.  Its output is fed to two output layers that output probabilities.  More can be found in my [competition writeup](https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/119430)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Add, Multiply, Dot\n",
    "from keras.layers import Embedding, Permute, Reshape\n",
    "from keras.layers.core import Dropout, Lambda, Dense, Flatten\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a custom layer that takes 4 numbers as input and outputs a logistic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(ScaleLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ScaleLayer, self).build(input_shape) \n",
    "\n",
    "    def call(self, x):\n",
    "        xx = K.arange(-99, 100, dtype=tf.float32)\n",
    "        mu = y_mean + tf.reshape(x[:, 0], (-1, 1))\n",
    "        sigma_minus = tf.identity(K.exp(0.5 * tf.reshape(x[:, 1], (-1, 1))), name=\"sigma\")\n",
    "        sigma_plus = tf.identity(K.exp(0.5 * tf.reshape(x[:, 2], (-1, 1))), name=\"sigma\")\n",
    "        xx = tf.subtract(xx, mu)\n",
    "        pcf = tf.where(xx >= 0, tf.divide (xx, sigma_plus),  tf.divide (xx, sigma_minus))\n",
    "        return pcf\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece is a distance attention. To update a given player embedding I use a weighted sum of the other players embeddings. The weight depends on the distance. I tried various ways, and a normalized squared inverse was best. I was about to try other transforms when I decided to have them learnt by the model, via a 1x1 convolution bloc on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_mult(dist, players):\n",
    "    res = Lambda(lambda c: K.batch_dot(c[0], c[1]))([dist, players])\n",
    "    return res\n",
    "\n",
    "def dist_attention(dist, players, dropout):\n",
    "    if 1:\n",
    "        dist1 = Reshape((22, 22, 1))(dist)\n",
    "        dist1 = Conv2D(16, 1, activation='relu', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(dist1)\n",
    "        dist1 = Conv2D(1, 1, activation='relu', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(dist1)\n",
    "        dist1 = Reshape((22, 22))(dist1)\n",
    "        dist = Add()([dist, dist1])\n",
    "    dist = LayerNormalization()(dist)\n",
    "    att = dist_mult(dist, players, )\n",
    "    x_player = Add()([players, att])\n",
    "    x_player = LayerNormalization()(x_player)\n",
    "    if dropout > 0:\n",
    "        x_player = Dropout(dropout)(x_player)\n",
    "    return x_player\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bloc is multi head attention.  I recommend these two tutorials to understand the logic behind the code:\n",
    "\n",
    "http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "https://nlp.seas.harvard.edu/2018/04/03/attention.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two variants of multi head attention. The first one is self attention, used in the encoder part of the transformer. Here we use it on an array of player features.  The second variant is used in the decoder transformer. Here we use it with the play embeddings and the output of the encoder part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(x_inner, x_outer, n_factor, dropout):\n",
    "    x_Q =  Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_inner)\n",
    "    x_K =  Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_outer)\n",
    "    x_V =  Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_outer)\n",
    "    x_KT = Permute((2, 1))(x_K)\n",
    "    res = Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n",
    "    att = Lambda(lambda c: K.softmax(c, axis=-1))(res)\n",
    "    att = Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n",
    "    return att\n",
    "\n",
    "def multi_head_self_attention(x, n_factor, n_head, dropout):\n",
    "    if n_head == 1:\n",
    "        att = attention(x, x, n_factor, dropout)\n",
    "    else:\n",
    "        n_factor_head = n_factor // n_head\n",
    "        heads = [attention(x, x, n_factor_head, dropout) for i in range(n_head)]\n",
    "        att = Concatenate()(heads)\n",
    "        att = Dense(n_factor, \n",
    "                      kernel_initializer='glorot_uniform',\n",
    "                      bias_initializer='glorot_uniform',\n",
    "                     )(att)\n",
    "    x = Add()([x, att])\n",
    "    x = LayerNormalization()(x)\n",
    "    if dropout > 0:\n",
    "        x = Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def multi_head_outer_attention(x_inner, x_outer, n_factor, n_head, dropout):\n",
    "    if n_head == 1:\n",
    "        att = attention(x_inner, x_outer, n_factor, dropout)\n",
    "    else:\n",
    "        n_factor_head = n_factor // n_head\n",
    "        heads = [attention(x_inner, x_outer, n_factor_head, dropout) for i in range(n_head)]\n",
    "        att = Concatenate()(heads)\n",
    "        att = Dense(n_factor, \n",
    "                      kernel_initializer='glorot_uniform',\n",
    "                      bias_initializer='glorot_uniform',\n",
    "                     )(att)\n",
    "    x_inner = Add()([x_inner, att])\n",
    "    x_inner = LayerNormalization()(x_inner)\n",
    "    if dropout > 0:\n",
    "        x = Dropout(dropout)(x_inner)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bloc is a position based feed forward network.  As noted in the original transformer paper, this is equivalent to a 1D convolution bloc, which is what we use here.  I added squeeze ande excitation, it improves performance a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_bloc(in_bloc, ch, ratio):\n",
    "    x = GlobalAveragePooling1D()(in_bloc)\n",
    "    x = Dense(ch//ratio, activation='relu')(x)\n",
    "    x = Dense(ch, activation='sigmoid')(x)\n",
    "    x = Multiply()([in_bloc, x])\n",
    "    return Add()([x, in_bloc])\n",
    "\n",
    "def conv_bloc(players, n_factor, n_hidden, se_ratio, dropout):\n",
    "    players0 = players\n",
    "    players = Conv1D(n_hidden, 1, activation='relu', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(players)\n",
    "    players = Conv1D(n_factor, 1, activation='relu', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(players)\n",
    "    players = Add()([players0, players])\n",
    "    players = se_bloc(players, n_factor, se_ratio)\n",
    "    players = LayerNormalization()(players)\n",
    "    if dropout > 0:\n",
    "        players = Dropout(dropout)(players)\n",
    "    return players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our model with these blocs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_player, n_factor, n_loop, n_head, n_hidden, se_ratio, dropout, n_player_cols, n_play_cols):\n",
    "    input_players = Input((n_player, n_player_cols), name=\"players\")\n",
    "    input_dmats = Input((n_player, n_player), name=\"inv_dist\")\n",
    "    input_play = Input((n_play_cols,), name=\"plays\")\n",
    "\n",
    "    x_player = input_players\n",
    "    x_player = Conv1D(n_factor, 1)(x_player)    \n",
    "    x_player = LayerNormalization()(x_player)\n",
    "\n",
    "    for l in range(n_loop):\n",
    "        x_player = dist_attention(input_dmats, x_player, dropout)\n",
    "        x_player = conv_bloc(x_player, n_factor, n_hidden, se_ratio, dropout)\n",
    "\n",
    "        x_player = multi_head_self_attention(x_player, n_factor, n_head, dropout)\n",
    "        x_player = conv_bloc(x_player, n_factor, n_hidden, se_ratio, dropout)\n",
    "\n",
    "    x_play = Dense(n_factor)(input_play)\n",
    "    x_play = Reshape((1, -1))(x_play)\n",
    "    \n",
    "    readout = multi_head_outer_attention(x_play, x_player, n_factor, n_head, dropout)\n",
    "    readout = Flatten()(readout)\n",
    "\n",
    "    out1 = Dense(199, activation='sigmoid')(readout)\n",
    "    readout = Dense(4)(readout)\n",
    "    readout = ScaleLayer(output_dim=199)(readout)\n",
    "    out2 = keras.layers.Activation('sigmoid')(readout)\n",
    "    return Model(inputs=[input_players, input_dmats, input_play], outputs=[out1, out2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inv_dist (InputLayer)           (None, 22, 22)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 22, 22, 1)    0           inv_dist[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 22, 22, 16)   32          reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 22, 22, 1)    17          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "players (InputLayer)            (None, 22, 10)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 22, 22)       0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 22, 64)       704         players[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 22, 22)       0           inv_dist[0][0]                   \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 22, 64)       128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 22, 22)       44          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 22, 64)       0           layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 22, 64)       0           layer_normalization_1[0][0]      \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 22, 64)       128         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 22, 64)       0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 22, 128)      8320        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 22, 64)       8256        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 22, 64)       0           dropout_1[0][0]                  \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1040        global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           1088        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 22, 64)       0           add_3[0][0]                      \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 22, 64)       0           multiply_1[0][0]                 \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 22, 64)       128         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 22, 64)       0           layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 22, 16)       1040        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 22, 16)       1040        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 22, 16)       1040        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 22, 16)       1040        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 22, 16)       1040        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 16, 22)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 22, 16)       1040        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 16, 22)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 22, 16)       1040        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 16, 22)       0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 22, 16)       1040        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 16, 22)       0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 22, 22)       0           conv1d_4[0][0]                   \n",
      "                                                                 permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 22, 22)       0           conv1d_7[0][0]                   \n",
      "                                                                 permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 22, 22)       0           conv1d_10[0][0]                  \n",
      "                                                                 permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 22, 22)       0           conv1d_13[0][0]                  \n",
      "                                                                 permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 22, 22)       0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 22, 16)       1040        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 22, 22)       0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 22, 16)       1040        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 22, 22)       0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 22, 16)       1040        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 22, 22)       0           lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 22, 16)       1040        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 22, 16)       0           lambda_3[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 22, 16)       0           lambda_6[0][0]                   \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 22, 16)       0           lambda_9[0][0]                   \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 22, 16)       0           lambda_12[0][0]                  \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 22, 64)       0           lambda_4[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 22, 64)       4160        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 22, 64)       0           dropout_2[0][0]                  \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 22, 64)       128         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 22, 64)       0           layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 22, 128)      8320        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 22, 64)       8256        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 22, 64)       0           dropout_3[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           1040        global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           1088        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 22, 64)       0           add_6[0][0]                      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 22, 64)       0           multiply_2[0][0]                 \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "plays (InputLayer)              (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 22, 64)       128         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 64)           256         plays[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 22, 64)       0           layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 64)        0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 22, 16)       1040        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 22, 16)       1040        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 22, 16)       1040        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 22, 16)       1040        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1, 16)        1040        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_5 (Permute)             (None, 16, 22)       0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1, 16)        1040        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_6 (Permute)             (None, 16, 22)       0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1, 16)        1040        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_7 (Permute)             (None, 16, 22)       0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1, 16)        1040        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_8 (Permute)             (None, 16, 22)       0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 1, 22)        0           conv1d_18[0][0]                  \n",
      "                                                                 permute_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 1, 22)        0           conv1d_21[0][0]                  \n",
      "                                                                 permute_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 1, 22)        0           conv1d_24[0][0]                  \n",
      "                                                                 permute_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 1, 22)        0           conv1d_27[0][0]                  \n",
      "                                                                 permute_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 1, 22)        0           lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 22, 16)       1040        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 1, 22)        0           lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 22, 16)       1040        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 1, 22)        0           lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 22, 16)       1040        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 1, 22)        0           lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 22, 16)       1040        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 1, 16)        0           lambda_15[0][0]                  \n",
      "                                                                 conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 1, 16)        0           lambda_18[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 1, 16)        0           lambda_21[0][0]                  \n",
      "                                                                 conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 1, 16)        0           lambda_24[0][0]                  \n",
      "                                                                 conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 64)        0           lambda_16[0][0]                  \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1, 64)        4160        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 1, 64)        0           reshape_3[0][0]                  \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 1, 64)        128         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1, 64)        0           layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 4)            260         flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "scale_layer_1 (ScaleLayer)      (None, 199)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 199)          12935       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 199)          0           scale_layer_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 85,704\n",
      "Trainable params: 85,704\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_player = 22\n",
    "n_factor = 64\n",
    "se_ratio = 4\n",
    "n_loop = 1\n",
    "n_head = 4\n",
    "n_hidden = 2*n_factor\n",
    "dropout = 0.25\n",
    "n_player_cols = len(player_cols)\n",
    "n_play_cols = len(play_cols)\n",
    "model = get_model(n_player, n_factor, n_loop, n_head, n_hidden, se_ratio, dropout, n_player_cols, n_play_cols)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At some point during our development we simulated the public test leaderboard by using that last 10% of the data for test, and the prevoous 90% for cross validation.  That code my not be up to date now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_test = False\n",
    "\n",
    "if simulate_test:\n",
    "    len_data = X_players.shape[0]\n",
    "    len_train = int(0.9 * len_data)\n",
    "else:\n",
    "    len_data = X_players.shape[0]\n",
    "    len_train = len_data\n",
    "\n",
    "X_players_train, X_players_test = X_players[:len_train], X_players[len_train:]\n",
    "inv_dmats_train, inv_dmats_test = inv_dmats[:len_train], inv_dmats[len_train:]\n",
    "X_play_train, X_play_test = X_play[:len_train], X_play[len_train:]\n",
    "y_mae_train, y_mae_test = y_mae[:len_train], y_mae[len_train:]\n",
    "y_crps_train, y_crps_test = y_crps[:len_train], y_crps[len_train:]\n",
    "yardline_train, yardline_test = yardline[:len_train], yardline[len_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our folds.  As said above, we used 2017 only for training folds, and build validation folds out of the remaining data.  We also downweight 2017 samples by 0.5.  The folds take data augmentation into account: plays derived from an original train play are put in the same fold as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=12, shuffle=False)\n",
    "ind_2017 = np.arange(len_2017)\n",
    "def add_2017(x,y):\n",
    "    x = np.concatenate((ind_2017, x +  len_2017))\n",
    "    y = y + len_2017\n",
    "    return x,y\n",
    "\n",
    "indices = [add_2017(x, y) for x, y in kf.split(X_players_train[len_2017:len_train_reorient], \n",
    "                                               y_mae_train[len_2017:len_train_reorient])]\n",
    "\n",
    "n_steps = len(time_steps)\n",
    "\n",
    "def augment_indices(x, y, time_steps):\n",
    "    x_a = np.concatenate([x + i*len_train_reorient for i in range(n_steps)])\n",
    "    y_a = np.concatenate([y + i*len_train_reorient for i in range(n_steps)])\n",
    "    return x_a, y_a\n",
    "\n",
    "indices = [augment_indices(x, y, time_steps) for x,y in indices ]\n",
    "indices = [(i, ind) for i,ind in enumerate(indices)]\n",
    "\n",
    "w = np.ones(len_train_reorient)\n",
    "w[:len_2017] = 0.5\n",
    "w = np.concatenate([w for i in range(n_steps)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our non decreasing code.  We also fix to 0 or 1 values that are always known. Last, given we use TTA, i.e. we augment data at prediction time, we take the average of predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(y_pred, yardline_train, time_steps):\n",
    "    upper = 100 - yardline_train\n",
    "    lower = - yardline_train\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        y_pred[i, 99 + upper[i]:] = 1\n",
    "        y_pred[i, :99 + lower[i]] = 0\n",
    "        y_pred[i] = nondecreasing(y_pred[i].ravel())\n",
    "    if 1:\n",
    "        n_steps = len(time_steps)\n",
    "        len_y_pred = len(y_pred) // n_steps\n",
    "        y_pred = np.mean([y_pred[len_y_pred * i: len_y_pred * (i+1)] for i in range(n_steps)], axis=0) \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time limit we only run two folds here.  During mdodel development we used 12 folds, which proved to be quite in line with LB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66693 samples, validate on 2820 samples\n",
      "Epoch 1/500\n",
      " - 19s - loss: 0.0336 - dense_17_loss: 0.0227 - activation_2_loss: 0.0109 - val_loss: 0.0274 - val_dense_17_loss: 0.0133 - val_activation_2_loss: 0.0130\n",
      "Epoch 2/500\n",
      " - 13s - loss: 0.0213 - dense_17_loss: 0.0107 - activation_2_loss: 0.0105 - val_loss: 0.0266 - val_dense_17_loss: 0.0128 - val_activation_2_loss: 0.0127\n",
      "Epoch 3/500\n",
      " - 14s - loss: 0.0206 - dense_17_loss: 0.0104 - activation_2_loss: 0.0103 - val_loss: 0.0264 - val_dense_17_loss: 0.0126 - val_activation_2_loss: 0.0126\n",
      "Epoch 4/500\n",
      " - 12s - loss: 0.0203 - dense_17_loss: 0.0102 - activation_2_loss: 0.0102 - val_loss: 0.0261 - val_dense_17_loss: 0.0126 - val_activation_2_loss: 0.0126\n",
      "Epoch 5/500\n",
      " - 13s - loss: 0.0201 - dense_17_loss: 0.0101 - activation_2_loss: 0.0101 - val_loss: 0.0258 - val_dense_17_loss: 0.0124 - val_activation_2_loss: 0.0123\n",
      "Epoch 6/500\n",
      " - 13s - loss: 0.0200 - dense_17_loss: 0.0100 - activation_2_loss: 0.0100 - val_loss: 0.0256 - val_dense_17_loss: 0.0122 - val_activation_2_loss: 0.0122\n",
      "Epoch 7/500\n",
      " - 12s - loss: 0.0198 - dense_17_loss: 0.0099 - activation_2_loss: 0.0099 - val_loss: 0.0254 - val_dense_17_loss: 0.0122 - val_activation_2_loss: 0.0122\n",
      "Epoch 8/500\n",
      " - 13s - loss: 0.0197 - dense_17_loss: 0.0098 - activation_2_loss: 0.0098 - val_loss: 0.0258 - val_dense_17_loss: 0.0125 - val_activation_2_loss: 0.0124\n",
      "Epoch 9/500\n",
      " - 13s - loss: 0.0196 - dense_17_loss: 0.0098 - activation_2_loss: 0.0098 - val_loss: 0.0254 - val_dense_17_loss: 0.0122 - val_activation_2_loss: 0.0121\n",
      "Epoch 10/500\n",
      " - 12s - loss: 0.0195 - dense_17_loss: 0.0098 - activation_2_loss: 0.0098 - val_loss: 0.0251 - val_dense_17_loss: 0.0120 - val_activation_2_loss: 0.0120\n",
      "Epoch 11/500\n",
      " - 13s - loss: 0.0195 - dense_17_loss: 0.0097 - activation_2_loss: 0.0098 - val_loss: 0.0252 - val_dense_17_loss: 0.0121 - val_activation_2_loss: 0.0121\n",
      "Epoch 12/500\n",
      " - 13s - loss: 0.0194 - dense_17_loss: 0.0097 - activation_2_loss: 0.0097 - val_loss: 0.0253 - val_dense_17_loss: 0.0121 - val_activation_2_loss: 0.0121\n",
      "Epoch 13/500\n",
      " - 13s - loss: 0.0194 - dense_17_loss: 0.0097 - activation_2_loss: 0.0097 - val_loss: 0.0249 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 14/500\n",
      " - 13s - loss: 0.0193 - dense_17_loss: 0.0096 - activation_2_loss: 0.0097 - val_loss: 0.0252 - val_dense_17_loss: 0.0120 - val_activation_2_loss: 0.0120\n",
      "Epoch 15/500\n",
      " - 13s - loss: 0.0193 - dense_17_loss: 0.0096 - activation_2_loss: 0.0097 - val_loss: 0.0249 - val_dense_17_loss: 0.0120 - val_activation_2_loss: 0.0119\n",
      "Epoch 16/500\n",
      " - 13s - loss: 0.0192 - dense_17_loss: 0.0096 - activation_2_loss: 0.0096 - val_loss: 0.0251 - val_dense_17_loss: 0.0120 - val_activation_2_loss: 0.0120\n",
      "Epoch 17/500\n",
      " - 13s - loss: 0.0192 - dense_17_loss: 0.0096 - activation_2_loss: 0.0096 - val_loss: 0.0249 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 18/500\n",
      " - 12s - loss: 0.0192 - dense_17_loss: 0.0096 - activation_2_loss: 0.0096 - val_loss: 0.0249 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 19/500\n",
      " - 12s - loss: 0.0191 - dense_17_loss: 0.0095 - activation_2_loss: 0.0096 - val_loss: 0.0248 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 20/500\n",
      " - 13s - loss: 0.0192 - dense_17_loss: 0.0096 - activation_2_loss: 0.0096 - val_loss: 0.0249 - val_dense_17_loss: 0.0120 - val_activation_2_loss: 0.0120\n",
      "Epoch 21/500\n",
      " - 13s - loss: 0.0191 - dense_17_loss: 0.0095 - activation_2_loss: 0.0096 - val_loss: 0.0249 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 22/500\n",
      " - 12s - loss: 0.0191 - dense_17_loss: 0.0095 - activation_2_loss: 0.0095 - val_loss: 0.0247 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 23/500\n",
      " - 13s - loss: 0.0190 - dense_17_loss: 0.0095 - activation_2_loss: 0.0095 - val_loss: 0.0249 - val_dense_17_loss: 0.0120 - val_activation_2_loss: 0.0120\n",
      "Epoch 24/500\n",
      " - 13s - loss: 0.0190 - dense_17_loss: 0.0095 - activation_2_loss: 0.0095 - val_loss: 0.0250 - val_dense_17_loss: 0.0120 - val_activation_2_loss: 0.0120\n",
      "Epoch 25/500\n",
      " - 12s - loss: 0.0189 - dense_17_loss: 0.0095 - activation_2_loss: 0.0095 - val_loss: 0.0250 - val_dense_17_loss: 0.0120 - val_activation_2_loss: 0.0120\n",
      "Epoch 26/500\n",
      " - 14s - loss: 0.0189 - dense_17_loss: 0.0095 - activation_2_loss: 0.0095 - val_loss: 0.0248 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 27/500\n",
      " - 13s - loss: 0.0189 - dense_17_loss: 0.0094 - activation_2_loss: 0.0095 - val_loss: 0.0249 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 28/500\n",
      " - 13s - loss: 0.0189 - dense_17_loss: 0.0094 - activation_2_loss: 0.0095 - val_loss: 0.0246 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 29/500\n",
      " - 13s - loss: 0.0189 - dense_17_loss: 0.0094 - activation_2_loss: 0.0094 - val_loss: 0.0249 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 30/500\n",
      " - 12s - loss: 0.0188 - dense_17_loss: 0.0094 - activation_2_loss: 0.0094 - val_loss: 0.0247 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 31/500\n",
      " - 12s - loss: 0.0188 - dense_17_loss: 0.0094 - activation_2_loss: 0.0094 - val_loss: 0.0248 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 32/500\n",
      " - 13s - loss: 0.0188 - dense_17_loss: 0.0094 - activation_2_loss: 0.0094 - val_loss: 0.0247 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 33/500\n",
      " - 13s - loss: 0.0188 - dense_17_loss: 0.0094 - activation_2_loss: 0.0094 - val_loss: 0.0245 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0117\n",
      "Epoch 34/500\n",
      " - 13s - loss: 0.0187 - dense_17_loss: 0.0094 - activation_2_loss: 0.0094 - val_loss: 0.0248 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 35/500\n",
      " - 13s - loss: 0.0187 - dense_17_loss: 0.0093 - activation_2_loss: 0.0094 - val_loss: 0.0246 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 36/500\n",
      " - 13s - loss: 0.0187 - dense_17_loss: 0.0093 - activation_2_loss: 0.0094 - val_loss: 0.0248 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 37/500\n",
      " - 13s - loss: 0.0187 - dense_17_loss: 0.0093 - activation_2_loss: 0.0094 - val_loss: 0.0245 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0117\n",
      "Epoch 38/500\n",
      " - 13s - loss: 0.0187 - dense_17_loss: 0.0093 - activation_2_loss: 0.0094 - val_loss: 0.0247 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 39/500\n",
      " - 13s - loss: 0.0186 - dense_17_loss: 0.0093 - activation_2_loss: 0.0093 - val_loss: 0.0247 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0119\n",
      "Epoch 40/500\n",
      " - 12s - loss: 0.0186 - dense_17_loss: 0.0093 - activation_2_loss: 0.0093 - val_loss: 0.0245 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0117\n",
      "Epoch 41/500\n",
      " - 13s - loss: 0.0186 - dense_17_loss: 0.0093 - activation_2_loss: 0.0093 - val_loss: 0.0246 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 42/500\n",
      " - 13s - loss: 0.0186 - dense_17_loss: 0.0093 - activation_2_loss: 0.0093 - val_loss: 0.0246 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0118\n",
      "Epoch 43/500\n",
      " - 12s - loss: 0.0186 - dense_17_loss: 0.0093 - activation_2_loss: 0.0093 - val_loss: 0.0244 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0117\n",
      "Epoch 44/500\n",
      " - 13s - loss: 0.0185 - dense_17_loss: 0.0093 - activation_2_loss: 0.0093 - val_loss: 0.0246 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 45/500\n",
      " - 13s - loss: 0.0185 - dense_17_loss: 0.0092 - activation_2_loss: 0.0093 - val_loss: 0.0246 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 46/500\n",
      " - 12s - loss: 0.0185 - dense_17_loss: 0.0092 - activation_2_loss: 0.0093 - val_loss: 0.0248 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0118\n",
      "Epoch 47/500\n",
      " - 12s - loss: 0.0185 - dense_17_loss: 0.0092 - activation_2_loss: 0.0093 - val_loss: 0.0245 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0117\n",
      "Epoch 48/500\n",
      " - 12s - loss: 0.0184 - dense_17_loss: 0.0092 - activation_2_loss: 0.0092 - val_loss: 0.0249 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0119\n",
      "Epoch 49/500\n",
      " - 13s - loss: 0.0184 - dense_17_loss: 0.0092 - activation_2_loss: 0.0092 - val_loss: 0.0245 - val_dense_17_loss: 0.0116 - val_activation_2_loss: 0.0117\n",
      "Epoch 50/500\n",
      " - 14s - loss: 0.0184 - dense_17_loss: 0.0092 - activation_2_loss: 0.0092 - val_loss: 0.0244 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0117\n",
      "Epoch 51/500\n",
      " - 12s - loss: 0.0184 - dense_17_loss: 0.0092 - activation_2_loss: 0.0092 - val_loss: 0.0246 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 52/500\n",
      " - 12s - loss: 0.0184 - dense_17_loss: 0.0092 - activation_2_loss: 0.0092 - val_loss: 0.0244 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0117\n",
      "Epoch 53/500\n",
      " - 13s - loss: 0.0184 - dense_17_loss: 0.0092 - activation_2_loss: 0.0092 - val_loss: 0.0246 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 54/500\n",
      " - 13s - loss: 0.0184 - dense_17_loss: 0.0092 - activation_2_loss: 0.0092 - val_loss: 0.0251 - val_dense_17_loss: 0.0121 - val_activation_2_loss: 0.0120\n",
      "Epoch 55/500\n",
      " - 12s - loss: 0.0183 - dense_17_loss: 0.0091 - activation_2_loss: 0.0092 - val_loss: 0.0242 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0117\n",
      "Epoch 56/500\n",
      " - 13s - loss: 0.0183 - dense_17_loss: 0.0091 - activation_2_loss: 0.0092 - val_loss: 0.0246 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 57/500\n",
      " - 13s - loss: 0.0183 - dense_17_loss: 0.0091 - activation_2_loss: 0.0092 - val_loss: 0.0243 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0117\n",
      "Epoch 58/500\n",
      " - 13s - loss: 0.0183 - dense_17_loss: 0.0091 - activation_2_loss: 0.0092 - val_loss: 0.0243 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0117\n",
      "Epoch 59/500\n",
      " - 13s - loss: 0.0183 - dense_17_loss: 0.0091 - activation_2_loss: 0.0092 - val_loss: 0.0250 - val_dense_17_loss: 0.0122 - val_activation_2_loss: 0.0120\n",
      "Epoch 60/500\n",
      " - 13s - loss: 0.0183 - dense_17_loss: 0.0091 - activation_2_loss: 0.0092 - val_loss: 0.0250 - val_dense_17_loss: 0.0120 - val_activation_2_loss: 0.0120\n",
      "Epoch 61/500\n",
      " - 13s - loss: 0.0182 - dense_17_loss: 0.0091 - activation_2_loss: 0.0091 - val_loss: 0.0248 - val_dense_17_loss: 0.0122 - val_activation_2_loss: 0.0121\n",
      "Epoch 62/500\n",
      " - 13s - loss: 0.0182 - dense_17_loss: 0.0091 - activation_2_loss: 0.0091 - val_loss: 0.0249 - val_dense_17_loss: 0.0120 - val_activation_2_loss: 0.0120\n",
      "Epoch 63/500\n",
      " - 13s - loss: 0.0181 - dense_17_loss: 0.0091 - activation_2_loss: 0.0091 - val_loss: 0.0244 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 64/500\n",
      " - 12s - loss: 0.0182 - dense_17_loss: 0.0091 - activation_2_loss: 0.0091 - val_loss: 0.0249 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 65/500\n",
      " - 13s - loss: 0.0182 - dense_17_loss: 0.0091 - activation_2_loss: 0.0091 - val_loss: 0.0247 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 66/500\n",
      " - 13s - loss: 0.0181 - dense_17_loss: 0.0090 - activation_2_loss: 0.0091 - val_loss: 0.0244 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0117\n",
      "Epoch 67/500\n",
      " - 12s - loss: 0.0181 - dense_17_loss: 0.0090 - activation_2_loss: 0.0091 - val_loss: 0.0244 - val_dense_17_loss: 0.0117 - val_activation_2_loss: 0.0117\n",
      "Epoch 68/500\n",
      " - 13s - loss: 0.0181 - dense_17_loss: 0.0090 - activation_2_loss: 0.0091 - val_loss: 0.0245 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 69/500\n",
      " - 12s - loss: 0.0180 - dense_17_loss: 0.0090 - activation_2_loss: 0.0090 - val_loss: 0.0250 - val_dense_17_loss: 0.0121 - val_activation_2_loss: 0.0120\n",
      "Epoch 70/500\n",
      " - 12s - loss: 0.0181 - dense_17_loss: 0.0090 - activation_2_loss: 0.0091 - val_loss: 0.0249 - val_dense_17_loss: 0.0120 - val_activation_2_loss: 0.0120\n",
      "Epoch 71/500\n",
      " - 13s - loss: 0.0180 - dense_17_loss: 0.0090 - activation_2_loss: 0.0090 - val_loss: 0.0245 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 72/500\n",
      " - 12s - loss: 0.0180 - dense_17_loss: 0.0090 - activation_2_loss: 0.0090 - val_loss: 0.0244 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      "Epoch 73/500\n",
      " - 12s - loss: 0.0180 - dense_17_loss: 0.0090 - activation_2_loss: 0.0090 - val_loss: 0.0247 - val_dense_17_loss: 0.0119 - val_activation_2_loss: 0.0119\n",
      "Epoch 74/500\n",
      " - 14s - loss: 0.0180 - dense_17_loss: 0.0090 - activation_2_loss: 0.0090 - val_loss: 0.0249 - val_dense_17_loss: 0.0121 - val_activation_2_loss: 0.0120\n",
      "Epoch 75/500\n",
      " - 13s - loss: 0.0180 - dense_17_loss: 0.0090 - activation_2_loss: 0.0090 - val_loss: 0.0247 - val_dense_17_loss: 0.0122 - val_activation_2_loss: 0.0121\n",
      "Epoch 76/500\n",
      " - 12s - loss: 0.0179 - dense_17_loss: 0.0089 - activation_2_loss: 0.0090 - val_loss: 0.0246 - val_dense_17_loss: 0.0118 - val_activation_2_loss: 0.0118\n",
      " val fold: 0 0.012094094\n",
      " val fold: 0 0.0119221\n",
      "****************************************\n",
      "Train on 66693 samples, validate on 2820 samples\n",
      "Epoch 1/500\n",
      " - 18s - loss: 0.0344 - dense_26_loss: 0.0234 - activation_3_loss: 0.0110 - val_loss: 0.0275 - val_dense_26_loss: 0.0147 - val_activation_3_loss: 0.0144\n",
      "Epoch 2/500\n",
      " - 13s - loss: 0.0213 - dense_26_loss: 0.0107 - activation_3_loss: 0.0106 - val_loss: 0.0266 - val_dense_26_loss: 0.0140 - val_activation_3_loss: 0.0138\n",
      "Epoch 3/500\n",
      " - 13s - loss: 0.0207 - dense_26_loss: 0.0104 - activation_3_loss: 0.0103 - val_loss: 0.0261 - val_dense_26_loss: 0.0137 - val_activation_3_loss: 0.0135\n",
      "Epoch 4/500\n",
      " - 13s - loss: 0.0203 - dense_26_loss: 0.0102 - activation_3_loss: 0.0101 - val_loss: 0.0257 - val_dense_26_loss: 0.0133 - val_activation_3_loss: 0.0133\n",
      "Epoch 5/500\n",
      " - 13s - loss: 0.0200 - dense_26_loss: 0.0100 - activation_3_loss: 0.0100 - val_loss: 0.0255 - val_dense_26_loss: 0.0132 - val_activation_3_loss: 0.0133\n",
      "Epoch 6/500\n",
      " - 13s - loss: 0.0199 - dense_26_loss: 0.0100 - activation_3_loss: 0.0099 - val_loss: 0.0257 - val_dense_26_loss: 0.0131 - val_activation_3_loss: 0.0132\n",
      "Epoch 7/500\n",
      " - 13s - loss: 0.0198 - dense_26_loss: 0.0099 - activation_3_loss: 0.0099 - val_loss: 0.0253 - val_dense_26_loss: 0.0131 - val_activation_3_loss: 0.0131\n",
      "Epoch 8/500\n",
      " - 13s - loss: 0.0197 - dense_26_loss: 0.0099 - activation_3_loss: 0.0099 - val_loss: 0.0253 - val_dense_26_loss: 0.0129 - val_activation_3_loss: 0.0130\n",
      "Epoch 9/500\n",
      " - 13s - loss: 0.0196 - dense_26_loss: 0.0098 - activation_3_loss: 0.0098 - val_loss: 0.0251 - val_dense_26_loss: 0.0127 - val_activation_3_loss: 0.0128\n",
      "Epoch 10/500\n",
      " - 12s - loss: 0.0195 - dense_26_loss: 0.0098 - activation_3_loss: 0.0098 - val_loss: 0.0251 - val_dense_26_loss: 0.0128 - val_activation_3_loss: 0.0129\n",
      "Epoch 11/500\n",
      " - 13s - loss: 0.0194 - dense_26_loss: 0.0097 - activation_3_loss: 0.0097 - val_loss: 0.0252 - val_dense_26_loss: 0.0128 - val_activation_3_loss: 0.0129\n",
      "Epoch 12/500\n",
      " - 13s - loss: 0.0194 - dense_26_loss: 0.0097 - activation_3_loss: 0.0097 - val_loss: 0.0252 - val_dense_26_loss: 0.0128 - val_activation_3_loss: 0.0128\n",
      "Epoch 13/500\n",
      " - 13s - loss: 0.0194 - dense_26_loss: 0.0097 - activation_3_loss: 0.0097 - val_loss: 0.0251 - val_dense_26_loss: 0.0128 - val_activation_3_loss: 0.0129\n",
      "Epoch 14/500\n",
      " - 13s - loss: 0.0193 - dense_26_loss: 0.0096 - activation_3_loss: 0.0097 - val_loss: 0.0251 - val_dense_26_loss: 0.0128 - val_activation_3_loss: 0.0129\n",
      "Epoch 15/500\n",
      " - 13s - loss: 0.0192 - dense_26_loss: 0.0096 - activation_3_loss: 0.0096 - val_loss: 0.0251 - val_dense_26_loss: 0.0130 - val_activation_3_loss: 0.0130\n",
      "Epoch 16/500\n",
      " - 13s - loss: 0.0192 - dense_26_loss: 0.0096 - activation_3_loss: 0.0096 - val_loss: 0.0251 - val_dense_26_loss: 0.0129 - val_activation_3_loss: 0.0129\n",
      "Epoch 17/500\n",
      " - 13s - loss: 0.0192 - dense_26_loss: 0.0096 - activation_3_loss: 0.0096 - val_loss: 0.0248 - val_dense_26_loss: 0.0127 - val_activation_3_loss: 0.0128\n",
      "Epoch 18/500\n",
      " - 13s - loss: 0.0191 - dense_26_loss: 0.0096 - activation_3_loss: 0.0096 - val_loss: 0.0249 - val_dense_26_loss: 0.0127 - val_activation_3_loss: 0.0128\n",
      "Epoch 19/500\n",
      " - 12s - loss: 0.0191 - dense_26_loss: 0.0095 - activation_3_loss: 0.0095 - val_loss: 0.0250 - val_dense_26_loss: 0.0126 - val_activation_3_loss: 0.0127\n",
      "Epoch 20/500\n",
      " - 14s - loss: 0.0191 - dense_26_loss: 0.0095 - activation_3_loss: 0.0095 - val_loss: 0.0249 - val_dense_26_loss: 0.0128 - val_activation_3_loss: 0.0128\n",
      "Epoch 21/500\n",
      " - 13s - loss: 0.0190 - dense_26_loss: 0.0095 - activation_3_loss: 0.0095 - val_loss: 0.0252 - val_dense_26_loss: 0.0130 - val_activation_3_loss: 0.0131\n",
      "Epoch 22/500\n",
      " - 13s - loss: 0.0190 - dense_26_loss: 0.0095 - activation_3_loss: 0.0095 - val_loss: 0.0249 - val_dense_26_loss: 0.0127 - val_activation_3_loss: 0.0128\n",
      "Epoch 23/500\n",
      " - 13s - loss: 0.0190 - dense_26_loss: 0.0095 - activation_3_loss: 0.0095 - val_loss: 0.0249 - val_dense_26_loss: 0.0127 - val_activation_3_loss: 0.0128\n",
      "Epoch 24/500\n",
      " - 13s - loss: 0.0190 - dense_26_loss: 0.0095 - activation_3_loss: 0.0095 - val_loss: 0.0249 - val_dense_26_loss: 0.0128 - val_activation_3_loss: 0.0129\n",
      "Epoch 25/500\n",
      " - 13s - loss: 0.0189 - dense_26_loss: 0.0094 - activation_3_loss: 0.0095 - val_loss: 0.0249 - val_dense_26_loss: 0.0127 - val_activation_3_loss: 0.0128\n",
      "Epoch 26/500\n",
      " - 12s - loss: 0.0189 - dense_26_loss: 0.0094 - activation_3_loss: 0.0095 - val_loss: 0.0250 - val_dense_26_loss: 0.0128 - val_activation_3_loss: 0.0129\n",
      "Epoch 27/500\n",
      " - 13s - loss: 0.0189 - dense_26_loss: 0.0094 - activation_3_loss: 0.0094 - val_loss: 0.0250 - val_dense_26_loss: 0.0129 - val_activation_3_loss: 0.0130\n",
      "Epoch 28/500\n",
      " - 13s - loss: 0.0188 - dense_26_loss: 0.0094 - activation_3_loss: 0.0094 - val_loss: 0.0248 - val_dense_26_loss: 0.0128 - val_activation_3_loss: 0.0129\n",
      "Epoch 29/500\n",
      " - 13s - loss: 0.0188 - dense_26_loss: 0.0094 - activation_3_loss: 0.0094 - val_loss: 0.0248 - val_dense_26_loss: 0.0128 - val_activation_3_loss: 0.0129\n",
      "Epoch 30/500\n",
      " - 13s - loss: 0.0188 - dense_26_loss: 0.0094 - activation_3_loss: 0.0094 - val_loss: 0.0250 - val_dense_26_loss: 0.0129 - val_activation_3_loss: 0.0128\n",
      "Epoch 31/500\n",
      " - 13s - loss: 0.0187 - dense_26_loss: 0.0093 - activation_3_loss: 0.0094 - val_loss: 0.0247 - val_dense_26_loss: 0.0126 - val_activation_3_loss: 0.0127\n",
      "Epoch 32/500\n",
      " - 13s - loss: 0.0187 - dense_26_loss: 0.0093 - activation_3_loss: 0.0094 - val_loss: 0.0249 - val_dense_26_loss: 0.0130 - val_activation_3_loss: 0.0130\n",
      "Epoch 33/500\n",
      " - 13s - loss: 0.0187 - dense_26_loss: 0.0093 - activation_3_loss: 0.0094 - val_loss: 0.0251 - val_dense_26_loss: 0.0131 - val_activation_3_loss: 0.0131\n",
      "Epoch 34/500\n",
      " - 12s - loss: 0.0187 - dense_26_loss: 0.0093 - activation_3_loss: 0.0093 - val_loss: 0.0247 - val_dense_26_loss: 0.0129 - val_activation_3_loss: 0.0129\n",
      "Epoch 35/500\n",
      " - 12s - loss: 0.0186 - dense_26_loss: 0.0093 - activation_3_loss: 0.0093 - val_loss: 0.0248 - val_dense_26_loss: 0.0130 - val_activation_3_loss: 0.0130\n",
      "Epoch 36/500\n",
      " - 13s - loss: 0.0186 - dense_26_loss: 0.0093 - activation_3_loss: 0.0093 - val_loss: 0.0249 - val_dense_26_loss: 0.0130 - val_activation_3_loss: 0.0130\n",
      "Epoch 37/500\n",
      " - 13s - loss: 0.0186 - dense_26_loss: 0.0093 - activation_3_loss: 0.0093 - val_loss: 0.0249 - val_dense_26_loss: 0.0131 - val_activation_3_loss: 0.0131\n",
      "Epoch 38/500\n",
      " - 13s - loss: 0.0186 - dense_26_loss: 0.0093 - activation_3_loss: 0.0093 - val_loss: 0.0247 - val_dense_26_loss: 0.0128 - val_activation_3_loss: 0.0129\n",
      "Epoch 39/500\n",
      " - 13s - loss: 0.0186 - dense_26_loss: 0.0093 - activation_3_loss: 0.0093 - val_loss: 0.0251 - val_dense_26_loss: 0.0129 - val_activation_3_loss: 0.0130\n",
      "Epoch 40/500\n",
      " - 13s - loss: 0.0186 - dense_26_loss: 0.0093 - activation_3_loss: 0.0093 - val_loss: 0.0249 - val_dense_26_loss: 0.0129 - val_activation_3_loss: 0.0129\n",
      "Epoch 41/500\n",
      " - 13s - loss: 0.0185 - dense_26_loss: 0.0093 - activation_3_loss: 0.0093 - val_loss: 0.0250 - val_dense_26_loss: 0.0131 - val_activation_3_loss: 0.0132\n",
      "Epoch 42/500\n",
      " - 13s - loss: 0.0185 - dense_26_loss: 0.0092 - activation_3_loss: 0.0093 - val_loss: 0.0250 - val_dense_26_loss: 0.0130 - val_activation_3_loss: 0.0130\n",
      "Epoch 43/500\n",
      " - 14s - loss: 0.0185 - dense_26_loss: 0.0092 - activation_3_loss: 0.0093 - val_loss: 0.0249 - val_dense_26_loss: 0.0131 - val_activation_3_loss: 0.0131\n",
      "Epoch 44/500\n",
      " - 13s - loss: 0.0185 - dense_26_loss: 0.0092 - activation_3_loss: 0.0093 - val_loss: 0.0248 - val_dense_26_loss: 0.0128 - val_activation_3_loss: 0.0128\n",
      "Epoch 45/500\n",
      " - 13s - loss: 0.0184 - dense_26_loss: 0.0092 - activation_3_loss: 0.0092 - val_loss: 0.0248 - val_dense_26_loss: 0.0131 - val_activation_3_loss: 0.0130\n",
      "Epoch 46/500\n",
      " - 13s - loss: 0.0184 - dense_26_loss: 0.0092 - activation_3_loss: 0.0092 - val_loss: 0.0250 - val_dense_26_loss: 0.0130 - val_activation_3_loss: 0.0130\n",
      "Epoch 47/500\n",
      " - 13s - loss: 0.0184 - dense_26_loss: 0.0092 - activation_3_loss: 0.0092 - val_loss: 0.0249 - val_dense_26_loss: 0.0131 - val_activation_3_loss: 0.0131\n",
      "Epoch 48/500\n",
      " - 13s - loss: 0.0183 - dense_26_loss: 0.0092 - activation_3_loss: 0.0092 - val_loss: 0.0251 - val_dense_26_loss: 0.0131 - val_activation_3_loss: 0.0131\n",
      "Epoch 49/500\n",
      " - 13s - loss: 0.0184 - dense_26_loss: 0.0092 - activation_3_loss: 0.0092 - val_loss: 0.0249 - val_dense_26_loss: 0.0132 - val_activation_3_loss: 0.0131\n",
      "Epoch 50/500\n",
      " - 13s - loss: 0.0184 - dense_26_loss: 0.0092 - activation_3_loss: 0.0092 - val_loss: 0.0248 - val_dense_26_loss: 0.0132 - val_activation_3_loss: 0.0132\n",
      "Epoch 51/500\n",
      " - 13s - loss: 0.0183 - dense_26_loss: 0.0091 - activation_3_loss: 0.0092 - val_loss: 0.0250 - val_dense_26_loss: 0.0133 - val_activation_3_loss: 0.0133\n",
      "Epoch 52/500\n",
      " - 13s - loss: 0.0183 - dense_26_loss: 0.0091 - activation_3_loss: 0.0092 - val_loss: 0.0250 - val_dense_26_loss: 0.0131 - val_activation_3_loss: 0.0130\n",
      "Epoch 53/500\n",
      " - 13s - loss: 0.0183 - dense_26_loss: 0.0091 - activation_3_loss: 0.0092 - val_loss: 0.0247 - val_dense_26_loss: 0.0130 - val_activation_3_loss: 0.0130\n",
      "Epoch 54/500\n",
      " - 13s - loss: 0.0183 - dense_26_loss: 0.0091 - activation_3_loss: 0.0092 - val_loss: 0.0247 - val_dense_26_loss: 0.0129 - val_activation_3_loss: 0.0128\n",
      "Epoch 55/500\n",
      " - 14s - loss: 0.0182 - dense_26_loss: 0.0091 - activation_3_loss: 0.0091 - val_loss: 0.0250 - val_dense_26_loss: 0.0131 - val_activation_3_loss: 0.0131\n",
      " val fold: 1 0.012323569\n",
      " val fold: 1 0.012178002\n",
      "****************************************\n",
      "Train on 66693 samples, validate on 2820 samples\n",
      "Epoch 1/500\n",
      " - 18s - loss: 0.0333 - dense_35_loss: 0.0224 - activation_4_loss: 0.0109 - val_loss: 0.0285 - val_dense_35_loss: 0.0140 - val_activation_4_loss: 0.0137\n",
      "Epoch 2/500\n",
      " - 13s - loss: 0.0212 - dense_35_loss: 0.0107 - activation_4_loss: 0.0105 - val_loss: 0.0276 - val_dense_35_loss: 0.0132 - val_activation_4_loss: 0.0132\n",
      "Epoch 3/500\n",
      " - 13s - loss: 0.0206 - dense_35_loss: 0.0103 - activation_4_loss: 0.0103 - val_loss: 0.0273 - val_dense_35_loss: 0.0131 - val_activation_4_loss: 0.0130\n",
      "Epoch 4/500\n",
      " - 13s - loss: 0.0203 - dense_35_loss: 0.0102 - activation_4_loss: 0.0101 - val_loss: 0.0270 - val_dense_35_loss: 0.0130 - val_activation_4_loss: 0.0129\n",
      "Epoch 5/500\n",
      " - 13s - loss: 0.0201 - dense_35_loss: 0.0100 - activation_4_loss: 0.0100 - val_loss: 0.0268 - val_dense_35_loss: 0.0129 - val_activation_4_loss: 0.0128\n",
      "Epoch 6/500\n",
      " - 13s - loss: 0.0199 - dense_35_loss: 0.0100 - activation_4_loss: 0.0100 - val_loss: 0.0264 - val_dense_35_loss: 0.0127 - val_activation_4_loss: 0.0126\n",
      "Epoch 7/500\n",
      " - 13s - loss: 0.0198 - dense_35_loss: 0.0099 - activation_4_loss: 0.0099 - val_loss: 0.0262 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0126\n",
      "Epoch 8/500\n",
      " - 13s - loss: 0.0197 - dense_35_loss: 0.0098 - activation_4_loss: 0.0098 - val_loss: 0.0262 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0126\n",
      "Epoch 9/500\n",
      " - 13s - loss: 0.0196 - dense_35_loss: 0.0098 - activation_4_loss: 0.0098 - val_loss: 0.0264 - val_dense_35_loss: 0.0128 - val_activation_4_loss: 0.0127\n",
      "Epoch 10/500\n",
      " - 13s - loss: 0.0195 - dense_35_loss: 0.0098 - activation_4_loss: 0.0098 - val_loss: 0.0261 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 11/500\n",
      " - 13s - loss: 0.0194 - dense_35_loss: 0.0097 - activation_4_loss: 0.0097 - val_loss: 0.0261 - val_dense_35_loss: 0.0127 - val_activation_4_loss: 0.0127\n",
      "Epoch 12/500\n",
      " - 13s - loss: 0.0194 - dense_35_loss: 0.0097 - activation_4_loss: 0.0097 - val_loss: 0.0261 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0126\n",
      "Epoch 13/500\n",
      " - 13s - loss: 0.0194 - dense_35_loss: 0.0097 - activation_4_loss: 0.0097 - val_loss: 0.0261 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0126\n",
      "Epoch 14/500\n",
      " - 13s - loss: 0.0193 - dense_35_loss: 0.0096 - activation_4_loss: 0.0097 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 15/500\n",
      " - 13s - loss: 0.0193 - dense_35_loss: 0.0096 - activation_4_loss: 0.0096 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 16/500\n",
      " - 13s - loss: 0.0192 - dense_35_loss: 0.0096 - activation_4_loss: 0.0096 - val_loss: 0.0260 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0125\n",
      "Epoch 17/500\n",
      " - 13s - loss: 0.0192 - dense_35_loss: 0.0096 - activation_4_loss: 0.0096 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 18/500\n",
      " - 14s - loss: 0.0192 - dense_35_loss: 0.0096 - activation_4_loss: 0.0096 - val_loss: 0.0263 - val_dense_35_loss: 0.0127 - val_activation_4_loss: 0.0126\n",
      "Epoch 19/500\n",
      " - 13s - loss: 0.0191 - dense_35_loss: 0.0095 - activation_4_loss: 0.0096 - val_loss: 0.0259 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 20/500\n",
      " - 14s - loss: 0.0191 - dense_35_loss: 0.0095 - activation_4_loss: 0.0095 - val_loss: 0.0257 - val_dense_35_loss: 0.0124 - val_activation_4_loss: 0.0124\n",
      "Epoch 21/500\n",
      " - 14s - loss: 0.0191 - dense_35_loss: 0.0095 - activation_4_loss: 0.0095 - val_loss: 0.0258 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 22/500\n",
      " - 13s - loss: 0.0190 - dense_35_loss: 0.0095 - activation_4_loss: 0.0095 - val_loss: 0.0259 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0125\n",
      "Epoch 23/500\n",
      " - 13s - loss: 0.0190 - dense_35_loss: 0.0095 - activation_4_loss: 0.0095 - val_loss: 0.0260 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0125\n",
      "Epoch 24/500\n",
      " - 13s - loss: 0.0190 - dense_35_loss: 0.0095 - activation_4_loss: 0.0095 - val_loss: 0.0259 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0124\n",
      "Epoch 25/500\n",
      " - 13s - loss: 0.0189 - dense_35_loss: 0.0095 - activation_4_loss: 0.0095 - val_loss: 0.0259 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 26/500\n",
      " - 13s - loss: 0.0189 - dense_35_loss: 0.0094 - activation_4_loss: 0.0095 - val_loss: 0.0258 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 27/500\n",
      " - 13s - loss: 0.0189 - dense_35_loss: 0.0094 - activation_4_loss: 0.0095 - val_loss: 0.0258 - val_dense_35_loss: 0.0124 - val_activation_4_loss: 0.0124\n",
      "Epoch 28/500\n",
      " - 13s - loss: 0.0189 - dense_35_loss: 0.0094 - activation_4_loss: 0.0094 - val_loss: 0.0258 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 29/500\n",
      " - 12s - loss: 0.0189 - dense_35_loss: 0.0094 - activation_4_loss: 0.0095 - val_loss: 0.0259 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 30/500\n",
      " - 13s - loss: 0.0188 - dense_35_loss: 0.0094 - activation_4_loss: 0.0094 - val_loss: 0.0261 - val_dense_35_loss: 0.0127 - val_activation_4_loss: 0.0126\n",
      "Epoch 31/500\n",
      " - 13s - loss: 0.0188 - dense_35_loss: 0.0094 - activation_4_loss: 0.0094 - val_loss: 0.0260 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0125\n",
      "Epoch 32/500\n",
      " - 13s - loss: 0.0187 - dense_35_loss: 0.0094 - activation_4_loss: 0.0094 - val_loss: 0.0260 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0126\n",
      "Epoch 33/500\n",
      " - 14s - loss: 0.0187 - dense_35_loss: 0.0093 - activation_4_loss: 0.0094 - val_loss: 0.0258 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0124\n",
      "Epoch 34/500\n",
      " - 13s - loss: 0.0187 - dense_35_loss: 0.0093 - activation_4_loss: 0.0094 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 35/500\n",
      " - 13s - loss: 0.0187 - dense_35_loss: 0.0093 - activation_4_loss: 0.0094 - val_loss: 0.0257 - val_dense_35_loss: 0.0124 - val_activation_4_loss: 0.0124\n",
      "Epoch 36/500\n",
      " - 12s - loss: 0.0186 - dense_35_loss: 0.0093 - activation_4_loss: 0.0093 - val_loss: 0.0258 - val_dense_35_loss: 0.0124 - val_activation_4_loss: 0.0124\n",
      "Epoch 37/500\n",
      " - 13s - loss: 0.0186 - dense_35_loss: 0.0093 - activation_4_loss: 0.0093 - val_loss: 0.0257 - val_dense_35_loss: 0.0124 - val_activation_4_loss: 0.0124\n",
      "Epoch 38/500\n",
      " - 13s - loss: 0.0186 - dense_35_loss: 0.0093 - activation_4_loss: 0.0093 - val_loss: 0.0263 - val_dense_35_loss: 0.0127 - val_activation_4_loss: 0.0127\n",
      "Epoch 39/500\n",
      " - 13s - loss: 0.0186 - dense_35_loss: 0.0093 - activation_4_loss: 0.0093 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 40/500\n",
      " - 12s - loss: 0.0186 - dense_35_loss: 0.0093 - activation_4_loss: 0.0093 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 41/500\n",
      " - 13s - loss: 0.0186 - dense_35_loss: 0.0093 - activation_4_loss: 0.0093 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 42/500\n",
      " - 12s - loss: 0.0185 - dense_35_loss: 0.0092 - activation_4_loss: 0.0093 - val_loss: 0.0262 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0126\n",
      "Epoch 43/500\n",
      " - 13s - loss: 0.0185 - dense_35_loss: 0.0092 - activation_4_loss: 0.0093 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 44/500\n",
      " - 13s - loss: 0.0185 - dense_35_loss: 0.0092 - activation_4_loss: 0.0093 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0124\n",
      "Epoch 45/500\n",
      " - 12s - loss: 0.0185 - dense_35_loss: 0.0093 - activation_4_loss: 0.0093 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 46/500\n",
      " - 13s - loss: 0.0185 - dense_35_loss: 0.0093 - activation_4_loss: 0.0093 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 47/500\n",
      " - 13s - loss: 0.0185 - dense_35_loss: 0.0092 - activation_4_loss: 0.0093 - val_loss: 0.0262 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0125\n",
      "Epoch 48/500\n",
      " - 13s - loss: 0.0184 - dense_35_loss: 0.0092 - activation_4_loss: 0.0092 - val_loss: 0.0259 - val_dense_35_loss: 0.0124 - val_activation_4_loss: 0.0124\n",
      "Epoch 49/500\n",
      " - 13s - loss: 0.0184 - dense_35_loss: 0.0092 - activation_4_loss: 0.0092 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 50/500\n",
      " - 13s - loss: 0.0184 - dense_35_loss: 0.0092 - activation_4_loss: 0.0092 - val_loss: 0.0259 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 51/500\n",
      " - 13s - loss: 0.0184 - dense_35_loss: 0.0092 - activation_4_loss: 0.0092 - val_loss: 0.0263 - val_dense_35_loss: 0.0127 - val_activation_4_loss: 0.0126\n",
      "Epoch 52/500\n",
      " - 13s - loss: 0.0183 - dense_35_loss: 0.0092 - activation_4_loss: 0.0092 - val_loss: 0.0259 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 53/500\n",
      " - 13s - loss: 0.0183 - dense_35_loss: 0.0092 - activation_4_loss: 0.0092 - val_loss: 0.0263 - val_dense_35_loss: 0.0127 - val_activation_4_loss: 0.0127\n",
      "Epoch 54/500\n",
      " - 12s - loss: 0.0183 - dense_35_loss: 0.0091 - activation_4_loss: 0.0092 - val_loss: 0.0260 - val_dense_35_loss: 0.0125 - val_activation_4_loss: 0.0125\n",
      "Epoch 55/500\n",
      " - 13s - loss: 0.0183 - dense_35_loss: 0.0091 - activation_4_loss: 0.0092 - val_loss: 0.0262 - val_dense_35_loss: 0.0127 - val_activation_4_loss: 0.0126\n",
      "Epoch 56/500\n",
      " - 14s - loss: 0.0183 - dense_35_loss: 0.0091 - activation_4_loss: 0.0092 - val_loss: 0.0261 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0125\n",
      "Epoch 57/500\n",
      " - 13s - loss: 0.0183 - dense_35_loss: 0.0091 - activation_4_loss: 0.0092 - val_loss: 0.0261 - val_dense_35_loss: 0.0126 - val_activation_4_loss: 0.0126\n",
      "Epoch 58/500\n",
      " - 13s - loss: 0.0182 - dense_35_loss: 0.0091 - activation_4_loss: 0.0091 - val_loss: 0.0262 - val_dense_35_loss: 0.0127 - val_activation_4_loss: 0.0126\n",
      " val fold: 2 0.012815344\n",
      " val fold: 2 0.0126527725\n",
      "****************************************\n",
      "Train on 66696 samples, validate on 2817 samples\n",
      "Epoch 1/500\n",
      " - 19s - loss: 0.0332 - dense_44_loss: 0.0223 - activation_5_loss: 0.0108 - val_loss: 0.0321 - val_dense_44_loss: 0.0209 - val_activation_5_loss: 0.0210\n",
      "Epoch 2/500\n",
      " - 13s - loss: 0.0209 - dense_44_loss: 0.0105 - activation_5_loss: 0.0104 - val_loss: 0.0310 - val_dense_44_loss: 0.0201 - val_activation_5_loss: 0.0200\n",
      "Epoch 3/500\n",
      " - 13s - loss: 0.0204 - dense_44_loss: 0.0102 - activation_5_loss: 0.0102 - val_loss: 0.0305 - val_dense_44_loss: 0.0197 - val_activation_5_loss: 0.0196\n",
      "Epoch 4/500\n",
      " - 13s - loss: 0.0200 - dense_44_loss: 0.0100 - activation_5_loss: 0.0100 - val_loss: 0.0305 - val_dense_44_loss: 0.0193 - val_activation_5_loss: 0.0192\n",
      "Epoch 5/500\n",
      " - 13s - loss: 0.0198 - dense_44_loss: 0.0099 - activation_5_loss: 0.0099 - val_loss: 0.0302 - val_dense_44_loss: 0.0194 - val_activation_5_loss: 0.0195\n",
      "Epoch 6/500\n",
      " - 13s - loss: 0.0197 - dense_44_loss: 0.0098 - activation_5_loss: 0.0098 - val_loss: 0.0300 - val_dense_44_loss: 0.0196 - val_activation_5_loss: 0.0197\n",
      "Epoch 7/500\n",
      " - 13s - loss: 0.0196 - dense_44_loss: 0.0098 - activation_5_loss: 0.0098 - val_loss: 0.0301 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0200\n",
      "Epoch 8/500\n",
      " - 14s - loss: 0.0195 - dense_44_loss: 0.0097 - activation_5_loss: 0.0097 - val_loss: 0.0300 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0200\n",
      "Epoch 9/500\n",
      " - 14s - loss: 0.0194 - dense_44_loss: 0.0097 - activation_5_loss: 0.0097 - val_loss: 0.0299 - val_dense_44_loss: 0.0195 - val_activation_5_loss: 0.0196\n",
      "Epoch 10/500\n",
      " - 14s - loss: 0.0193 - dense_44_loss: 0.0097 - activation_5_loss: 0.0097 - val_loss: 0.0300 - val_dense_44_loss: 0.0201 - val_activation_5_loss: 0.0203\n",
      "Epoch 11/500\n",
      " - 14s - loss: 0.0193 - dense_44_loss: 0.0096 - activation_5_loss: 0.0097 - val_loss: 0.0297 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      "Epoch 12/500\n",
      " - 14s - loss: 0.0192 - dense_44_loss: 0.0096 - activation_5_loss: 0.0096 - val_loss: 0.0295 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0200\n",
      "Epoch 13/500\n",
      " - 14s - loss: 0.0192 - dense_44_loss: 0.0096 - activation_5_loss: 0.0096 - val_loss: 0.0296 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0200\n",
      "Epoch 14/500\n",
      " - 15s - loss: 0.0191 - dense_44_loss: 0.0095 - activation_5_loss: 0.0095 - val_loss: 0.0294 - val_dense_44_loss: 0.0197 - val_activation_5_loss: 0.0199\n",
      "Epoch 15/500\n",
      " - 14s - loss: 0.0191 - dense_44_loss: 0.0095 - activation_5_loss: 0.0095 - val_loss: 0.0295 - val_dense_44_loss: 0.0197 - val_activation_5_loss: 0.0199\n",
      "Epoch 16/500\n",
      " - 14s - loss: 0.0190 - dense_44_loss: 0.0095 - activation_5_loss: 0.0095 - val_loss: 0.0297 - val_dense_44_loss: 0.0200 - val_activation_5_loss: 0.0202\n",
      "Epoch 17/500\n",
      " - 14s - loss: 0.0190 - dense_44_loss: 0.0095 - activation_5_loss: 0.0095 - val_loss: 0.0295 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      "Epoch 18/500\n",
      " - 14s - loss: 0.0189 - dense_44_loss: 0.0095 - activation_5_loss: 0.0095 - val_loss: 0.0297 - val_dense_44_loss: 0.0202 - val_activation_5_loss: 0.0205\n",
      "Epoch 19/500\n",
      " - 13s - loss: 0.0189 - dense_44_loss: 0.0094 - activation_5_loss: 0.0095 - val_loss: 0.0302 - val_dense_44_loss: 0.0204 - val_activation_5_loss: 0.0206\n",
      "Epoch 20/500\n",
      " - 14s - loss: 0.0189 - dense_44_loss: 0.0094 - activation_5_loss: 0.0095 - val_loss: 0.0298 - val_dense_44_loss: 0.0202 - val_activation_5_loss: 0.0204\n",
      "Epoch 21/500\n",
      " - 13s - loss: 0.0189 - dense_44_loss: 0.0094 - activation_5_loss: 0.0095 - val_loss: 0.0296 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0202\n",
      "Epoch 22/500\n",
      " - 13s - loss: 0.0188 - dense_44_loss: 0.0094 - activation_5_loss: 0.0094 - val_loss: 0.0296 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0201\n",
      "Epoch 23/500\n",
      " - 13s - loss: 0.0188 - dense_44_loss: 0.0094 - activation_5_loss: 0.0094 - val_loss: 0.0295 - val_dense_44_loss: 0.0200 - val_activation_5_loss: 0.0203\n",
      "Epoch 24/500\n",
      " - 13s - loss: 0.0188 - dense_44_loss: 0.0094 - activation_5_loss: 0.0094 - val_loss: 0.0295 - val_dense_44_loss: 0.0200 - val_activation_5_loss: 0.0203\n",
      "Epoch 25/500\n",
      " - 13s - loss: 0.0187 - dense_44_loss: 0.0093 - activation_5_loss: 0.0094 - val_loss: 0.0292 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0201\n",
      "Epoch 26/500\n",
      " - 13s - loss: 0.0187 - dense_44_loss: 0.0093 - activation_5_loss: 0.0094 - val_loss: 0.0295 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0201\n",
      "Epoch 27/500\n",
      " - 13s - loss: 0.0187 - dense_44_loss: 0.0093 - activation_5_loss: 0.0093 - val_loss: 0.0294 - val_dense_44_loss: 0.0197 - val_activation_5_loss: 0.0200\n",
      "Epoch 28/500\n",
      " - 13s - loss: 0.0187 - dense_44_loss: 0.0093 - activation_5_loss: 0.0093 - val_loss: 0.0296 - val_dense_44_loss: 0.0200 - val_activation_5_loss: 0.0203\n",
      "Epoch 29/500\n",
      " - 13s - loss: 0.0186 - dense_44_loss: 0.0093 - activation_5_loss: 0.0093 - val_loss: 0.0295 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0202\n",
      "Epoch 30/500\n",
      " - 13s - loss: 0.0186 - dense_44_loss: 0.0093 - activation_5_loss: 0.0093 - val_loss: 0.0293 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      "Epoch 31/500\n",
      " - 13s - loss: 0.0186 - dense_44_loss: 0.0093 - activation_5_loss: 0.0093 - val_loss: 0.0293 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0202\n",
      "Epoch 32/500\n",
      " - 13s - loss: 0.0185 - dense_44_loss: 0.0093 - activation_5_loss: 0.0093 - val_loss: 0.0292 - val_dense_44_loss: 0.0197 - val_activation_5_loss: 0.0200\n",
      "Epoch 33/500\n",
      " - 13s - loss: 0.0185 - dense_44_loss: 0.0092 - activation_5_loss: 0.0093 - val_loss: 0.0290 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0200\n",
      "Epoch 34/500\n",
      " - 13s - loss: 0.0185 - dense_44_loss: 0.0092 - activation_5_loss: 0.0093 - val_loss: 0.0293 - val_dense_44_loss: 0.0200 - val_activation_5_loss: 0.0203\n",
      "Epoch 35/500\n",
      " - 13s - loss: 0.0185 - dense_44_loss: 0.0092 - activation_5_loss: 0.0092 - val_loss: 0.0295 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      "Epoch 36/500\n",
      " - 13s - loss: 0.0184 - dense_44_loss: 0.0092 - activation_5_loss: 0.0092 - val_loss: 0.0294 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0201\n",
      "Epoch 37/500\n",
      " - 13s - loss: 0.0185 - dense_44_loss: 0.0092 - activation_5_loss: 0.0092 - val_loss: 0.0296 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0201\n",
      "Epoch 38/500\n",
      " - 13s - loss: 0.0184 - dense_44_loss: 0.0092 - activation_5_loss: 0.0092 - val_loss: 0.0297 - val_dense_44_loss: 0.0202 - val_activation_5_loss: 0.0205\n",
      "Epoch 39/500\n",
      " - 13s - loss: 0.0184 - dense_44_loss: 0.0092 - activation_5_loss: 0.0092 - val_loss: 0.0295 - val_dense_44_loss: 0.0200 - val_activation_5_loss: 0.0202\n",
      "Epoch 40/500\n",
      " - 13s - loss: 0.0184 - dense_44_loss: 0.0092 - activation_5_loss: 0.0092 - val_loss: 0.0292 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0201\n",
      "Epoch 41/500\n",
      " - 14s - loss: 0.0184 - dense_44_loss: 0.0092 - activation_5_loss: 0.0092 - val_loss: 0.0293 - val_dense_44_loss: 0.0200 - val_activation_5_loss: 0.0203\n",
      "Epoch 42/500\n",
      " - 13s - loss: 0.0184 - dense_44_loss: 0.0092 - activation_5_loss: 0.0092 - val_loss: 0.0296 - val_dense_44_loss: 0.0200 - val_activation_5_loss: 0.0202\n",
      "Epoch 43/500\n",
      " - 13s - loss: 0.0183 - dense_44_loss: 0.0092 - activation_5_loss: 0.0092 - val_loss: 0.0293 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      "Epoch 44/500\n",
      " - 13s - loss: 0.0183 - dense_44_loss: 0.0091 - activation_5_loss: 0.0092 - val_loss: 0.0293 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      "Epoch 45/500\n",
      " - 13s - loss: 0.0183 - dense_44_loss: 0.0091 - activation_5_loss: 0.0091 - val_loss: 0.0291 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0201\n",
      "Epoch 46/500\n",
      " - 13s - loss: 0.0183 - dense_44_loss: 0.0091 - activation_5_loss: 0.0092 - val_loss: 0.0294 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0200\n",
      "Epoch 47/500\n",
      " - 13s - loss: 0.0182 - dense_44_loss: 0.0091 - activation_5_loss: 0.0091 - val_loss: 0.0294 - val_dense_44_loss: 0.0197 - val_activation_5_loss: 0.0200\n",
      "Epoch 48/500\n",
      " - 13s - loss: 0.0182 - dense_44_loss: 0.0091 - activation_5_loss: 0.0091 - val_loss: 0.0292 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      "Epoch 49/500\n",
      " - 13s - loss: 0.0182 - dense_44_loss: 0.0091 - activation_5_loss: 0.0091 - val_loss: 0.0297 - val_dense_44_loss: 0.0201 - val_activation_5_loss: 0.0203\n",
      "Epoch 50/500\n",
      " - 13s - loss: 0.0182 - dense_44_loss: 0.0091 - activation_5_loss: 0.0091 - val_loss: 0.0291 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0202\n",
      "Epoch 51/500\n",
      " - 13s - loss: 0.0182 - dense_44_loss: 0.0091 - activation_5_loss: 0.0091 - val_loss: 0.0293 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0201\n",
      "Epoch 52/500\n",
      " - 13s - loss: 0.0182 - dense_44_loss: 0.0091 - activation_5_loss: 0.0091 - val_loss: 0.0289 - val_dense_44_loss: 0.0197 - val_activation_5_loss: 0.0199\n",
      "Epoch 53/500\n",
      " - 13s - loss: 0.0181 - dense_44_loss: 0.0090 - activation_5_loss: 0.0091 - val_loss: 0.0294 - val_dense_44_loss: 0.0200 - val_activation_5_loss: 0.0203\n",
      "Epoch 54/500\n",
      " - 13s - loss: 0.0182 - dense_44_loss: 0.0091 - activation_5_loss: 0.0091 - val_loss: 0.0293 - val_dense_44_loss: 0.0200 - val_activation_5_loss: 0.0202\n",
      "Epoch 55/500\n",
      " - 13s - loss: 0.0181 - dense_44_loss: 0.0090 - activation_5_loss: 0.0091 - val_loss: 0.0294 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0200\n",
      "Epoch 56/500\n",
      " - 13s - loss: 0.0181 - dense_44_loss: 0.0090 - activation_5_loss: 0.0091 - val_loss: 0.0293 - val_dense_44_loss: 0.0197 - val_activation_5_loss: 0.0199\n",
      "Epoch 57/500\n",
      " - 13s - loss: 0.0181 - dense_44_loss: 0.0090 - activation_5_loss: 0.0091 - val_loss: 0.0293 - val_dense_44_loss: 0.0197 - val_activation_5_loss: 0.0199\n",
      "Epoch 58/500\n",
      " - 13s - loss: 0.0181 - dense_44_loss: 0.0090 - activation_5_loss: 0.0091 - val_loss: 0.0291 - val_dense_44_loss: 0.0197 - val_activation_5_loss: 0.0200\n",
      "Epoch 59/500\n",
      " - 13s - loss: 0.0181 - dense_44_loss: 0.0090 - activation_5_loss: 0.0091 - val_loss: 0.0293 - val_dense_44_loss: 0.0197 - val_activation_5_loss: 0.0199\n",
      "Epoch 60/500\n",
      " - 13s - loss: 0.0180 - dense_44_loss: 0.0090 - activation_5_loss: 0.0090 - val_loss: 0.0292 - val_dense_44_loss: 0.0196 - val_activation_5_loss: 0.0199\n",
      "Epoch 61/500\n",
      " - 13s - loss: 0.0180 - dense_44_loss: 0.0090 - activation_5_loss: 0.0090 - val_loss: 0.0293 - val_dense_44_loss: 0.0197 - val_activation_5_loss: 0.0200\n",
      "Epoch 62/500\n",
      " - 13s - loss: 0.0181 - dense_44_loss: 0.0090 - activation_5_loss: 0.0090 - val_loss: 0.0293 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      "Epoch 63/500\n",
      " - 13s - loss: 0.0180 - dense_44_loss: 0.0090 - activation_5_loss: 0.0090 - val_loss: 0.0292 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      "Epoch 64/500\n",
      " - 14s - loss: 0.0180 - dense_44_loss: 0.0090 - activation_5_loss: 0.0090 - val_loss: 0.0292 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0201\n",
      "Epoch 65/500\n",
      " - 13s - loss: 0.0179 - dense_44_loss: 0.0089 - activation_5_loss: 0.0090 - val_loss: 0.0293 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0201\n",
      "Epoch 66/500\n",
      " - 13s - loss: 0.0179 - dense_44_loss: 0.0090 - activation_5_loss: 0.0090 - val_loss: 0.0293 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      "Epoch 67/500\n",
      " - 13s - loss: 0.0179 - dense_44_loss: 0.0089 - activation_5_loss: 0.0090 - val_loss: 0.0292 - val_dense_44_loss: 0.0200 - val_activation_5_loss: 0.0202\n",
      "Epoch 68/500\n",
      " - 13s - loss: 0.0179 - dense_44_loss: 0.0089 - activation_5_loss: 0.0090 - val_loss: 0.0294 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      "Epoch 69/500\n",
      " - 13s - loss: 0.0179 - dense_44_loss: 0.0089 - activation_5_loss: 0.0090 - val_loss: 0.0293 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0201\n",
      "Epoch 70/500\n",
      " - 13s - loss: 0.0179 - dense_44_loss: 0.0089 - activation_5_loss: 0.0090 - val_loss: 0.0292 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      "Epoch 71/500\n",
      " - 13s - loss: 0.0179 - dense_44_loss: 0.0089 - activation_5_loss: 0.0090 - val_loss: 0.0294 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0201\n",
      "Epoch 72/500\n",
      " - 13s - loss: 0.0179 - dense_44_loss: 0.0089 - activation_5_loss: 0.0090 - val_loss: 0.0293 - val_dense_44_loss: 0.0199 - val_activation_5_loss: 0.0201\n",
      "Epoch 73/500\n",
      " - 12s - loss: 0.0178 - dense_44_loss: 0.0089 - activation_5_loss: 0.0089 - val_loss: 0.0295 - val_dense_44_loss: 0.0198 - val_activation_5_loss: 0.0200\n",
      " val fold: 3 0.014440916\n",
      " val fold: 3 0.014294522\n",
      "****************************************\n",
      "Train on 66696 samples, validate on 2817 samples\n",
      "Epoch 1/500\n",
      " - 19s - loss: 0.0334 - dense_53_loss: 0.0225 - activation_6_loss: 0.0109 - val_loss: 0.0294 - val_dense_53_loss: 0.0140 - val_activation_6_loss: 0.0138\n",
      "Epoch 2/500\n",
      " - 13s - loss: 0.0212 - dense_53_loss: 0.0107 - activation_6_loss: 0.0105 - val_loss: 0.0284 - val_dense_53_loss: 0.0134 - val_activation_6_loss: 0.0132\n",
      "Epoch 3/500\n",
      " - 13s - loss: 0.0206 - dense_53_loss: 0.0103 - activation_6_loss: 0.0102 - val_loss: 0.0280 - val_dense_53_loss: 0.0131 - val_activation_6_loss: 0.0131\n",
      "Epoch 4/500\n",
      " - 13s - loss: 0.0202 - dense_53_loss: 0.0101 - activation_6_loss: 0.0101 - val_loss: 0.0278 - val_dense_53_loss: 0.0130 - val_activation_6_loss: 0.0130\n",
      "Epoch 5/500\n",
      " - 13s - loss: 0.0200 - dense_53_loss: 0.0100 - activation_6_loss: 0.0100 - val_loss: 0.0277 - val_dense_53_loss: 0.0130 - val_activation_6_loss: 0.0130\n",
      "Epoch 6/500\n",
      " - 13s - loss: 0.0198 - dense_53_loss: 0.0099 - activation_6_loss: 0.0099 - val_loss: 0.0277 - val_dense_53_loss: 0.0131 - val_activation_6_loss: 0.0131\n",
      "Epoch 7/500\n",
      " - 14s - loss: 0.0197 - dense_53_loss: 0.0099 - activation_6_loss: 0.0099 - val_loss: 0.0277 - val_dense_53_loss: 0.0131 - val_activation_6_loss: 0.0131\n",
      "Epoch 8/500\n",
      " - 13s - loss: 0.0196 - dense_53_loss: 0.0098 - activation_6_loss: 0.0098 - val_loss: 0.0275 - val_dense_53_loss: 0.0132 - val_activation_6_loss: 0.0131\n",
      "Epoch 9/500\n",
      " - 13s - loss: 0.0195 - dense_53_loss: 0.0098 - activation_6_loss: 0.0098 - val_loss: 0.0274 - val_dense_53_loss: 0.0131 - val_activation_6_loss: 0.0131\n",
      "Epoch 10/500\n",
      " - 13s - loss: 0.0194 - dense_53_loss: 0.0097 - activation_6_loss: 0.0097 - val_loss: 0.0273 - val_dense_53_loss: 0.0130 - val_activation_6_loss: 0.0130\n",
      "Epoch 11/500\n",
      " - 14s - loss: 0.0194 - dense_53_loss: 0.0097 - activation_6_loss: 0.0097 - val_loss: 0.0272 - val_dense_53_loss: 0.0129 - val_activation_6_loss: 0.0129\n",
      "Epoch 12/500\n",
      " - 13s - loss: 0.0193 - dense_53_loss: 0.0097 - activation_6_loss: 0.0097 - val_loss: 0.0271 - val_dense_53_loss: 0.0129 - val_activation_6_loss: 0.0130\n",
      "Epoch 13/500\n",
      " - 13s - loss: 0.0193 - dense_53_loss: 0.0096 - activation_6_loss: 0.0097 - val_loss: 0.0272 - val_dense_53_loss: 0.0131 - val_activation_6_loss: 0.0131\n",
      "Epoch 14/500\n",
      " - 13s - loss: 0.0192 - dense_53_loss: 0.0096 - activation_6_loss: 0.0096 - val_loss: 0.0273 - val_dense_53_loss: 0.0128 - val_activation_6_loss: 0.0129\n",
      "Epoch 15/500\n",
      " - 13s - loss: 0.0192 - dense_53_loss: 0.0096 - activation_6_loss: 0.0096 - val_loss: 0.0270 - val_dense_53_loss: 0.0127 - val_activation_6_loss: 0.0128\n",
      "Epoch 16/500\n",
      " - 13s - loss: 0.0192 - dense_53_loss: 0.0096 - activation_6_loss: 0.0096 - val_loss: 0.0273 - val_dense_53_loss: 0.0129 - val_activation_6_loss: 0.0130\n",
      "Epoch 17/500\n",
      " - 13s - loss: 0.0191 - dense_53_loss: 0.0096 - activation_6_loss: 0.0096 - val_loss: 0.0270 - val_dense_53_loss: 0.0127 - val_activation_6_loss: 0.0128\n",
      "Epoch 18/500\n",
      " - 13s - loss: 0.0191 - dense_53_loss: 0.0095 - activation_6_loss: 0.0096 - val_loss: 0.0269 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0127\n",
      "Epoch 19/500\n",
      " - 14s - loss: 0.0191 - dense_53_loss: 0.0095 - activation_6_loss: 0.0095 - val_loss: 0.0268 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0127\n",
      "Epoch 20/500\n",
      " - 13s - loss: 0.0190 - dense_53_loss: 0.0095 - activation_6_loss: 0.0095 - val_loss: 0.0271 - val_dense_53_loss: 0.0128 - val_activation_6_loss: 0.0128\n",
      "Epoch 21/500\n",
      " - 13s - loss: 0.0190 - dense_53_loss: 0.0095 - activation_6_loss: 0.0095 - val_loss: 0.0271 - val_dense_53_loss: 0.0128 - val_activation_6_loss: 0.0128\n",
      "Epoch 22/500\n",
      " - 13s - loss: 0.0190 - dense_53_loss: 0.0095 - activation_6_loss: 0.0095 - val_loss: 0.0269 - val_dense_53_loss: 0.0128 - val_activation_6_loss: 0.0128\n",
      "Epoch 23/500\n",
      " - 13s - loss: 0.0189 - dense_53_loss: 0.0095 - activation_6_loss: 0.0095 - val_loss: 0.0267 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0127\n",
      "Epoch 24/500\n",
      " - 14s - loss: 0.0189 - dense_53_loss: 0.0095 - activation_6_loss: 0.0095 - val_loss: 0.0268 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0127\n",
      "Epoch 25/500\n",
      " - 13s - loss: 0.0189 - dense_53_loss: 0.0094 - activation_6_loss: 0.0094 - val_loss: 0.0268 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0127\n",
      "Epoch 26/500\n",
      " - 13s - loss: 0.0189 - dense_53_loss: 0.0094 - activation_6_loss: 0.0094 - val_loss: 0.0267 - val_dense_53_loss: 0.0125 - val_activation_6_loss: 0.0125\n",
      "Epoch 27/500\n",
      " - 13s - loss: 0.0188 - dense_53_loss: 0.0094 - activation_6_loss: 0.0094 - val_loss: 0.0266 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0126\n",
      "Epoch 28/500\n",
      " - 13s - loss: 0.0188 - dense_53_loss: 0.0094 - activation_6_loss: 0.0094 - val_loss: 0.0268 - val_dense_53_loss: 0.0127 - val_activation_6_loss: 0.0128\n",
      "Epoch 29/500\n",
      " - 13s - loss: 0.0188 - dense_53_loss: 0.0094 - activation_6_loss: 0.0094 - val_loss: 0.0267 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0126\n",
      "Epoch 30/500\n",
      " - 13s - loss: 0.0188 - dense_53_loss: 0.0094 - activation_6_loss: 0.0094 - val_loss: 0.0266 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0126\n",
      "Epoch 31/500\n",
      " - 13s - loss: 0.0188 - dense_53_loss: 0.0094 - activation_6_loss: 0.0094 - val_loss: 0.0266 - val_dense_53_loss: 0.0125 - val_activation_6_loss: 0.0126\n",
      "Epoch 32/500\n",
      " - 13s - loss: 0.0187 - dense_53_loss: 0.0093 - activation_6_loss: 0.0094 - val_loss: 0.0267 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0127\n",
      "Epoch 33/500\n",
      " - 13s - loss: 0.0187 - dense_53_loss: 0.0093 - activation_6_loss: 0.0094 - val_loss: 0.0266 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0126\n",
      "Epoch 34/500\n",
      " - 14s - loss: 0.0187 - dense_53_loss: 0.0093 - activation_6_loss: 0.0093 - val_loss: 0.0265 - val_dense_53_loss: 0.0127 - val_activation_6_loss: 0.0127\n",
      "Epoch 35/500\n",
      " - 13s - loss: 0.0186 - dense_53_loss: 0.0093 - activation_6_loss: 0.0093 - val_loss: 0.0267 - val_dense_53_loss: 0.0125 - val_activation_6_loss: 0.0126\n",
      "Epoch 36/500\n",
      " - 13s - loss: 0.0187 - dense_53_loss: 0.0093 - activation_6_loss: 0.0093 - val_loss: 0.0269 - val_dense_53_loss: 0.0129 - val_activation_6_loss: 0.0129\n",
      "Epoch 37/500\n",
      " - 13s - loss: 0.0186 - dense_53_loss: 0.0093 - activation_6_loss: 0.0093 - val_loss: 0.0265 - val_dense_53_loss: 0.0125 - val_activation_6_loss: 0.0126\n",
      "Epoch 38/500\n",
      " - 13s - loss: 0.0186 - dense_53_loss: 0.0093 - activation_6_loss: 0.0093 - val_loss: 0.0265 - val_dense_53_loss: 0.0125 - val_activation_6_loss: 0.0125\n",
      "Epoch 39/500\n",
      " - 13s - loss: 0.0186 - dense_53_loss: 0.0093 - activation_6_loss: 0.0093 - val_loss: 0.0267 - val_dense_53_loss: 0.0127 - val_activation_6_loss: 0.0127\n",
      "Epoch 40/500\n",
      " - 13s - loss: 0.0185 - dense_53_loss: 0.0092 - activation_6_loss: 0.0093 - val_loss: 0.0266 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0126\n",
      "Epoch 41/500\n",
      " - 13s - loss: 0.0185 - dense_53_loss: 0.0092 - activation_6_loss: 0.0093 - val_loss: 0.0265 - val_dense_53_loss: 0.0127 - val_activation_6_loss: 0.0127\n",
      "Epoch 42/500\n",
      " - 13s - loss: 0.0185 - dense_53_loss: 0.0092 - activation_6_loss: 0.0093 - val_loss: 0.0266 - val_dense_53_loss: 0.0128 - val_activation_6_loss: 0.0127\n",
      "Epoch 43/500\n",
      " - 13s - loss: 0.0184 - dense_53_loss: 0.0092 - activation_6_loss: 0.0092 - val_loss: 0.0264 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0126\n",
      "Epoch 44/500\n",
      " - 13s - loss: 0.0185 - dense_53_loss: 0.0092 - activation_6_loss: 0.0092 - val_loss: 0.0264 - val_dense_53_loss: 0.0125 - val_activation_6_loss: 0.0125\n",
      "Epoch 45/500\n",
      " - 13s - loss: 0.0185 - dense_53_loss: 0.0092 - activation_6_loss: 0.0092 - val_loss: 0.0264 - val_dense_53_loss: 0.0125 - val_activation_6_loss: 0.0125\n",
      "Epoch 46/500\n",
      " - 13s - loss: 0.0184 - dense_53_loss: 0.0092 - activation_6_loss: 0.0092 - val_loss: 0.0265 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0126\n",
      "Epoch 47/500\n",
      " - 13s - loss: 0.0184 - dense_53_loss: 0.0092 - activation_6_loss: 0.0092 - val_loss: 0.0265 - val_dense_53_loss: 0.0127 - val_activation_6_loss: 0.0126\n",
      "Epoch 48/500\n",
      " - 13s - loss: 0.0184 - dense_53_loss: 0.0092 - activation_6_loss: 0.0092 - val_loss: 0.0268 - val_dense_53_loss: 0.0130 - val_activation_6_loss: 0.0129\n",
      "Epoch 49/500\n",
      " - 13s - loss: 0.0183 - dense_53_loss: 0.0091 - activation_6_loss: 0.0092 - val_loss: 0.0269 - val_dense_53_loss: 0.0129 - val_activation_6_loss: 0.0129\n",
      "Epoch 50/500\n",
      " - 13s - loss: 0.0184 - dense_53_loss: 0.0092 - activation_6_loss: 0.0092 - val_loss: 0.0267 - val_dense_53_loss: 0.0129 - val_activation_6_loss: 0.0129\n",
      "Epoch 51/500\n",
      " - 13s - loss: 0.0183 - dense_53_loss: 0.0091 - activation_6_loss: 0.0092 - val_loss: 0.0264 - val_dense_53_loss: 0.0125 - val_activation_6_loss: 0.0125\n",
      "Epoch 52/500\n",
      " - 14s - loss: 0.0183 - dense_53_loss: 0.0091 - activation_6_loss: 0.0092 - val_loss: 0.0265 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0126\n",
      "Epoch 53/500\n",
      " - 13s - loss: 0.0183 - dense_53_loss: 0.0091 - activation_6_loss: 0.0091 - val_loss: 0.0269 - val_dense_53_loss: 0.0128 - val_activation_6_loss: 0.0128\n",
      "Epoch 54/500\n",
      " - 13s - loss: 0.0183 - dense_53_loss: 0.0091 - activation_6_loss: 0.0092 - val_loss: 0.0265 - val_dense_53_loss: 0.0127 - val_activation_6_loss: 0.0127\n",
      "Epoch 55/500\n",
      " - 13s - loss: 0.0182 - dense_53_loss: 0.0091 - activation_6_loss: 0.0091 - val_loss: 0.0266 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0126\n",
      "Epoch 56/500\n",
      " - 13s - loss: 0.0182 - dense_53_loss: 0.0091 - activation_6_loss: 0.0091 - val_loss: 0.0265 - val_dense_53_loss: 0.0128 - val_activation_6_loss: 0.0127\n",
      "Epoch 57/500\n",
      " - 14s - loss: 0.0182 - dense_53_loss: 0.0091 - activation_6_loss: 0.0091 - val_loss: 0.0264 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0126\n",
      "Epoch 58/500\n",
      " - 13s - loss: 0.0182 - dense_53_loss: 0.0091 - activation_6_loss: 0.0091 - val_loss: 0.0266 - val_dense_53_loss: 0.0128 - val_activation_6_loss: 0.0128\n",
      "Epoch 59/500\n",
      " - 13s - loss: 0.0182 - dense_53_loss: 0.0091 - activation_6_loss: 0.0091 - val_loss: 0.0268 - val_dense_53_loss: 0.0128 - val_activation_6_loss: 0.0128\n",
      "Epoch 60/500\n",
      " - 13s - loss: 0.0181 - dense_53_loss: 0.0090 - activation_6_loss: 0.0091 - val_loss: 0.0267 - val_dense_53_loss: 0.0128 - val_activation_6_loss: 0.0127\n",
      "Epoch 61/500\n",
      " - 13s - loss: 0.0181 - dense_53_loss: 0.0090 - activation_6_loss: 0.0091 - val_loss: 0.0264 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0126\n",
      "Epoch 62/500\n",
      " - 13s - loss: 0.0181 - dense_53_loss: 0.0090 - activation_6_loss: 0.0091 - val_loss: 0.0268 - val_dense_53_loss: 0.0129 - val_activation_6_loss: 0.0128\n",
      "Epoch 63/500\n",
      " - 13s - loss: 0.0180 - dense_53_loss: 0.0090 - activation_6_loss: 0.0090 - val_loss: 0.0265 - val_dense_53_loss: 0.0126 - val_activation_6_loss: 0.0126\n",
      "Epoch 64/500\n",
      " - 13s - loss: 0.0181 - dense_53_loss: 0.0090 - activation_6_loss: 0.0091 - val_loss: 0.0266 - val_dense_53_loss: 0.0127 - val_activation_6_loss: 0.0126\n",
      "Epoch 65/500\n",
      " - 13s - loss: 0.0181 - dense_53_loss: 0.0090 - activation_6_loss: 0.0090 - val_loss: 0.0264 - val_dense_53_loss: 0.0125 - val_activation_6_loss: 0.0125\n",
      " val fold: 4 0.013161616\n",
      " val fold: 4 0.013022779\n",
      "****************************************\n",
      "Train on 66696 samples, validate on 2817 samples\n",
      "Epoch 1/500\n",
      " - 20s - loss: 0.0345 - dense_62_loss: 0.0236 - activation_7_loss: 0.0109 - val_loss: 0.0290 - val_dense_62_loss: 0.0138 - val_activation_7_loss: 0.0136\n",
      "Epoch 2/500\n",
      " - 13s - loss: 0.0213 - dense_62_loss: 0.0107 - activation_7_loss: 0.0106 - val_loss: 0.0284 - val_dense_62_loss: 0.0135 - val_activation_7_loss: 0.0134\n",
      "Epoch 3/500\n",
      " - 14s - loss: 0.0207 - dense_62_loss: 0.0104 - activation_7_loss: 0.0103 - val_loss: 0.0277 - val_dense_62_loss: 0.0134 - val_activation_7_loss: 0.0133\n",
      "Epoch 4/500\n",
      " - 13s - loss: 0.0203 - dense_62_loss: 0.0102 - activation_7_loss: 0.0101 - val_loss: 0.0275 - val_dense_62_loss: 0.0132 - val_activation_7_loss: 0.0132\n",
      "Epoch 5/500\n",
      " - 13s - loss: 0.0201 - dense_62_loss: 0.0100 - activation_7_loss: 0.0100 - val_loss: 0.0274 - val_dense_62_loss: 0.0135 - val_activation_7_loss: 0.0133\n",
      "Epoch 6/500\n",
      " - 13s - loss: 0.0199 - dense_62_loss: 0.0099 - activation_7_loss: 0.0099 - val_loss: 0.0272 - val_dense_62_loss: 0.0131 - val_activation_7_loss: 0.0130\n",
      "Epoch 7/500\n",
      " - 14s - loss: 0.0198 - dense_62_loss: 0.0099 - activation_7_loss: 0.0099 - val_loss: 0.0271 - val_dense_62_loss: 0.0132 - val_activation_7_loss: 0.0131\n",
      "Epoch 8/500\n",
      " - 14s - loss: 0.0196 - dense_62_loss: 0.0098 - activation_7_loss: 0.0098 - val_loss: 0.0280 - val_dense_62_loss: 0.0134 - val_activation_7_loss: 0.0134\n",
      "Epoch 9/500\n",
      " - 13s - loss: 0.0195 - dense_62_loss: 0.0098 - activation_7_loss: 0.0098 - val_loss: 0.0270 - val_dense_62_loss: 0.0129 - val_activation_7_loss: 0.0130\n",
      "Epoch 10/500\n",
      " - 13s - loss: 0.0195 - dense_62_loss: 0.0097 - activation_7_loss: 0.0097 - val_loss: 0.0270 - val_dense_62_loss: 0.0129 - val_activation_7_loss: 0.0129\n",
      "Epoch 11/500\n",
      " - 13s - loss: 0.0194 - dense_62_loss: 0.0097 - activation_7_loss: 0.0097 - val_loss: 0.0268 - val_dense_62_loss: 0.0128 - val_activation_7_loss: 0.0128\n",
      "Epoch 12/500\n",
      " - 15s - loss: 0.0193 - dense_62_loss: 0.0097 - activation_7_loss: 0.0097 - val_loss: 0.0269 - val_dense_62_loss: 0.0130 - val_activation_7_loss: 0.0130\n",
      "Epoch 13/500\n",
      " - 13s - loss: 0.0193 - dense_62_loss: 0.0096 - activation_7_loss: 0.0097 - val_loss: 0.0271 - val_dense_62_loss: 0.0131 - val_activation_7_loss: 0.0130\n",
      "Epoch 14/500\n",
      " - 14s - loss: 0.0192 - dense_62_loss: 0.0096 - activation_7_loss: 0.0096 - val_loss: 0.0267 - val_dense_62_loss: 0.0128 - val_activation_7_loss: 0.0128\n",
      "Epoch 15/500\n",
      " - 13s - loss: 0.0192 - dense_62_loss: 0.0096 - activation_7_loss: 0.0096 - val_loss: 0.0268 - val_dense_62_loss: 0.0129 - val_activation_7_loss: 0.0129\n",
      "Epoch 16/500\n",
      " - 13s - loss: 0.0192 - dense_62_loss: 0.0096 - activation_7_loss: 0.0096 - val_loss: 0.0269 - val_dense_62_loss: 0.0129 - val_activation_7_loss: 0.0128\n",
      "Epoch 17/500\n",
      " - 13s - loss: 0.0191 - dense_62_loss: 0.0095 - activation_7_loss: 0.0096 - val_loss: 0.0267 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 18/500\n",
      " - 14s - loss: 0.0190 - dense_62_loss: 0.0095 - activation_7_loss: 0.0095 - val_loss: 0.0268 - val_dense_62_loss: 0.0128 - val_activation_7_loss: 0.0127\n",
      "Epoch 19/500\n",
      " - 13s - loss: 0.0191 - dense_62_loss: 0.0095 - activation_7_loss: 0.0095 - val_loss: 0.0267 - val_dense_62_loss: 0.0128 - val_activation_7_loss: 0.0128\n",
      "Epoch 20/500\n",
      " - 13s - loss: 0.0190 - dense_62_loss: 0.0095 - activation_7_loss: 0.0095 - val_loss: 0.0270 - val_dense_62_loss: 0.0130 - val_activation_7_loss: 0.0129\n",
      "Epoch 21/500\n",
      " - 14s - loss: 0.0190 - dense_62_loss: 0.0095 - activation_7_loss: 0.0095 - val_loss: 0.0268 - val_dense_62_loss: 0.0128 - val_activation_7_loss: 0.0128\n",
      "Epoch 22/500\n",
      " - 13s - loss: 0.0190 - dense_62_loss: 0.0095 - activation_7_loss: 0.0095 - val_loss: 0.0268 - val_dense_62_loss: 0.0129 - val_activation_7_loss: 0.0129\n",
      "Epoch 23/500\n",
      " - 13s - loss: 0.0189 - dense_62_loss: 0.0095 - activation_7_loss: 0.0095 - val_loss: 0.0266 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 24/500\n",
      " - 13s - loss: 0.0189 - dense_62_loss: 0.0094 - activation_7_loss: 0.0094 - val_loss: 0.0269 - val_dense_62_loss: 0.0130 - val_activation_7_loss: 0.0129\n",
      "Epoch 25/500\n",
      " - 14s - loss: 0.0189 - dense_62_loss: 0.0094 - activation_7_loss: 0.0095 - val_loss: 0.0278 - val_dense_62_loss: 0.0136 - val_activation_7_loss: 0.0134\n",
      "Epoch 26/500\n",
      " - 13s - loss: 0.0189 - dense_62_loss: 0.0094 - activation_7_loss: 0.0094 - val_loss: 0.0266 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 27/500\n",
      " - 13s - loss: 0.0188 - dense_62_loss: 0.0094 - activation_7_loss: 0.0094 - val_loss: 0.0266 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 28/500\n",
      " - 14s - loss: 0.0188 - dense_62_loss: 0.0094 - activation_7_loss: 0.0094 - val_loss: 0.0265 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0126\n",
      "Epoch 29/500\n",
      " - 13s - loss: 0.0188 - dense_62_loss: 0.0094 - activation_7_loss: 0.0094 - val_loss: 0.0273 - val_dense_62_loss: 0.0133 - val_activation_7_loss: 0.0131\n",
      "Epoch 30/500\n",
      " - 13s - loss: 0.0188 - dense_62_loss: 0.0094 - activation_7_loss: 0.0094 - val_loss: 0.0265 - val_dense_62_loss: 0.0126 - val_activation_7_loss: 0.0126\n",
      "Epoch 31/500\n",
      " - 13s - loss: 0.0187 - dense_62_loss: 0.0093 - activation_7_loss: 0.0094 - val_loss: 0.0269 - val_dense_62_loss: 0.0130 - val_activation_7_loss: 0.0129\n",
      "Epoch 32/500\n",
      " - 13s - loss: 0.0187 - dense_62_loss: 0.0093 - activation_7_loss: 0.0094 - val_loss: 0.0267 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 33/500\n",
      " - 13s - loss: 0.0187 - dense_62_loss: 0.0093 - activation_7_loss: 0.0094 - val_loss: 0.0269 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 34/500\n",
      " - 13s - loss: 0.0187 - dense_62_loss: 0.0093 - activation_7_loss: 0.0093 - val_loss: 0.0268 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 35/500\n",
      " - 14s - loss: 0.0186 - dense_62_loss: 0.0093 - activation_7_loss: 0.0093 - val_loss: 0.0269 - val_dense_62_loss: 0.0128 - val_activation_7_loss: 0.0128\n",
      "Epoch 36/500\n",
      " - 13s - loss: 0.0186 - dense_62_loss: 0.0093 - activation_7_loss: 0.0093 - val_loss: 0.0268 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 37/500\n",
      " - 13s - loss: 0.0186 - dense_62_loss: 0.0093 - activation_7_loss: 0.0093 - val_loss: 0.0264 - val_dense_62_loss: 0.0126 - val_activation_7_loss: 0.0126\n",
      "Epoch 38/500\n",
      " - 13s - loss: 0.0186 - dense_62_loss: 0.0093 - activation_7_loss: 0.0093 - val_loss: 0.0273 - val_dense_62_loss: 0.0130 - val_activation_7_loss: 0.0130\n",
      "Epoch 39/500\n",
      " - 14s - loss: 0.0186 - dense_62_loss: 0.0093 - activation_7_loss: 0.0093 - val_loss: 0.0265 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 40/500\n",
      " - 13s - loss: 0.0186 - dense_62_loss: 0.0093 - activation_7_loss: 0.0093 - val_loss: 0.0270 - val_dense_62_loss: 0.0129 - val_activation_7_loss: 0.0129\n",
      "Epoch 41/500\n",
      " - 13s - loss: 0.0185 - dense_62_loss: 0.0092 - activation_7_loss: 0.0093 - val_loss: 0.0266 - val_dense_62_loss: 0.0125 - val_activation_7_loss: 0.0126\n",
      "Epoch 42/500\n",
      " - 13s - loss: 0.0185 - dense_62_loss: 0.0092 - activation_7_loss: 0.0093 - val_loss: 0.0268 - val_dense_62_loss: 0.0129 - val_activation_7_loss: 0.0129\n",
      "Epoch 43/500\n",
      " - 13s - loss: 0.0185 - dense_62_loss: 0.0092 - activation_7_loss: 0.0093 - val_loss: 0.0272 - val_dense_62_loss: 0.0129 - val_activation_7_loss: 0.0130\n",
      "Epoch 44/500\n",
      " - 14s - loss: 0.0184 - dense_62_loss: 0.0092 - activation_7_loss: 0.0092 - val_loss: 0.0267 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0126\n",
      "Epoch 45/500\n",
      " - 13s - loss: 0.0184 - dense_62_loss: 0.0092 - activation_7_loss: 0.0092 - val_loss: 0.0267 - val_dense_62_loss: 0.0126 - val_activation_7_loss: 0.0127\n",
      "Epoch 46/500\n",
      " - 13s - loss: 0.0184 - dense_62_loss: 0.0092 - activation_7_loss: 0.0092 - val_loss: 0.0275 - val_dense_62_loss: 0.0131 - val_activation_7_loss: 0.0129\n",
      "Epoch 47/500\n",
      " - 13s - loss: 0.0184 - dense_62_loss: 0.0092 - activation_7_loss: 0.0092 - val_loss: 0.0268 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 48/500\n",
      " - 13s - loss: 0.0184 - dense_62_loss: 0.0092 - activation_7_loss: 0.0092 - val_loss: 0.0265 - val_dense_62_loss: 0.0126 - val_activation_7_loss: 0.0126\n",
      "Epoch 49/500\n",
      " - 13s - loss: 0.0183 - dense_62_loss: 0.0091 - activation_7_loss: 0.0092 - val_loss: 0.0267 - val_dense_62_loss: 0.0126 - val_activation_7_loss: 0.0126\n",
      "Epoch 50/500\n",
      " - 13s - loss: 0.0183 - dense_62_loss: 0.0091 - activation_7_loss: 0.0092 - val_loss: 0.0267 - val_dense_62_loss: 0.0126 - val_activation_7_loss: 0.0126\n",
      "Epoch 51/500\n",
      " - 13s - loss: 0.0183 - dense_62_loss: 0.0091 - activation_7_loss: 0.0092 - val_loss: 0.0269 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 52/500\n",
      " - 13s - loss: 0.0183 - dense_62_loss: 0.0091 - activation_7_loss: 0.0091 - val_loss: 0.0265 - val_dense_62_loss: 0.0125 - val_activation_7_loss: 0.0125\n",
      "Epoch 53/500\n",
      " - 13s - loss: 0.0182 - dense_62_loss: 0.0091 - activation_7_loss: 0.0091 - val_loss: 0.0267 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 54/500\n",
      " - 13s - loss: 0.0183 - dense_62_loss: 0.0091 - activation_7_loss: 0.0092 - val_loss: 0.0269 - val_dense_62_loss: 0.0128 - val_activation_7_loss: 0.0128\n",
      "Epoch 55/500\n",
      " - 13s - loss: 0.0182 - dense_62_loss: 0.0091 - activation_7_loss: 0.0091 - val_loss: 0.0268 - val_dense_62_loss: 0.0127 - val_activation_7_loss: 0.0127\n",
      "Epoch 56/500\n",
      " - 13s - loss: 0.0181 - dense_62_loss: 0.0091 - activation_7_loss: 0.0091 - val_loss: 0.0270 - val_dense_62_loss: 0.0128 - val_activation_7_loss: 0.0127\n",
      "Epoch 57/500\n",
      " - 14s - loss: 0.0182 - dense_62_loss: 0.0091 - activation_7_loss: 0.0091 - val_loss: 0.0271 - val_dense_62_loss: 0.0129 - val_activation_7_loss: 0.0128\n",
      "Epoch 58/500\n",
      " - 14s - loss: 0.0181 - dense_62_loss: 0.0090 - activation_7_loss: 0.0091 - val_loss: 0.0270 - val_dense_62_loss: 0.0128 - val_activation_7_loss: 0.0128\n",
      " val fold: 5 0.013205413\n",
      " val fold: 5 0.0130896615\n",
      "****************************************\n",
      "Train on 66696 samples, validate on 2817 samples\n",
      "Epoch 1/500\n",
      " - 20s - loss: 0.0340 - dense_71_loss: 0.0230 - activation_8_loss: 0.0109 - val_loss: 0.0312 - val_dense_71_loss: 0.0149 - val_activation_8_loss: 0.0147\n",
      "Epoch 2/500\n",
      " - 13s - loss: 0.0211 - dense_71_loss: 0.0106 - activation_8_loss: 0.0105 - val_loss: 0.0303 - val_dense_71_loss: 0.0145 - val_activation_8_loss: 0.0144\n",
      "Epoch 3/500\n",
      " - 13s - loss: 0.0204 - dense_71_loss: 0.0103 - activation_8_loss: 0.0102 - val_loss: 0.0294 - val_dense_71_loss: 0.0143 - val_activation_8_loss: 0.0143\n",
      "Epoch 4/500\n",
      " - 14s - loss: 0.0201 - dense_71_loss: 0.0101 - activation_8_loss: 0.0100 - val_loss: 0.0291 - val_dense_71_loss: 0.0142 - val_activation_8_loss: 0.0142\n",
      "Epoch 5/500\n",
      " - 14s - loss: 0.0199 - dense_71_loss: 0.0099 - activation_8_loss: 0.0099 - val_loss: 0.0289 - val_dense_71_loss: 0.0139 - val_activation_8_loss: 0.0139\n",
      "Epoch 6/500\n",
      " - 14s - loss: 0.0197 - dense_71_loss: 0.0098 - activation_8_loss: 0.0098 - val_loss: 0.0286 - val_dense_71_loss: 0.0139 - val_activation_8_loss: 0.0140\n",
      "Epoch 7/500\n",
      " - 13s - loss: 0.0196 - dense_71_loss: 0.0098 - activation_8_loss: 0.0098 - val_loss: 0.0285 - val_dense_71_loss: 0.0137 - val_activation_8_loss: 0.0138\n",
      "Epoch 8/500\n",
      " - 13s - loss: 0.0195 - dense_71_loss: 0.0097 - activation_8_loss: 0.0098 - val_loss: 0.0284 - val_dense_71_loss: 0.0137 - val_activation_8_loss: 0.0138\n",
      "Epoch 9/500\n",
      " - 13s - loss: 0.0194 - dense_71_loss: 0.0097 - activation_8_loss: 0.0097 - val_loss: 0.0279 - val_dense_71_loss: 0.0134 - val_activation_8_loss: 0.0136\n",
      "Epoch 10/500\n",
      " - 13s - loss: 0.0194 - dense_71_loss: 0.0097 - activation_8_loss: 0.0097 - val_loss: 0.0279 - val_dense_71_loss: 0.0135 - val_activation_8_loss: 0.0136\n",
      "Epoch 11/500\n",
      " - 14s - loss: 0.0193 - dense_71_loss: 0.0096 - activation_8_loss: 0.0096 - val_loss: 0.0277 - val_dense_71_loss: 0.0133 - val_activation_8_loss: 0.0134\n",
      "Epoch 12/500\n",
      " - 14s - loss: 0.0192 - dense_71_loss: 0.0096 - activation_8_loss: 0.0096 - val_loss: 0.0280 - val_dense_71_loss: 0.0133 - val_activation_8_loss: 0.0134\n",
      "Epoch 13/500\n",
      " - 14s - loss: 0.0192 - dense_71_loss: 0.0096 - activation_8_loss: 0.0096 - val_loss: 0.0278 - val_dense_71_loss: 0.0135 - val_activation_8_loss: 0.0136\n",
      "Epoch 14/500\n",
      " - 13s - loss: 0.0191 - dense_71_loss: 0.0095 - activation_8_loss: 0.0096 - val_loss: 0.0283 - val_dense_71_loss: 0.0134 - val_activation_8_loss: 0.0134\n",
      "Epoch 15/500\n",
      " - 13s - loss: 0.0191 - dense_71_loss: 0.0095 - activation_8_loss: 0.0096 - val_loss: 0.0276 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0132\n",
      "Epoch 16/500\n",
      " - 13s - loss: 0.0190 - dense_71_loss: 0.0095 - activation_8_loss: 0.0095 - val_loss: 0.0276 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0132\n",
      "Epoch 17/500\n",
      " - 13s - loss: 0.0190 - dense_71_loss: 0.0095 - activation_8_loss: 0.0095 - val_loss: 0.0277 - val_dense_71_loss: 0.0133 - val_activation_8_loss: 0.0134\n",
      "Epoch 18/500\n",
      " - 14s - loss: 0.0190 - dense_71_loss: 0.0095 - activation_8_loss: 0.0095 - val_loss: 0.0276 - val_dense_71_loss: 0.0133 - val_activation_8_loss: 0.0134\n",
      "Epoch 19/500\n",
      " - 14s - loss: 0.0190 - dense_71_loss: 0.0095 - activation_8_loss: 0.0095 - val_loss: 0.0274 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0132\n",
      "Epoch 20/500\n",
      " - 13s - loss: 0.0189 - dense_71_loss: 0.0094 - activation_8_loss: 0.0095 - val_loss: 0.0273 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0131\n",
      "Epoch 21/500\n",
      " - 13s - loss: 0.0189 - dense_71_loss: 0.0094 - activation_8_loss: 0.0094 - val_loss: 0.0273 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0132\n",
      "Epoch 22/500\n",
      " - 14s - loss: 0.0188 - dense_71_loss: 0.0094 - activation_8_loss: 0.0094 - val_loss: 0.0276 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0132\n",
      "Epoch 23/500\n",
      " - 13s - loss: 0.0189 - dense_71_loss: 0.0094 - activation_8_loss: 0.0095 - val_loss: 0.0274 - val_dense_71_loss: 0.0132 - val_activation_8_loss: 0.0133\n",
      "Epoch 24/500\n",
      " - 13s - loss: 0.0188 - dense_71_loss: 0.0094 - activation_8_loss: 0.0094 - val_loss: 0.0273 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0131\n",
      "Epoch 25/500\n",
      " - 13s - loss: 0.0188 - dense_71_loss: 0.0094 - activation_8_loss: 0.0094 - val_loss: 0.0276 - val_dense_71_loss: 0.0132 - val_activation_8_loss: 0.0132\n",
      "Epoch 26/500\n",
      " - 13s - loss: 0.0188 - dense_71_loss: 0.0094 - activation_8_loss: 0.0094 - val_loss: 0.0273 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0132\n",
      "Epoch 27/500\n",
      " - 13s - loss: 0.0188 - dense_71_loss: 0.0094 - activation_8_loss: 0.0094 - val_loss: 0.0271 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0130\n",
      "Epoch 28/500\n",
      " - 13s - loss: 0.0187 - dense_71_loss: 0.0093 - activation_8_loss: 0.0094 - val_loss: 0.0272 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0131\n",
      "Epoch 29/500\n",
      " - 13s - loss: 0.0187 - dense_71_loss: 0.0093 - activation_8_loss: 0.0094 - val_loss: 0.0270 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0131\n",
      "Epoch 30/500\n",
      " - 13s - loss: 0.0187 - dense_71_loss: 0.0093 - activation_8_loss: 0.0093 - val_loss: 0.0271 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0129\n",
      "Epoch 31/500\n",
      " - 14s - loss: 0.0187 - dense_71_loss: 0.0093 - activation_8_loss: 0.0093 - val_loss: 0.0270 - val_dense_71_loss: 0.0128 - val_activation_8_loss: 0.0128\n",
      "Epoch 32/500\n",
      " - 13s - loss: 0.0186 - dense_71_loss: 0.0093 - activation_8_loss: 0.0093 - val_loss: 0.0270 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0131\n",
      "Epoch 33/500\n",
      " - 13s - loss: 0.0186 - dense_71_loss: 0.0093 - activation_8_loss: 0.0093 - val_loss: 0.0272 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0130\n",
      "Epoch 34/500\n",
      " - 13s - loss: 0.0186 - dense_71_loss: 0.0093 - activation_8_loss: 0.0093 - val_loss: 0.0275 - val_dense_71_loss: 0.0133 - val_activation_8_loss: 0.0132\n",
      "Epoch 35/500\n",
      " - 14s - loss: 0.0186 - dense_71_loss: 0.0093 - activation_8_loss: 0.0093 - val_loss: 0.0271 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0132\n",
      "Epoch 36/500\n",
      " - 14s - loss: 0.0185 - dense_71_loss: 0.0093 - activation_8_loss: 0.0093 - val_loss: 0.0270 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0129\n",
      "Epoch 37/500\n",
      " - 13s - loss: 0.0185 - dense_71_loss: 0.0092 - activation_8_loss: 0.0093 - val_loss: 0.0271 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0131\n",
      "Epoch 38/500\n",
      " - 14s - loss: 0.0185 - dense_71_loss: 0.0092 - activation_8_loss: 0.0093 - val_loss: 0.0273 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0130\n",
      "Epoch 39/500\n",
      " - 14s - loss: 0.0184 - dense_71_loss: 0.0092 - activation_8_loss: 0.0092 - val_loss: 0.0271 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0130\n",
      "Epoch 40/500\n",
      " - 13s - loss: 0.0185 - dense_71_loss: 0.0092 - activation_8_loss: 0.0092 - val_loss: 0.0272 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0132\n",
      "Epoch 41/500\n",
      " - 14s - loss: 0.0184 - dense_71_loss: 0.0092 - activation_8_loss: 0.0092 - val_loss: 0.0271 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0130\n",
      "Epoch 42/500\n",
      " - 14s - loss: 0.0184 - dense_71_loss: 0.0092 - activation_8_loss: 0.0092 - val_loss: 0.0271 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0130\n",
      "Epoch 43/500\n",
      " - 13s - loss: 0.0184 - dense_71_loss: 0.0092 - activation_8_loss: 0.0092 - val_loss: 0.0270 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0130\n",
      "Epoch 44/500\n",
      " - 13s - loss: 0.0183 - dense_71_loss: 0.0092 - activation_8_loss: 0.0092 - val_loss: 0.0271 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0130\n",
      "Epoch 45/500\n",
      " - 14s - loss: 0.0183 - dense_71_loss: 0.0092 - activation_8_loss: 0.0092 - val_loss: 0.0271 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0130\n",
      "Epoch 46/500\n",
      " - 13s - loss: 0.0183 - dense_71_loss: 0.0092 - activation_8_loss: 0.0092 - val_loss: 0.0271 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0131\n",
      "Epoch 47/500\n",
      " - 13s - loss: 0.0183 - dense_71_loss: 0.0091 - activation_8_loss: 0.0091 - val_loss: 0.0268 - val_dense_71_loss: 0.0128 - val_activation_8_loss: 0.0129\n",
      "Epoch 48/500\n",
      " - 13s - loss: 0.0182 - dense_71_loss: 0.0091 - activation_8_loss: 0.0091 - val_loss: 0.0269 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0130\n",
      "Epoch 49/500\n",
      " - 14s - loss: 0.0183 - dense_71_loss: 0.0091 - activation_8_loss: 0.0092 - val_loss: 0.0273 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0130\n",
      "Epoch 50/500\n",
      " - 13s - loss: 0.0182 - dense_71_loss: 0.0091 - activation_8_loss: 0.0091 - val_loss: 0.0271 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0130\n",
      "Epoch 51/500\n",
      " - 13s - loss: 0.0182 - dense_71_loss: 0.0091 - activation_8_loss: 0.0091 - val_loss: 0.0274 - val_dense_71_loss: 0.0132 - val_activation_8_loss: 0.0131\n",
      "Epoch 52/500\n",
      " - 14s - loss: 0.0182 - dense_71_loss: 0.0091 - activation_8_loss: 0.0091 - val_loss: 0.0272 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0131\n",
      "Epoch 53/500\n",
      " - 14s - loss: 0.0182 - dense_71_loss: 0.0091 - activation_8_loss: 0.0091 - val_loss: 0.0273 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0130\n",
      "Epoch 54/500\n",
      " - 14s - loss: 0.0182 - dense_71_loss: 0.0091 - activation_8_loss: 0.0091 - val_loss: 0.0270 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0130\n",
      "Epoch 55/500\n",
      " - 14s - loss: 0.0182 - dense_71_loss: 0.0091 - activation_8_loss: 0.0091 - val_loss: 0.0270 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0130\n",
      "Epoch 56/500\n",
      " - 14s - loss: 0.0181 - dense_71_loss: 0.0090 - activation_8_loss: 0.0091 - val_loss: 0.0273 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0130\n",
      "Epoch 57/500\n",
      " - 14s - loss: 0.0181 - dense_71_loss: 0.0090 - activation_8_loss: 0.0091 - val_loss: 0.0268 - val_dense_71_loss: 0.0128 - val_activation_8_loss: 0.0129\n",
      "Epoch 58/500\n",
      " - 14s - loss: 0.0181 - dense_71_loss: 0.0090 - activation_8_loss: 0.0091 - val_loss: 0.0271 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0130\n",
      "Epoch 59/500\n",
      " - 13s - loss: 0.0180 - dense_71_loss: 0.0090 - activation_8_loss: 0.0090 - val_loss: 0.0274 - val_dense_71_loss: 0.0132 - val_activation_8_loss: 0.0133\n",
      "Epoch 60/500\n",
      " - 14s - loss: 0.0180 - dense_71_loss: 0.0090 - activation_8_loss: 0.0090 - val_loss: 0.0268 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0130\n",
      "Epoch 61/500\n",
      " - 14s - loss: 0.0180 - dense_71_loss: 0.0090 - activation_8_loss: 0.0090 - val_loss: 0.0269 - val_dense_71_loss: 0.0128 - val_activation_8_loss: 0.0129\n",
      "Epoch 62/500\n",
      " - 14s - loss: 0.0180 - dense_71_loss: 0.0090 - activation_8_loss: 0.0090 - val_loss: 0.0270 - val_dense_71_loss: 0.0128 - val_activation_8_loss: 0.0129\n",
      "Epoch 63/500\n",
      " - 14s - loss: 0.0180 - dense_71_loss: 0.0090 - activation_8_loss: 0.0090 - val_loss: 0.0272 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0130\n",
      "Epoch 64/500\n",
      " - 15s - loss: 0.0179 - dense_71_loss: 0.0089 - activation_8_loss: 0.0090 - val_loss: 0.0271 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0129\n",
      "Epoch 65/500\n",
      " - 14s - loss: 0.0179 - dense_71_loss: 0.0089 - activation_8_loss: 0.0090 - val_loss: 0.0267 - val_dense_71_loss: 0.0127 - val_activation_8_loss: 0.0128\n",
      "Epoch 66/500\n",
      " - 14s - loss: 0.0179 - dense_71_loss: 0.0089 - activation_8_loss: 0.0090 - val_loss: 0.0270 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0130\n",
      "Epoch 67/500\n",
      " - 13s - loss: 0.0179 - dense_71_loss: 0.0089 - activation_8_loss: 0.0090 - val_loss: 0.0269 - val_dense_71_loss: 0.0128 - val_activation_8_loss: 0.0128\n",
      "Epoch 68/500\n",
      " - 13s - loss: 0.0179 - dense_71_loss: 0.0090 - activation_8_loss: 0.0090 - val_loss: 0.0269 - val_dense_71_loss: 0.0128 - val_activation_8_loss: 0.0129\n",
      "Epoch 69/500\n",
      " - 13s - loss: 0.0179 - dense_71_loss: 0.0089 - activation_8_loss: 0.0090 - val_loss: 0.0273 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0130\n",
      "Epoch 70/500\n",
      " - 13s - loss: 0.0179 - dense_71_loss: 0.0089 - activation_8_loss: 0.0090 - val_loss: 0.0276 - val_dense_71_loss: 0.0133 - val_activation_8_loss: 0.0133\n",
      "Epoch 71/500\n",
      " - 14s - loss: 0.0178 - dense_71_loss: 0.0089 - activation_8_loss: 0.0089 - val_loss: 0.0279 - val_dense_71_loss: 0.0134 - val_activation_8_loss: 0.0132\n",
      "Epoch 72/500\n",
      " - 14s - loss: 0.0179 - dense_71_loss: 0.0089 - activation_8_loss: 0.0090 - val_loss: 0.0270 - val_dense_71_loss: 0.0128 - val_activation_8_loss: 0.0128\n",
      "Epoch 73/500\n",
      " - 13s - loss: 0.0178 - dense_71_loss: 0.0089 - activation_8_loss: 0.0089 - val_loss: 0.0274 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0130\n",
      "Epoch 74/500\n",
      " - 13s - loss: 0.0178 - dense_71_loss: 0.0089 - activation_8_loss: 0.0089 - val_loss: 0.0271 - val_dense_71_loss: 0.0128 - val_activation_8_loss: 0.0130\n",
      "Epoch 75/500\n",
      " - 14s - loss: 0.0178 - dense_71_loss: 0.0089 - activation_8_loss: 0.0089 - val_loss: 0.0273 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0130\n",
      "Epoch 76/500\n",
      " - 14s - loss: 0.0178 - dense_71_loss: 0.0089 - activation_8_loss: 0.0089 - val_loss: 0.0268 - val_dense_71_loss: 0.0128 - val_activation_8_loss: 0.0128\n",
      "Epoch 77/500\n",
      " - 13s - loss: 0.0178 - dense_71_loss: 0.0089 - activation_8_loss: 0.0089 - val_loss: 0.0272 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0131\n",
      "Epoch 78/500\n",
      " - 13s - loss: 0.0178 - dense_71_loss: 0.0089 - activation_8_loss: 0.0089 - val_loss: 0.0272 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0131\n",
      "Epoch 79/500\n",
      " - 13s - loss: 0.0177 - dense_71_loss: 0.0088 - activation_8_loss: 0.0089 - val_loss: 0.0271 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0131\n",
      "Epoch 80/500\n",
      " - 13s - loss: 0.0178 - dense_71_loss: 0.0089 - activation_8_loss: 0.0089 - val_loss: 0.0270 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0130\n",
      "Epoch 81/500\n",
      " - 13s - loss: 0.0177 - dense_71_loss: 0.0088 - activation_8_loss: 0.0089 - val_loss: 0.0272 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0131\n",
      "Epoch 82/500\n",
      " - 13s - loss: 0.0177 - dense_71_loss: 0.0088 - activation_8_loss: 0.0089 - val_loss: 0.0271 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0130\n",
      "Epoch 83/500\n",
      " - 13s - loss: 0.0177 - dense_71_loss: 0.0088 - activation_8_loss: 0.0089 - val_loss: 0.0268 - val_dense_71_loss: 0.0128 - val_activation_8_loss: 0.0129\n",
      "Epoch 84/500\n",
      " - 14s - loss: 0.0177 - dense_71_loss: 0.0088 - activation_8_loss: 0.0089 - val_loss: 0.0274 - val_dense_71_loss: 0.0131 - val_activation_8_loss: 0.0131\n",
      "Epoch 85/500\n",
      " - 13s - loss: 0.0177 - dense_71_loss: 0.0088 - activation_8_loss: 0.0089 - val_loss: 0.0274 - val_dense_71_loss: 0.0130 - val_activation_8_loss: 0.0130\n",
      "Epoch 86/500\n",
      " - 14s - loss: 0.0176 - dense_71_loss: 0.0088 - activation_8_loss: 0.0089 - val_loss: 0.0270 - val_dense_71_loss: 0.0129 - val_activation_8_loss: 0.0129\n",
      " val fold: 6 0.013342368\n",
      " val fold: 6 0.013162894\n",
      "****************************************\n",
      "Train on 66696 samples, validate on 2817 samples\n",
      "Epoch 1/500\n",
      " - 21s - loss: 0.0326 - dense_80_loss: 0.0218 - activation_9_loss: 0.0108 - val_loss: 0.0298 - val_dense_80_loss: 0.0142 - val_activation_9_loss: 0.0139\n",
      "Epoch 2/500\n",
      " - 13s - loss: 0.0210 - dense_80_loss: 0.0106 - activation_9_loss: 0.0104 - val_loss: 0.0291 - val_dense_80_loss: 0.0137 - val_activation_9_loss: 0.0136\n",
      "Epoch 3/500\n",
      " - 13s - loss: 0.0205 - dense_80_loss: 0.0103 - activation_9_loss: 0.0102 - val_loss: 0.0294 - val_dense_80_loss: 0.0138 - val_activation_9_loss: 0.0139\n",
      "Epoch 4/500\n",
      " - 14s - loss: 0.0201 - dense_80_loss: 0.0101 - activation_9_loss: 0.0100 - val_loss: 0.0287 - val_dense_80_loss: 0.0135 - val_activation_9_loss: 0.0134\n",
      "Epoch 5/500\n",
      " - 14s - loss: 0.0199 - dense_80_loss: 0.0100 - activation_9_loss: 0.0100 - val_loss: 0.0283 - val_dense_80_loss: 0.0133 - val_activation_9_loss: 0.0133\n",
      "Epoch 6/500\n",
      " - 13s - loss: 0.0198 - dense_80_loss: 0.0099 - activation_9_loss: 0.0099 - val_loss: 0.0280 - val_dense_80_loss: 0.0132 - val_activation_9_loss: 0.0132\n",
      "Epoch 7/500\n",
      " - 13s - loss: 0.0197 - dense_80_loss: 0.0098 - activation_9_loss: 0.0098 - val_loss: 0.0278 - val_dense_80_loss: 0.0131 - val_activation_9_loss: 0.0133\n",
      "Epoch 8/500\n",
      " - 14s - loss: 0.0196 - dense_80_loss: 0.0098 - activation_9_loss: 0.0098 - val_loss: 0.0278 - val_dense_80_loss: 0.0131 - val_activation_9_loss: 0.0133\n",
      "Epoch 9/500\n",
      " - 13s - loss: 0.0195 - dense_80_loss: 0.0097 - activation_9_loss: 0.0097 - val_loss: 0.0277 - val_dense_80_loss: 0.0131 - val_activation_9_loss: 0.0133\n",
      "Epoch 10/500\n",
      " - 13s - loss: 0.0194 - dense_80_loss: 0.0097 - activation_9_loss: 0.0097 - val_loss: 0.0276 - val_dense_80_loss: 0.0131 - val_activation_9_loss: 0.0132\n",
      "Epoch 11/500\n",
      " - 14s - loss: 0.0193 - dense_80_loss: 0.0097 - activation_9_loss: 0.0097 - val_loss: 0.0275 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 12/500\n",
      " - 13s - loss: 0.0193 - dense_80_loss: 0.0096 - activation_9_loss: 0.0097 - val_loss: 0.0279 - val_dense_80_loss: 0.0131 - val_activation_9_loss: 0.0131\n",
      "Epoch 13/500\n",
      " - 14s - loss: 0.0193 - dense_80_loss: 0.0096 - activation_9_loss: 0.0096 - val_loss: 0.0276 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0130\n",
      "Epoch 14/500\n",
      " - 14s - loss: 0.0192 - dense_80_loss: 0.0096 - activation_9_loss: 0.0096 - val_loss: 0.0277 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0130\n",
      "Epoch 15/500\n",
      " - 13s - loss: 0.0192 - dense_80_loss: 0.0096 - activation_9_loss: 0.0096 - val_loss: 0.0276 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0131\n",
      "Epoch 16/500\n",
      " - 13s - loss: 0.0191 - dense_80_loss: 0.0095 - activation_9_loss: 0.0096 - val_loss: 0.0274 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0131\n",
      "Epoch 17/500\n",
      " - 14s - loss: 0.0191 - dense_80_loss: 0.0095 - activation_9_loss: 0.0096 - val_loss: 0.0273 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0130\n",
      "Epoch 18/500\n",
      " - 13s - loss: 0.0190 - dense_80_loss: 0.0095 - activation_9_loss: 0.0095 - val_loss: 0.0274 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0131\n",
      "Epoch 19/500\n",
      " - 14s - loss: 0.0190 - dense_80_loss: 0.0095 - activation_9_loss: 0.0095 - val_loss: 0.0277 - val_dense_80_loss: 0.0131 - val_activation_9_loss: 0.0132\n",
      "Epoch 20/500\n",
      " - 13s - loss: 0.0190 - dense_80_loss: 0.0095 - activation_9_loss: 0.0095 - val_loss: 0.0273 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 21/500\n",
      " - 14s - loss: 0.0190 - dense_80_loss: 0.0095 - activation_9_loss: 0.0095 - val_loss: 0.0275 - val_dense_80_loss: 0.0128 - val_activation_9_loss: 0.0128\n",
      "Epoch 22/500\n",
      " - 14s - loss: 0.0189 - dense_80_loss: 0.0095 - activation_9_loss: 0.0095 - val_loss: 0.0275 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0129\n",
      "Epoch 23/500\n",
      " - 13s - loss: 0.0189 - dense_80_loss: 0.0094 - activation_9_loss: 0.0095 - val_loss: 0.0274 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 24/500\n",
      " - 14s - loss: 0.0189 - dense_80_loss: 0.0094 - activation_9_loss: 0.0094 - val_loss: 0.0272 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0130\n",
      "Epoch 25/500\n",
      " - 13s - loss: 0.0188 - dense_80_loss: 0.0094 - activation_9_loss: 0.0094 - val_loss: 0.0274 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 26/500\n",
      " - 13s - loss: 0.0188 - dense_80_loss: 0.0094 - activation_9_loss: 0.0094 - val_loss: 0.0277 - val_dense_80_loss: 0.0131 - val_activation_9_loss: 0.0132\n",
      "Epoch 27/500\n",
      " - 13s - loss: 0.0188 - dense_80_loss: 0.0094 - activation_9_loss: 0.0094 - val_loss: 0.0274 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 28/500\n",
      " - 14s - loss: 0.0188 - dense_80_loss: 0.0094 - activation_9_loss: 0.0094 - val_loss: 0.0273 - val_dense_80_loss: 0.0128 - val_activation_9_loss: 0.0129\n",
      "Epoch 29/500\n",
      " - 14s - loss: 0.0187 - dense_80_loss: 0.0093 - activation_9_loss: 0.0094 - val_loss: 0.0272 - val_dense_80_loss: 0.0128 - val_activation_9_loss: 0.0129\n",
      "Epoch 30/500\n",
      " - 13s - loss: 0.0187 - dense_80_loss: 0.0094 - activation_9_loss: 0.0094 - val_loss: 0.0276 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0130\n",
      "Epoch 31/500\n",
      " - 13s - loss: 0.0187 - dense_80_loss: 0.0093 - activation_9_loss: 0.0094 - val_loss: 0.0275 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 32/500\n",
      " - 14s - loss: 0.0187 - dense_80_loss: 0.0093 - activation_9_loss: 0.0093 - val_loss: 0.0273 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0130\n",
      "Epoch 33/500\n",
      " - 13s - loss: 0.0186 - dense_80_loss: 0.0093 - activation_9_loss: 0.0093 - val_loss: 0.0275 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0130\n",
      "Epoch 34/500\n",
      " - 14s - loss: 0.0187 - dense_80_loss: 0.0093 - activation_9_loss: 0.0094 - val_loss: 0.0274 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0130\n",
      "Epoch 35/500\n",
      " - 13s - loss: 0.0186 - dense_80_loss: 0.0093 - activation_9_loss: 0.0093 - val_loss: 0.0274 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0130\n",
      "Epoch 36/500\n",
      " - 13s - loss: 0.0186 - dense_80_loss: 0.0093 - activation_9_loss: 0.0093 - val_loss: 0.0272 - val_dense_80_loss: 0.0128 - val_activation_9_loss: 0.0127\n",
      "Epoch 37/500\n",
      " - 14s - loss: 0.0186 - dense_80_loss: 0.0093 - activation_9_loss: 0.0093 - val_loss: 0.0274 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0130\n",
      "Epoch 38/500\n",
      " - 13s - loss: 0.0185 - dense_80_loss: 0.0093 - activation_9_loss: 0.0093 - val_loss: 0.0275 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0131\n",
      "Epoch 39/500\n",
      " - 13s - loss: 0.0185 - dense_80_loss: 0.0092 - activation_9_loss: 0.0093 - val_loss: 0.0273 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 40/500\n",
      " - 13s - loss: 0.0185 - dense_80_loss: 0.0092 - activation_9_loss: 0.0092 - val_loss: 0.0272 - val_dense_80_loss: 0.0128 - val_activation_9_loss: 0.0128\n",
      "Epoch 41/500\n",
      " - 14s - loss: 0.0184 - dense_80_loss: 0.0092 - activation_9_loss: 0.0092 - val_loss: 0.0276 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0131\n",
      "Epoch 42/500\n",
      " - 14s - loss: 0.0184 - dense_80_loss: 0.0092 - activation_9_loss: 0.0092 - val_loss: 0.0273 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 43/500\n",
      " - 13s - loss: 0.0184 - dense_80_loss: 0.0092 - activation_9_loss: 0.0092 - val_loss: 0.0273 - val_dense_80_loss: 0.0128 - val_activation_9_loss: 0.0129\n",
      "Epoch 44/500\n",
      " - 14s - loss: 0.0184 - dense_80_loss: 0.0092 - activation_9_loss: 0.0092 - val_loss: 0.0273 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0130\n",
      "Epoch 45/500\n",
      " - 14s - loss: 0.0184 - dense_80_loss: 0.0092 - activation_9_loss: 0.0092 - val_loss: 0.0272 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 46/500\n",
      " - 14s - loss: 0.0184 - dense_80_loss: 0.0092 - activation_9_loss: 0.0092 - val_loss: 0.0274 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0130\n",
      "Epoch 47/500\n",
      " - 14s - loss: 0.0183 - dense_80_loss: 0.0091 - activation_9_loss: 0.0092 - val_loss: 0.0272 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 48/500\n",
      " - 14s - loss: 0.0183 - dense_80_loss: 0.0091 - activation_9_loss: 0.0092 - val_loss: 0.0274 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0130\n",
      "Epoch 49/500\n",
      " - 13s - loss: 0.0183 - dense_80_loss: 0.0091 - activation_9_loss: 0.0092 - val_loss: 0.0273 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0131\n",
      "Epoch 50/500\n",
      " - 14s - loss: 0.0182 - dense_80_loss: 0.0091 - activation_9_loss: 0.0091 - val_loss: 0.0271 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0128\n",
      "Epoch 51/500\n",
      " - 14s - loss: 0.0182 - dense_80_loss: 0.0091 - activation_9_loss: 0.0091 - val_loss: 0.0272 - val_dense_80_loss: 0.0128 - val_activation_9_loss: 0.0128\n",
      "Epoch 52/500\n",
      " - 14s - loss: 0.0182 - dense_80_loss: 0.0091 - activation_9_loss: 0.0091 - val_loss: 0.0275 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0130\n",
      "Epoch 53/500\n",
      " - 13s - loss: 0.0182 - dense_80_loss: 0.0091 - activation_9_loss: 0.0091 - val_loss: 0.0276 - val_dense_80_loss: 0.0131 - val_activation_9_loss: 0.0131\n",
      "Epoch 54/500\n",
      " - 14s - loss: 0.0182 - dense_80_loss: 0.0091 - activation_9_loss: 0.0091 - val_loss: 0.0275 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0129\n",
      "Epoch 55/500\n",
      " - 14s - loss: 0.0182 - dense_80_loss: 0.0091 - activation_9_loss: 0.0091 - val_loss: 0.0272 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 56/500\n",
      " - 14s - loss: 0.0181 - dense_80_loss: 0.0090 - activation_9_loss: 0.0091 - val_loss: 0.0272 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0130\n",
      "Epoch 57/500\n",
      " - 14s - loss: 0.0181 - dense_80_loss: 0.0090 - activation_9_loss: 0.0091 - val_loss: 0.0274 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 58/500\n",
      " - 13s - loss: 0.0181 - dense_80_loss: 0.0091 - activation_9_loss: 0.0091 - val_loss: 0.0274 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0130\n",
      "Epoch 59/500\n",
      " - 13s - loss: 0.0181 - dense_80_loss: 0.0090 - activation_9_loss: 0.0091 - val_loss: 0.0273 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 60/500\n",
      " - 13s - loss: 0.0181 - dense_80_loss: 0.0090 - activation_9_loss: 0.0091 - val_loss: 0.0275 - val_dense_80_loss: 0.0131 - val_activation_9_loss: 0.0130\n",
      "Epoch 61/500\n",
      " - 13s - loss: 0.0181 - dense_80_loss: 0.0090 - activation_9_loss: 0.0091 - val_loss: 0.0274 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0129\n",
      "Epoch 62/500\n",
      " - 13s - loss: 0.0180 - dense_80_loss: 0.0090 - activation_9_loss: 0.0090 - val_loss: 0.0273 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 63/500\n",
      " - 14s - loss: 0.0180 - dense_80_loss: 0.0090 - activation_9_loss: 0.0090 - val_loss: 0.0274 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 64/500\n",
      " - 13s - loss: 0.0180 - dense_80_loss: 0.0089 - activation_9_loss: 0.0090 - val_loss: 0.0275 - val_dense_80_loss: 0.0130 - val_activation_9_loss: 0.0130\n",
      "Epoch 65/500\n",
      " - 13s - loss: 0.0180 - dense_80_loss: 0.0090 - activation_9_loss: 0.0090 - val_loss: 0.0273 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0129\n",
      "Epoch 66/500\n",
      " - 13s - loss: 0.0180 - dense_80_loss: 0.0090 - activation_9_loss: 0.0090 - val_loss: 0.0275 - val_dense_80_loss: 0.0131 - val_activation_9_loss: 0.0130\n",
      "Epoch 67/500\n",
      " - 14s - loss: 0.0180 - dense_80_loss: 0.0090 - activation_9_loss: 0.0090 - val_loss: 0.0272 - val_dense_80_loss: 0.0129 - val_activation_9_loss: 0.0128\n",
      "Epoch 68/500\n",
      " - 14s - loss: 0.0179 - dense_80_loss: 0.0089 - activation_9_loss: 0.0090 - val_loss: 0.0280 - val_dense_80_loss: 0.0133 - val_activation_9_loss: 0.0134\n",
      "Epoch 69/500\n",
      " - 13s - loss: 0.0180 - dense_80_loss: 0.0090 - activation_9_loss: 0.0090 - val_loss: 0.0275 - val_dense_80_loss: 0.0131 - val_activation_9_loss: 0.0131\n",
      "Epoch 70/500\n",
      " - 13s - loss: 0.0179 - dense_80_loss: 0.0089 - activation_9_loss: 0.0090 - val_loss: 0.0271 - val_dense_80_loss: 0.0128 - val_activation_9_loss: 0.0129\n",
      "Epoch 71/500\n",
      " - 14s - loss: 0.0179 - dense_80_loss: 0.0089 - activation_9_loss: 0.0090 - val_loss: 0.0271 - val_dense_80_loss: 0.0128 - val_activation_9_loss: 0.0128\n",
      " val fold: 7 0.013516236\n",
      " val fold: 7 0.013369378\n",
      "****************************************\n",
      "Train on 66696 samples, validate on 2817 samples\n",
      "Epoch 1/500\n",
      " - 21s - loss: 0.0341 - dense_89_loss: 0.0232 - activation_10_loss: 0.0108 - val_loss: 0.0319 - val_dense_89_loss: 0.0155 - val_activation_10_loss: 0.0153\n",
      "Epoch 2/500\n",
      " - 13s - loss: 0.0212 - dense_89_loss: 0.0107 - activation_10_loss: 0.0105 - val_loss: 0.0309 - val_dense_89_loss: 0.0147 - val_activation_10_loss: 0.0145\n",
      "Epoch 3/500\n",
      " - 13s - loss: 0.0205 - dense_89_loss: 0.0103 - activation_10_loss: 0.0102 - val_loss: 0.0302 - val_dense_89_loss: 0.0141 - val_activation_10_loss: 0.0141\n",
      "Epoch 4/500\n",
      " - 14s - loss: 0.0201 - dense_89_loss: 0.0101 - activation_10_loss: 0.0101 - val_loss: 0.0300 - val_dense_89_loss: 0.0140 - val_activation_10_loss: 0.0140\n",
      "Epoch 5/500\n",
      " - 13s - loss: 0.0199 - dense_89_loss: 0.0099 - activation_10_loss: 0.0099 - val_loss: 0.0296 - val_dense_89_loss: 0.0138 - val_activation_10_loss: 0.0138\n",
      "Epoch 6/500\n",
      " - 14s - loss: 0.0197 - dense_89_loss: 0.0098 - activation_10_loss: 0.0098 - val_loss: 0.0293 - val_dense_89_loss: 0.0138 - val_activation_10_loss: 0.0138\n",
      "Epoch 7/500\n",
      " - 13s - loss: 0.0196 - dense_89_loss: 0.0098 - activation_10_loss: 0.0098 - val_loss: 0.0293 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0137\n",
      "Epoch 8/500\n",
      " - 14s - loss: 0.0195 - dense_89_loss: 0.0097 - activation_10_loss: 0.0097 - val_loss: 0.0291 - val_dense_89_loss: 0.0138 - val_activation_10_loss: 0.0138\n",
      "Epoch 9/500\n",
      " - 13s - loss: 0.0194 - dense_89_loss: 0.0097 - activation_10_loss: 0.0097 - val_loss: 0.0297 - val_dense_89_loss: 0.0142 - val_activation_10_loss: 0.0143\n",
      "Epoch 10/500\n",
      " - 14s - loss: 0.0193 - dense_89_loss: 0.0097 - activation_10_loss: 0.0097 - val_loss: 0.0292 - val_dense_89_loss: 0.0139 - val_activation_10_loss: 0.0139\n",
      "Epoch 11/500\n",
      " - 14s - loss: 0.0193 - dense_89_loss: 0.0096 - activation_10_loss: 0.0096 - val_loss: 0.0291 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0137\n",
      "Epoch 12/500\n",
      " - 14s - loss: 0.0192 - dense_89_loss: 0.0096 - activation_10_loss: 0.0096 - val_loss: 0.0290 - val_dense_89_loss: 0.0136 - val_activation_10_loss: 0.0135\n",
      "Epoch 13/500\n",
      " - 14s - loss: 0.0191 - dense_89_loss: 0.0096 - activation_10_loss: 0.0096 - val_loss: 0.0290 - val_dense_89_loss: 0.0136 - val_activation_10_loss: 0.0136\n",
      "Epoch 14/500\n",
      " - 13s - loss: 0.0191 - dense_89_loss: 0.0095 - activation_10_loss: 0.0096 - val_loss: 0.0291 - val_dense_89_loss: 0.0139 - val_activation_10_loss: 0.0139\n",
      "Epoch 15/500\n",
      " - 13s - loss: 0.0191 - dense_89_loss: 0.0095 - activation_10_loss: 0.0095 - val_loss: 0.0289 - val_dense_89_loss: 0.0136 - val_activation_10_loss: 0.0136\n",
      "Epoch 16/500\n",
      " - 13s - loss: 0.0190 - dense_89_loss: 0.0095 - activation_10_loss: 0.0095 - val_loss: 0.0286 - val_dense_89_loss: 0.0134 - val_activation_10_loss: 0.0134\n",
      "Epoch 17/500\n",
      " - 13s - loss: 0.0190 - dense_89_loss: 0.0095 - activation_10_loss: 0.0095 - val_loss: 0.0288 - val_dense_89_loss: 0.0136 - val_activation_10_loss: 0.0136\n",
      "Epoch 18/500\n",
      " - 13s - loss: 0.0190 - dense_89_loss: 0.0095 - activation_10_loss: 0.0095 - val_loss: 0.0289 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0136\n",
      "Epoch 19/500\n",
      " - 13s - loss: 0.0189 - dense_89_loss: 0.0094 - activation_10_loss: 0.0095 - val_loss: 0.0289 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0136\n",
      "Epoch 20/500\n",
      " - 14s - loss: 0.0189 - dense_89_loss: 0.0094 - activation_10_loss: 0.0095 - val_loss: 0.0287 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0136\n",
      "Epoch 21/500\n",
      " - 13s - loss: 0.0188 - dense_89_loss: 0.0094 - activation_10_loss: 0.0094 - val_loss: 0.0288 - val_dense_89_loss: 0.0135 - val_activation_10_loss: 0.0134\n",
      "Epoch 22/500\n",
      " - 13s - loss: 0.0188 - dense_89_loss: 0.0094 - activation_10_loss: 0.0094 - val_loss: 0.0290 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0137\n",
      "Epoch 23/500\n",
      " - 14s - loss: 0.0188 - dense_89_loss: 0.0094 - activation_10_loss: 0.0094 - val_loss: 0.0290 - val_dense_89_loss: 0.0136 - val_activation_10_loss: 0.0136\n",
      "Epoch 24/500\n",
      " - 13s - loss: 0.0188 - dense_89_loss: 0.0094 - activation_10_loss: 0.0094 - val_loss: 0.0288 - val_dense_89_loss: 0.0136 - val_activation_10_loss: 0.0134\n",
      "Epoch 25/500\n",
      " - 13s - loss: 0.0187 - dense_89_loss: 0.0094 - activation_10_loss: 0.0094 - val_loss: 0.0286 - val_dense_89_loss: 0.0136 - val_activation_10_loss: 0.0136\n",
      "Epoch 26/500\n",
      " - 14s - loss: 0.0187 - dense_89_loss: 0.0094 - activation_10_loss: 0.0094 - val_loss: 0.0287 - val_dense_89_loss: 0.0136 - val_activation_10_loss: 0.0136\n",
      "Epoch 27/500\n",
      " - 13s - loss: 0.0187 - dense_89_loss: 0.0093 - activation_10_loss: 0.0094 - val_loss: 0.0289 - val_dense_89_loss: 0.0138 - val_activation_10_loss: 0.0138\n",
      "Epoch 28/500\n",
      " - 13s - loss: 0.0187 - dense_89_loss: 0.0093 - activation_10_loss: 0.0093 - val_loss: 0.0287 - val_dense_89_loss: 0.0136 - val_activation_10_loss: 0.0136\n",
      "Epoch 29/500\n",
      " - 13s - loss: 0.0186 - dense_89_loss: 0.0093 - activation_10_loss: 0.0093 - val_loss: 0.0286 - val_dense_89_loss: 0.0134 - val_activation_10_loss: 0.0134\n",
      "Epoch 30/500\n",
      " - 14s - loss: 0.0186 - dense_89_loss: 0.0093 - activation_10_loss: 0.0093 - val_loss: 0.0290 - val_dense_89_loss: 0.0138 - val_activation_10_loss: 0.0137\n",
      "Epoch 31/500\n",
      " - 14s - loss: 0.0186 - dense_89_loss: 0.0093 - activation_10_loss: 0.0093 - val_loss: 0.0287 - val_dense_89_loss: 0.0136 - val_activation_10_loss: 0.0136\n",
      "Epoch 32/500\n",
      " - 13s - loss: 0.0186 - dense_89_loss: 0.0093 - activation_10_loss: 0.0093 - val_loss: 0.0285 - val_dense_89_loss: 0.0134 - val_activation_10_loss: 0.0134\n",
      "Epoch 33/500\n",
      " - 14s - loss: 0.0186 - dense_89_loss: 0.0093 - activation_10_loss: 0.0093 - val_loss: 0.0287 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0137\n",
      "Epoch 34/500\n",
      " - 14s - loss: 0.0185 - dense_89_loss: 0.0092 - activation_10_loss: 0.0093 - val_loss: 0.0285 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0138\n",
      "Epoch 35/500\n",
      " - 13s - loss: 0.0185 - dense_89_loss: 0.0092 - activation_10_loss: 0.0093 - val_loss: 0.0289 - val_dense_89_loss: 0.0139 - val_activation_10_loss: 0.0139\n",
      "Epoch 36/500\n",
      " - 13s - loss: 0.0185 - dense_89_loss: 0.0092 - activation_10_loss: 0.0093 - val_loss: 0.0288 - val_dense_89_loss: 0.0138 - val_activation_10_loss: 0.0138\n",
      "Epoch 37/500\n",
      " - 14s - loss: 0.0185 - dense_89_loss: 0.0092 - activation_10_loss: 0.0093 - val_loss: 0.0286 - val_dense_89_loss: 0.0135 - val_activation_10_loss: 0.0135\n",
      "Epoch 38/500\n",
      " - 14s - loss: 0.0184 - dense_89_loss: 0.0092 - activation_10_loss: 0.0092 - val_loss: 0.0288 - val_dense_89_loss: 0.0138 - val_activation_10_loss: 0.0137\n",
      "Epoch 39/500\n",
      " - 13s - loss: 0.0185 - dense_89_loss: 0.0092 - activation_10_loss: 0.0092 - val_loss: 0.0287 - val_dense_89_loss: 0.0138 - val_activation_10_loss: 0.0139\n",
      "Epoch 40/500\n",
      " - 13s - loss: 0.0184 - dense_89_loss: 0.0092 - activation_10_loss: 0.0092 - val_loss: 0.0288 - val_dense_89_loss: 0.0139 - val_activation_10_loss: 0.0139\n",
      "Epoch 41/500\n",
      " - 14s - loss: 0.0184 - dense_89_loss: 0.0092 - activation_10_loss: 0.0092 - val_loss: 0.0291 - val_dense_89_loss: 0.0141 - val_activation_10_loss: 0.0142\n",
      "Epoch 42/500\n",
      " - 13s - loss: 0.0184 - dense_89_loss: 0.0092 - activation_10_loss: 0.0092 - val_loss: 0.0288 - val_dense_89_loss: 0.0138 - val_activation_10_loss: 0.0138\n",
      "Epoch 43/500\n",
      " - 13s - loss: 0.0183 - dense_89_loss: 0.0091 - activation_10_loss: 0.0092 - val_loss: 0.0285 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0137\n",
      "Epoch 44/500\n",
      " - 13s - loss: 0.0183 - dense_89_loss: 0.0091 - activation_10_loss: 0.0092 - val_loss: 0.0287 - val_dense_89_loss: 0.0141 - val_activation_10_loss: 0.0141\n",
      "Epoch 45/500\n",
      " - 13s - loss: 0.0183 - dense_89_loss: 0.0091 - activation_10_loss: 0.0092 - val_loss: 0.0287 - val_dense_89_loss: 0.0139 - val_activation_10_loss: 0.0139\n",
      "Epoch 46/500\n",
      " - 14s - loss: 0.0183 - dense_89_loss: 0.0091 - activation_10_loss: 0.0092 - val_loss: 0.0286 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0138\n",
      "Epoch 47/500\n",
      " - 13s - loss: 0.0183 - dense_89_loss: 0.0091 - activation_10_loss: 0.0091 - val_loss: 0.0287 - val_dense_89_loss: 0.0139 - val_activation_10_loss: 0.0139\n",
      "Epoch 48/500\n",
      " - 14s - loss: 0.0183 - dense_89_loss: 0.0091 - activation_10_loss: 0.0092 - val_loss: 0.0285 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0137\n",
      "Epoch 49/500\n",
      " - 14s - loss: 0.0182 - dense_89_loss: 0.0091 - activation_10_loss: 0.0091 - val_loss: 0.0290 - val_dense_89_loss: 0.0140 - val_activation_10_loss: 0.0141\n",
      "Epoch 50/500\n",
      " - 13s - loss: 0.0183 - dense_89_loss: 0.0091 - activation_10_loss: 0.0092 - val_loss: 0.0286 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0136\n",
      "Epoch 51/500\n",
      " - 13s - loss: 0.0182 - dense_89_loss: 0.0091 - activation_10_loss: 0.0091 - val_loss: 0.0287 - val_dense_89_loss: 0.0138 - val_activation_10_loss: 0.0139\n",
      "Epoch 52/500\n",
      " - 13s - loss: 0.0182 - dense_89_loss: 0.0091 - activation_10_loss: 0.0091 - val_loss: 0.0287 - val_dense_89_loss: 0.0138 - val_activation_10_loss: 0.0139\n",
      "Epoch 53/500\n",
      " - 13s - loss: 0.0182 - dense_89_loss: 0.0091 - activation_10_loss: 0.0091 - val_loss: 0.0288 - val_dense_89_loss: 0.0137 - val_activation_10_loss: 0.0138\n",
      " val fold: 8 0.014220391\n",
      " val fold: 8 0.01409386\n",
      "****************************************\n",
      "Train on 66696 samples, validate on 2817 samples\n",
      "Epoch 1/500\n",
      " - 22s - loss: 0.0330 - dense_98_loss: 0.0221 - activation_11_loss: 0.0109 - val_loss: 0.0282 - val_dense_98_loss: 0.0134 - val_activation_11_loss: 0.0132\n",
      "Epoch 2/500\n",
      " - 13s - loss: 0.0212 - dense_98_loss: 0.0107 - activation_11_loss: 0.0105 - val_loss: 0.0275 - val_dense_98_loss: 0.0130 - val_activation_11_loss: 0.0130\n",
      "Epoch 3/500\n",
      " - 13s - loss: 0.0206 - dense_98_loss: 0.0103 - activation_11_loss: 0.0103 - val_loss: 0.0270 - val_dense_98_loss: 0.0127 - val_activation_11_loss: 0.0127\n",
      "Epoch 4/500\n",
      " - 13s - loss: 0.0203 - dense_98_loss: 0.0102 - activation_11_loss: 0.0101 - val_loss: 0.0271 - val_dense_98_loss: 0.0129 - val_activation_11_loss: 0.0128\n",
      "Epoch 5/500\n",
      " - 14s - loss: 0.0200 - dense_98_loss: 0.0100 - activation_11_loss: 0.0100 - val_loss: 0.0268 - val_dense_98_loss: 0.0126 - val_activation_11_loss: 0.0126\n",
      "Epoch 6/500\n",
      " - 13s - loss: 0.0199 - dense_98_loss: 0.0100 - activation_11_loss: 0.0100 - val_loss: 0.0265 - val_dense_98_loss: 0.0125 - val_activation_11_loss: 0.0125\n",
      "Epoch 7/500\n",
      " - 13s - loss: 0.0198 - dense_98_loss: 0.0099 - activation_11_loss: 0.0099 - val_loss: 0.0263 - val_dense_98_loss: 0.0125 - val_activation_11_loss: 0.0125\n",
      "Epoch 8/500\n",
      " - 14s - loss: 0.0197 - dense_98_loss: 0.0098 - activation_11_loss: 0.0098 - val_loss: 0.0264 - val_dense_98_loss: 0.0124 - val_activation_11_loss: 0.0124\n",
      "Epoch 9/500\n",
      " - 14s - loss: 0.0196 - dense_98_loss: 0.0098 - activation_11_loss: 0.0098 - val_loss: 0.0263 - val_dense_98_loss: 0.0124 - val_activation_11_loss: 0.0124\n",
      "Epoch 10/500\n",
      " - 13s - loss: 0.0195 - dense_98_loss: 0.0098 - activation_11_loss: 0.0098 - val_loss: 0.0262 - val_dense_98_loss: 0.0123 - val_activation_11_loss: 0.0123\n",
      "Epoch 11/500\n",
      " - 14s - loss: 0.0194 - dense_98_loss: 0.0097 - activation_11_loss: 0.0097 - val_loss: 0.0263 - val_dense_98_loss: 0.0123 - val_activation_11_loss: 0.0124\n",
      "Epoch 12/500\n",
      " - 14s - loss: 0.0193 - dense_98_loss: 0.0097 - activation_11_loss: 0.0097 - val_loss: 0.0268 - val_dense_98_loss: 0.0126 - val_activation_11_loss: 0.0126\n",
      "Epoch 13/500\n",
      " - 14s - loss: 0.0194 - dense_98_loss: 0.0097 - activation_11_loss: 0.0097 - val_loss: 0.0264 - val_dense_98_loss: 0.0125 - val_activation_11_loss: 0.0125\n",
      "Epoch 14/500\n",
      " - 13s - loss: 0.0193 - dense_98_loss: 0.0096 - activation_11_loss: 0.0097 - val_loss: 0.0261 - val_dense_98_loss: 0.0123 - val_activation_11_loss: 0.0123\n",
      "Epoch 15/500\n",
      " - 13s - loss: 0.0193 - dense_98_loss: 0.0096 - activation_11_loss: 0.0096 - val_loss: 0.0261 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0123\n",
      "Epoch 16/500\n",
      " - 14s - loss: 0.0192 - dense_98_loss: 0.0096 - activation_11_loss: 0.0096 - val_loss: 0.0259 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0123\n",
      "Epoch 17/500\n",
      " - 13s - loss: 0.0192 - dense_98_loss: 0.0096 - activation_11_loss: 0.0096 - val_loss: 0.0259 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0123\n",
      "Epoch 18/500\n",
      " - 13s - loss: 0.0191 - dense_98_loss: 0.0096 - activation_11_loss: 0.0096 - val_loss: 0.0260 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0122\n",
      "Epoch 19/500\n",
      " - 14s - loss: 0.0191 - dense_98_loss: 0.0095 - activation_11_loss: 0.0096 - val_loss: 0.0259 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0122\n",
      "Epoch 20/500\n",
      " - 13s - loss: 0.0191 - dense_98_loss: 0.0095 - activation_11_loss: 0.0096 - val_loss: 0.0258 - val_dense_98_loss: 0.0121 - val_activation_11_loss: 0.0121\n",
      "Epoch 21/500\n",
      " - 14s - loss: 0.0191 - dense_98_loss: 0.0095 - activation_11_loss: 0.0095 - val_loss: 0.0259 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0122\n",
      "Epoch 22/500\n",
      " - 14s - loss: 0.0190 - dense_98_loss: 0.0095 - activation_11_loss: 0.0095 - val_loss: 0.0264 - val_dense_98_loss: 0.0124 - val_activation_11_loss: 0.0125\n",
      "Epoch 23/500\n",
      " - 13s - loss: 0.0190 - dense_98_loss: 0.0095 - activation_11_loss: 0.0095 - val_loss: 0.0260 - val_dense_98_loss: 0.0121 - val_activation_11_loss: 0.0122\n",
      "Epoch 24/500\n",
      " - 13s - loss: 0.0189 - dense_98_loss: 0.0095 - activation_11_loss: 0.0095 - val_loss: 0.0263 - val_dense_98_loss: 0.0123 - val_activation_11_loss: 0.0124\n",
      "Epoch 25/500\n",
      " - 14s - loss: 0.0190 - dense_98_loss: 0.0095 - activation_11_loss: 0.0095 - val_loss: 0.0258 - val_dense_98_loss: 0.0121 - val_activation_11_loss: 0.0122\n",
      "Epoch 26/500\n",
      " - 14s - loss: 0.0189 - dense_98_loss: 0.0094 - activation_11_loss: 0.0095 - val_loss: 0.0261 - val_dense_98_loss: 0.0123 - val_activation_11_loss: 0.0123\n",
      "Epoch 27/500\n",
      " - 14s - loss: 0.0189 - dense_98_loss: 0.0094 - activation_11_loss: 0.0094 - val_loss: 0.0259 - val_dense_98_loss: 0.0121 - val_activation_11_loss: 0.0122\n",
      "Epoch 28/500\n",
      " - 13s - loss: 0.0188 - dense_98_loss: 0.0094 - activation_11_loss: 0.0094 - val_loss: 0.0259 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0123\n",
      "Epoch 29/500\n",
      " - 14s - loss: 0.0188 - dense_98_loss: 0.0094 - activation_11_loss: 0.0094 - val_loss: 0.0260 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0122\n",
      "Epoch 30/500\n",
      " - 14s - loss: 0.0188 - dense_98_loss: 0.0094 - activation_11_loss: 0.0094 - val_loss: 0.0258 - val_dense_98_loss: 0.0121 - val_activation_11_loss: 0.0122\n",
      "Epoch 31/500\n",
      " - 14s - loss: 0.0187 - dense_98_loss: 0.0093 - activation_11_loss: 0.0094 - val_loss: 0.0263 - val_dense_98_loss: 0.0124 - val_activation_11_loss: 0.0124\n",
      "Epoch 32/500\n",
      " - 14s - loss: 0.0187 - dense_98_loss: 0.0093 - activation_11_loss: 0.0094 - val_loss: 0.0259 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0123\n",
      "Epoch 33/500\n",
      " - 14s - loss: 0.0187 - dense_98_loss: 0.0093 - activation_11_loss: 0.0094 - val_loss: 0.0257 - val_dense_98_loss: 0.0121 - val_activation_11_loss: 0.0122\n",
      "Epoch 34/500\n",
      " - 14s - loss: 0.0186 - dense_98_loss: 0.0093 - activation_11_loss: 0.0093 - val_loss: 0.0259 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0123\n",
      "Epoch 35/500\n",
      " - 15s - loss: 0.0186 - dense_98_loss: 0.0093 - activation_11_loss: 0.0093 - val_loss: 0.0261 - val_dense_98_loss: 0.0123 - val_activation_11_loss: 0.0124\n",
      "Epoch 36/500\n",
      " - 15s - loss: 0.0186 - dense_98_loss: 0.0093 - activation_11_loss: 0.0093 - val_loss: 0.0259 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0123\n",
      "Epoch 37/500\n",
      " - 15s - loss: 0.0186 - dense_98_loss: 0.0093 - activation_11_loss: 0.0093 - val_loss: 0.0260 - val_dense_98_loss: 0.0123 - val_activation_11_loss: 0.0123\n",
      "Epoch 38/500\n",
      " - 15s - loss: 0.0186 - dense_98_loss: 0.0093 - activation_11_loss: 0.0093 - val_loss: 0.0257 - val_dense_98_loss: 0.0120 - val_activation_11_loss: 0.0121\n",
      "Epoch 39/500\n",
      " - 14s - loss: 0.0186 - dense_98_loss: 0.0093 - activation_11_loss: 0.0093 - val_loss: 0.0255 - val_dense_98_loss: 0.0120 - val_activation_11_loss: 0.0121\n",
      "Epoch 40/500\n",
      " - 14s - loss: 0.0185 - dense_98_loss: 0.0092 - activation_11_loss: 0.0093 - val_loss: 0.0258 - val_dense_98_loss: 0.0121 - val_activation_11_loss: 0.0122\n",
      "Epoch 41/500\n",
      " - 14s - loss: 0.0185 - dense_98_loss: 0.0093 - activation_11_loss: 0.0093 - val_loss: 0.0260 - val_dense_98_loss: 0.0123 - val_activation_11_loss: 0.0123\n",
      "Epoch 42/500\n",
      " - 14s - loss: 0.0185 - dense_98_loss: 0.0092 - activation_11_loss: 0.0093 - val_loss: 0.0259 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0123\n",
      "Epoch 43/500\n",
      " - 14s - loss: 0.0184 - dense_98_loss: 0.0092 - activation_11_loss: 0.0092 - val_loss: 0.0262 - val_dense_98_loss: 0.0125 - val_activation_11_loss: 0.0125\n",
      "Epoch 44/500\n",
      " - 13s - loss: 0.0185 - dense_98_loss: 0.0092 - activation_11_loss: 0.0093 - val_loss: 0.0257 - val_dense_98_loss: 0.0121 - val_activation_11_loss: 0.0122\n",
      "Epoch 45/500\n",
      " - 13s - loss: 0.0185 - dense_98_loss: 0.0092 - activation_11_loss: 0.0093 - val_loss: 0.0261 - val_dense_98_loss: 0.0125 - val_activation_11_loss: 0.0124\n",
      "Epoch 46/500\n",
      " - 14s - loss: 0.0185 - dense_98_loss: 0.0092 - activation_11_loss: 0.0093 - val_loss: 0.0261 - val_dense_98_loss: 0.0123 - val_activation_11_loss: 0.0123\n",
      "Epoch 47/500\n",
      " - 13s - loss: 0.0184 - dense_98_loss: 0.0092 - activation_11_loss: 0.0092 - val_loss: 0.0258 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0123\n",
      "Epoch 48/500\n",
      " - 13s - loss: 0.0184 - dense_98_loss: 0.0092 - activation_11_loss: 0.0092 - val_loss: 0.0262 - val_dense_98_loss: 0.0124 - val_activation_11_loss: 0.0124\n",
      "Epoch 49/500\n",
      " - 13s - loss: 0.0184 - dense_98_loss: 0.0092 - activation_11_loss: 0.0092 - val_loss: 0.0264 - val_dense_98_loss: 0.0125 - val_activation_11_loss: 0.0124\n",
      "Epoch 50/500\n",
      " - 14s - loss: 0.0183 - dense_98_loss: 0.0091 - activation_11_loss: 0.0092 - val_loss: 0.0261 - val_dense_98_loss: 0.0123 - val_activation_11_loss: 0.0123\n",
      "Epoch 51/500\n",
      " - 13s - loss: 0.0183 - dense_98_loss: 0.0091 - activation_11_loss: 0.0092 - val_loss: 0.0262 - val_dense_98_loss: 0.0124 - val_activation_11_loss: 0.0123\n",
      "Epoch 52/500\n",
      " - 14s - loss: 0.0183 - dense_98_loss: 0.0091 - activation_11_loss: 0.0092 - val_loss: 0.0260 - val_dense_98_loss: 0.0123 - val_activation_11_loss: 0.0123\n",
      "Epoch 53/500\n",
      " - 13s - loss: 0.0183 - dense_98_loss: 0.0091 - activation_11_loss: 0.0092 - val_loss: 0.0259 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0122\n",
      "Epoch 54/500\n",
      " - 13s - loss: 0.0183 - dense_98_loss: 0.0091 - activation_11_loss: 0.0092 - val_loss: 0.0262 - val_dense_98_loss: 0.0124 - val_activation_11_loss: 0.0123\n",
      "Epoch 55/500\n",
      " - 13s - loss: 0.0183 - dense_98_loss: 0.0091 - activation_11_loss: 0.0092 - val_loss: 0.0260 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0122\n",
      "Epoch 56/500\n",
      " - 14s - loss: 0.0182 - dense_98_loss: 0.0091 - activation_11_loss: 0.0091 - val_loss: 0.0260 - val_dense_98_loss: 0.0122 - val_activation_11_loss: 0.0121\n",
      "Epoch 57/500\n",
      " - 14s - loss: 0.0182 - dense_98_loss: 0.0091 - activation_11_loss: 0.0091 - val_loss: 0.0265 - val_dense_98_loss: 0.0125 - val_activation_11_loss: 0.0124\n",
      "Epoch 58/500\n",
      " - 13s - loss: 0.0182 - dense_98_loss: 0.0091 - activation_11_loss: 0.0091 - val_loss: 0.0262 - val_dense_98_loss: 0.0124 - val_activation_11_loss: 0.0122\n",
      "Epoch 59/500\n",
      " - 13s - loss: 0.0182 - dense_98_loss: 0.0091 - activation_11_loss: 0.0091 - val_loss: 0.0258 - val_dense_98_loss: 0.0121 - val_activation_11_loss: 0.0121\n",
      "Epoch 60/500\n",
      " - 13s - loss: 0.0181 - dense_98_loss: 0.0090 - activation_11_loss: 0.0091 - val_loss: 0.0260 - val_dense_98_loss: 0.0124 - val_activation_11_loss: 0.0124\n",
      " val fold: 9 0.012745863\n",
      " val fold: 9 0.012598708\n",
      "****************************************\n",
      "Train on 66696 samples, validate on 2817 samples\n",
      "Epoch 1/500\n",
      " - 23s - loss: 0.0326 - dense_107_loss: 0.0216 - activation_12_loss: 0.0109 - val_loss: 0.0291 - val_dense_107_loss: 0.0143 - val_activation_12_loss: 0.0140\n",
      "Epoch 2/500\n",
      " - 14s - loss: 0.0210 - dense_107_loss: 0.0106 - activation_12_loss: 0.0105 - val_loss: 0.0288 - val_dense_107_loss: 0.0146 - val_activation_12_loss: 0.0146\n",
      "Epoch 3/500\n",
      " - 14s - loss: 0.0205 - dense_107_loss: 0.0103 - activation_12_loss: 0.0103 - val_loss: 0.0283 - val_dense_107_loss: 0.0140 - val_activation_12_loss: 0.0141\n",
      "Epoch 4/500\n",
      " - 14s - loss: 0.0202 - dense_107_loss: 0.0101 - activation_12_loss: 0.0101 - val_loss: 0.0281 - val_dense_107_loss: 0.0138 - val_activation_12_loss: 0.0138\n",
      "Epoch 5/500\n",
      " - 14s - loss: 0.0200 - dense_107_loss: 0.0100 - activation_12_loss: 0.0100 - val_loss: 0.0278 - val_dense_107_loss: 0.0141 - val_activation_12_loss: 0.0142\n",
      "Epoch 6/500\n",
      " - 14s - loss: 0.0198 - dense_107_loss: 0.0099 - activation_12_loss: 0.0099 - val_loss: 0.0278 - val_dense_107_loss: 0.0140 - val_activation_12_loss: 0.0140\n",
      "Epoch 7/500\n",
      " - 14s - loss: 0.0197 - dense_107_loss: 0.0099 - activation_12_loss: 0.0099 - val_loss: 0.0274 - val_dense_107_loss: 0.0136 - val_activation_12_loss: 0.0136\n",
      "Epoch 8/500\n",
      " - 14s - loss: 0.0196 - dense_107_loss: 0.0098 - activation_12_loss: 0.0098 - val_loss: 0.0273 - val_dense_107_loss: 0.0136 - val_activation_12_loss: 0.0137\n",
      "Epoch 9/500\n",
      " - 13s - loss: 0.0195 - dense_107_loss: 0.0097 - activation_12_loss: 0.0097 - val_loss: 0.0271 - val_dense_107_loss: 0.0135 - val_activation_12_loss: 0.0136\n",
      "Epoch 10/500\n",
      " - 14s - loss: 0.0194 - dense_107_loss: 0.0097 - activation_12_loss: 0.0097 - val_loss: 0.0273 - val_dense_107_loss: 0.0135 - val_activation_12_loss: 0.0134\n",
      "Epoch 11/500\n",
      " - 13s - loss: 0.0194 - dense_107_loss: 0.0097 - activation_12_loss: 0.0097 - val_loss: 0.0272 - val_dense_107_loss: 0.0136 - val_activation_12_loss: 0.0136\n",
      "Epoch 12/500\n",
      " - 14s - loss: 0.0193 - dense_107_loss: 0.0096 - activation_12_loss: 0.0097 - val_loss: 0.0272 - val_dense_107_loss: 0.0135 - val_activation_12_loss: 0.0135\n",
      "Epoch 13/500\n",
      " - 14s - loss: 0.0193 - dense_107_loss: 0.0096 - activation_12_loss: 0.0096 - val_loss: 0.0272 - val_dense_107_loss: 0.0137 - val_activation_12_loss: 0.0137\n",
      "Epoch 14/500\n",
      " - 14s - loss: 0.0192 - dense_107_loss: 0.0096 - activation_12_loss: 0.0096 - val_loss: 0.0271 - val_dense_107_loss: 0.0135 - val_activation_12_loss: 0.0134\n",
      "Epoch 15/500\n",
      " - 14s - loss: 0.0192 - dense_107_loss: 0.0096 - activation_12_loss: 0.0096 - val_loss: 0.0272 - val_dense_107_loss: 0.0136 - val_activation_12_loss: 0.0136\n",
      "Epoch 16/500\n",
      " - 14s - loss: 0.0191 - dense_107_loss: 0.0096 - activation_12_loss: 0.0096 - val_loss: 0.0271 - val_dense_107_loss: 0.0135 - val_activation_12_loss: 0.0135\n",
      "Epoch 17/500\n",
      " - 14s - loss: 0.0191 - dense_107_loss: 0.0095 - activation_12_loss: 0.0095 - val_loss: 0.0269 - val_dense_107_loss: 0.0133 - val_activation_12_loss: 0.0132\n",
      "Epoch 18/500\n",
      " - 14s - loss: 0.0190 - dense_107_loss: 0.0095 - activation_12_loss: 0.0095 - val_loss: 0.0269 - val_dense_107_loss: 0.0133 - val_activation_12_loss: 0.0133\n",
      "Epoch 19/500\n",
      " - 13s - loss: 0.0190 - dense_107_loss: 0.0095 - activation_12_loss: 0.0095 - val_loss: 0.0268 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0131\n",
      "Epoch 20/500\n",
      " - 14s - loss: 0.0190 - dense_107_loss: 0.0095 - activation_12_loss: 0.0095 - val_loss: 0.0270 - val_dense_107_loss: 0.0133 - val_activation_12_loss: 0.0133\n",
      "Epoch 21/500\n",
      " - 14s - loss: 0.0190 - dense_107_loss: 0.0095 - activation_12_loss: 0.0095 - val_loss: 0.0271 - val_dense_107_loss: 0.0134 - val_activation_12_loss: 0.0133\n",
      "Epoch 22/500\n",
      " - 14s - loss: 0.0189 - dense_107_loss: 0.0095 - activation_12_loss: 0.0095 - val_loss: 0.0270 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0131\n",
      "Epoch 23/500\n",
      " - 14s - loss: 0.0189 - dense_107_loss: 0.0094 - activation_12_loss: 0.0095 - val_loss: 0.0267 - val_dense_107_loss: 0.0133 - val_activation_12_loss: 0.0132\n",
      "Epoch 24/500\n",
      " - 14s - loss: 0.0189 - dense_107_loss: 0.0094 - activation_12_loss: 0.0094 - val_loss: 0.0271 - val_dense_107_loss: 0.0134 - val_activation_12_loss: 0.0133\n",
      "Epoch 25/500\n",
      " - 13s - loss: 0.0189 - dense_107_loss: 0.0094 - activation_12_loss: 0.0094 - val_loss: 0.0269 - val_dense_107_loss: 0.0136 - val_activation_12_loss: 0.0136\n",
      "Epoch 26/500\n",
      " - 13s - loss: 0.0188 - dense_107_loss: 0.0094 - activation_12_loss: 0.0094 - val_loss: 0.0273 - val_dense_107_loss: 0.0134 - val_activation_12_loss: 0.0133\n",
      "Epoch 27/500\n",
      " - 14s - loss: 0.0188 - dense_107_loss: 0.0094 - activation_12_loss: 0.0094 - val_loss: 0.0270 - val_dense_107_loss: 0.0133 - val_activation_12_loss: 0.0133\n",
      "Epoch 28/500\n",
      " - 13s - loss: 0.0188 - dense_107_loss: 0.0094 - activation_12_loss: 0.0094 - val_loss: 0.0268 - val_dense_107_loss: 0.0130 - val_activation_12_loss: 0.0129\n",
      "Epoch 29/500\n",
      " - 14s - loss: 0.0187 - dense_107_loss: 0.0093 - activation_12_loss: 0.0094 - val_loss: 0.0269 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0131\n",
      "Epoch 30/500\n",
      " - 13s - loss: 0.0187 - dense_107_loss: 0.0093 - activation_12_loss: 0.0094 - val_loss: 0.0271 - val_dense_107_loss: 0.0135 - val_activation_12_loss: 0.0135\n",
      "Epoch 31/500\n",
      " - 14s - loss: 0.0187 - dense_107_loss: 0.0093 - activation_12_loss: 0.0094 - val_loss: 0.0265 - val_dense_107_loss: 0.0130 - val_activation_12_loss: 0.0130\n",
      "Epoch 32/500\n",
      " - 14s - loss: 0.0187 - dense_107_loss: 0.0093 - activation_12_loss: 0.0093 - val_loss: 0.0269 - val_dense_107_loss: 0.0135 - val_activation_12_loss: 0.0134\n",
      "Epoch 33/500\n",
      " - 14s - loss: 0.0187 - dense_107_loss: 0.0093 - activation_12_loss: 0.0093 - val_loss: 0.0267 - val_dense_107_loss: 0.0134 - val_activation_12_loss: 0.0134\n",
      "Epoch 34/500\n",
      " - 14s - loss: 0.0186 - dense_107_loss: 0.0093 - activation_12_loss: 0.0093 - val_loss: 0.0271 - val_dense_107_loss: 0.0134 - val_activation_12_loss: 0.0133\n",
      "Epoch 35/500\n",
      " - 13s - loss: 0.0186 - dense_107_loss: 0.0093 - activation_12_loss: 0.0093 - val_loss: 0.0266 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0131\n",
      "Epoch 36/500\n",
      " - 13s - loss: 0.0186 - dense_107_loss: 0.0093 - activation_12_loss: 0.0093 - val_loss: 0.0267 - val_dense_107_loss: 0.0134 - val_activation_12_loss: 0.0134\n",
      "Epoch 37/500\n",
      " - 14s - loss: 0.0186 - dense_107_loss: 0.0093 - activation_12_loss: 0.0093 - val_loss: 0.0266 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0132\n",
      "Epoch 38/500\n",
      " - 14s - loss: 0.0185 - dense_107_loss: 0.0093 - activation_12_loss: 0.0093 - val_loss: 0.0266 - val_dense_107_loss: 0.0133 - val_activation_12_loss: 0.0133\n",
      "Epoch 39/500\n",
      " - 13s - loss: 0.0185 - dense_107_loss: 0.0093 - activation_12_loss: 0.0093 - val_loss: 0.0269 - val_dense_107_loss: 0.0131 - val_activation_12_loss: 0.0131\n",
      "Epoch 40/500\n",
      " - 14s - loss: 0.0185 - dense_107_loss: 0.0093 - activation_12_loss: 0.0093 - val_loss: 0.0270 - val_dense_107_loss: 0.0133 - val_activation_12_loss: 0.0133\n",
      "Epoch 41/500\n",
      " - 14s - loss: 0.0185 - dense_107_loss: 0.0092 - activation_12_loss: 0.0093 - val_loss: 0.0265 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0132\n",
      "Epoch 42/500\n",
      " - 13s - loss: 0.0185 - dense_107_loss: 0.0092 - activation_12_loss: 0.0093 - val_loss: 0.0266 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0132\n",
      "Epoch 43/500\n",
      " - 14s - loss: 0.0185 - dense_107_loss: 0.0092 - activation_12_loss: 0.0092 - val_loss: 0.0265 - val_dense_107_loss: 0.0131 - val_activation_12_loss: 0.0131\n",
      "Epoch 44/500\n",
      " - 14s - loss: 0.0184 - dense_107_loss: 0.0092 - activation_12_loss: 0.0092 - val_loss: 0.0269 - val_dense_107_loss: 0.0131 - val_activation_12_loss: 0.0131\n",
      "Epoch 45/500\n",
      " - 14s - loss: 0.0185 - dense_107_loss: 0.0092 - activation_12_loss: 0.0092 - val_loss: 0.0267 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0132\n",
      "Epoch 46/500\n",
      " - 14s - loss: 0.0184 - dense_107_loss: 0.0092 - activation_12_loss: 0.0092 - val_loss: 0.0268 - val_dense_107_loss: 0.0133 - val_activation_12_loss: 0.0134\n",
      "Epoch 47/500\n",
      " - 14s - loss: 0.0184 - dense_107_loss: 0.0092 - activation_12_loss: 0.0092 - val_loss: 0.0268 - val_dense_107_loss: 0.0133 - val_activation_12_loss: 0.0133\n",
      "Epoch 48/500\n",
      " - 14s - loss: 0.0184 - dense_107_loss: 0.0092 - activation_12_loss: 0.0092 - val_loss: 0.0266 - val_dense_107_loss: 0.0130 - val_activation_12_loss: 0.0130\n",
      "Epoch 49/500\n",
      " - 14s - loss: 0.0184 - dense_107_loss: 0.0092 - activation_12_loss: 0.0092 - val_loss: 0.0269 - val_dense_107_loss: 0.0133 - val_activation_12_loss: 0.0133\n",
      "Epoch 50/500\n",
      " - 13s - loss: 0.0183 - dense_107_loss: 0.0092 - activation_12_loss: 0.0092 - val_loss: 0.0266 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0132\n",
      "Epoch 51/500\n",
      " - 14s - loss: 0.0183 - dense_107_loss: 0.0091 - activation_12_loss: 0.0092 - val_loss: 0.0270 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0131\n",
      "Epoch 52/500\n",
      " - 14s - loss: 0.0183 - dense_107_loss: 0.0091 - activation_12_loss: 0.0092 - val_loss: 0.0273 - val_dense_107_loss: 0.0135 - val_activation_12_loss: 0.0134\n",
      "Epoch 53/500\n",
      " - 14s - loss: 0.0182 - dense_107_loss: 0.0091 - activation_12_loss: 0.0091 - val_loss: 0.0267 - val_dense_107_loss: 0.0133 - val_activation_12_loss: 0.0133\n",
      "Epoch 54/500\n",
      " - 13s - loss: 0.0182 - dense_107_loss: 0.0091 - activation_12_loss: 0.0091 - val_loss: 0.0271 - val_dense_107_loss: 0.0134 - val_activation_12_loss: 0.0134\n",
      "Epoch 55/500\n",
      " - 13s - loss: 0.0182 - dense_107_loss: 0.0091 - activation_12_loss: 0.0091 - val_loss: 0.0267 - val_dense_107_loss: 0.0130 - val_activation_12_loss: 0.0130\n",
      "Epoch 56/500\n",
      " - 14s - loss: 0.0181 - dense_107_loss: 0.0090 - activation_12_loss: 0.0091 - val_loss: 0.0268 - val_dense_107_loss: 0.0130 - val_activation_12_loss: 0.0130\n",
      "Epoch 57/500\n",
      " - 14s - loss: 0.0181 - dense_107_loss: 0.0090 - activation_12_loss: 0.0091 - val_loss: 0.0267 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0132\n",
      "Epoch 58/500\n",
      " - 13s - loss: 0.0181 - dense_107_loss: 0.0090 - activation_12_loss: 0.0091 - val_loss: 0.0271 - val_dense_107_loss: 0.0136 - val_activation_12_loss: 0.0135\n",
      "Epoch 59/500\n",
      " - 13s - loss: 0.0181 - dense_107_loss: 0.0090 - activation_12_loss: 0.0091 - val_loss: 0.0266 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0132\n",
      "Epoch 60/500\n",
      " - 14s - loss: 0.0181 - dense_107_loss: 0.0090 - activation_12_loss: 0.0091 - val_loss: 0.0276 - val_dense_107_loss: 0.0136 - val_activation_12_loss: 0.0134\n",
      "Epoch 61/500\n",
      " - 14s - loss: 0.0180 - dense_107_loss: 0.0090 - activation_12_loss: 0.0090 - val_loss: 0.0269 - val_dense_107_loss: 0.0133 - val_activation_12_loss: 0.0133\n",
      "Epoch 62/500\n",
      " - 14s - loss: 0.0180 - dense_107_loss: 0.0090 - activation_12_loss: 0.0090 - val_loss: 0.0267 - val_dense_107_loss: 0.0132 - val_activation_12_loss: 0.0132\n",
      " val fold: 10 0.013202029\n",
      " val fold: 10 0.013023273\n",
      "****************************************\n",
      "Train on 66696 samples, validate on 2817 samples\n",
      "Epoch 1/500\n",
      " - 22s - loss: 0.0336 - dense_116_loss: 0.0227 - activation_13_loss: 0.0109 - val_loss: 0.0284 - val_dense_116_loss: 0.0155 - val_activation_13_loss: 0.0153\n",
      "Epoch 2/500\n",
      " - 13s - loss: 0.0213 - dense_116_loss: 0.0107 - activation_13_loss: 0.0106 - val_loss: 0.0281 - val_dense_116_loss: 0.0155 - val_activation_13_loss: 0.0156\n",
      "Epoch 3/500\n",
      " - 14s - loss: 0.0206 - dense_116_loss: 0.0103 - activation_13_loss: 0.0103 - val_loss: 0.0270 - val_dense_116_loss: 0.0145 - val_activation_13_loss: 0.0144\n",
      "Epoch 4/500\n",
      " - 14s - loss: 0.0202 - dense_116_loss: 0.0101 - activation_13_loss: 0.0101 - val_loss: 0.0265 - val_dense_116_loss: 0.0140 - val_activation_13_loss: 0.0140\n",
      "Epoch 5/500\n",
      " - 14s - loss: 0.0200 - dense_116_loss: 0.0100 - activation_13_loss: 0.0100 - val_loss: 0.0264 - val_dense_116_loss: 0.0140 - val_activation_13_loss: 0.0140\n",
      "Epoch 6/500\n",
      " - 14s - loss: 0.0199 - dense_116_loss: 0.0099 - activation_13_loss: 0.0099 - val_loss: 0.0262 - val_dense_116_loss: 0.0136 - val_activation_13_loss: 0.0136\n",
      "Epoch 7/500\n",
      " - 14s - loss: 0.0198 - dense_116_loss: 0.0099 - activation_13_loss: 0.0099 - val_loss: 0.0262 - val_dense_116_loss: 0.0143 - val_activation_13_loss: 0.0143\n",
      "Epoch 8/500\n",
      " - 14s - loss: 0.0196 - dense_116_loss: 0.0098 - activation_13_loss: 0.0098 - val_loss: 0.0260 - val_dense_116_loss: 0.0141 - val_activation_13_loss: 0.0142\n",
      "Epoch 9/500\n",
      " - 14s - loss: 0.0196 - dense_116_loss: 0.0098 - activation_13_loss: 0.0098 - val_loss: 0.0259 - val_dense_116_loss: 0.0142 - val_activation_13_loss: 0.0143\n",
      "Epoch 10/500\n",
      " - 14s - loss: 0.0195 - dense_116_loss: 0.0097 - activation_13_loss: 0.0098 - val_loss: 0.0259 - val_dense_116_loss: 0.0145 - val_activation_13_loss: 0.0145\n",
      "Epoch 11/500\n",
      " - 14s - loss: 0.0194 - dense_116_loss: 0.0097 - activation_13_loss: 0.0097 - val_loss: 0.0257 - val_dense_116_loss: 0.0143 - val_activation_13_loss: 0.0143\n",
      "Epoch 12/500\n",
      " - 14s - loss: 0.0194 - dense_116_loss: 0.0097 - activation_13_loss: 0.0097 - val_loss: 0.0258 - val_dense_116_loss: 0.0145 - val_activation_13_loss: 0.0145\n",
      "Epoch 13/500\n",
      " - 13s - loss: 0.0193 - dense_116_loss: 0.0097 - activation_13_loss: 0.0097 - val_loss: 0.0256 - val_dense_116_loss: 0.0142 - val_activation_13_loss: 0.0142\n",
      "Epoch 14/500\n",
      " - 14s - loss: 0.0193 - dense_116_loss: 0.0096 - activation_13_loss: 0.0096 - val_loss: 0.0256 - val_dense_116_loss: 0.0146 - val_activation_13_loss: 0.0148\n",
      "Epoch 15/500\n",
      " - 13s - loss: 0.0192 - dense_116_loss: 0.0096 - activation_13_loss: 0.0096 - val_loss: 0.0256 - val_dense_116_loss: 0.0145 - val_activation_13_loss: 0.0146\n",
      "Epoch 16/500\n",
      " - 13s - loss: 0.0192 - dense_116_loss: 0.0096 - activation_13_loss: 0.0096 - val_loss: 0.0257 - val_dense_116_loss: 0.0147 - val_activation_13_loss: 0.0148\n",
      "Epoch 17/500\n",
      " - 14s - loss: 0.0192 - dense_116_loss: 0.0096 - activation_13_loss: 0.0096 - val_loss: 0.0255 - val_dense_116_loss: 0.0145 - val_activation_13_loss: 0.0145\n",
      "Epoch 18/500\n",
      " - 14s - loss: 0.0191 - dense_116_loss: 0.0096 - activation_13_loss: 0.0096 - val_loss: 0.0253 - val_dense_116_loss: 0.0145 - val_activation_13_loss: 0.0145\n",
      "Epoch 19/500\n",
      " - 14s - loss: 0.0191 - dense_116_loss: 0.0096 - activation_13_loss: 0.0096 - val_loss: 0.0255 - val_dense_116_loss: 0.0146 - val_activation_13_loss: 0.0147\n",
      "Epoch 20/500\n",
      " - 13s - loss: 0.0191 - dense_116_loss: 0.0095 - activation_13_loss: 0.0095 - val_loss: 0.0254 - val_dense_116_loss: 0.0143 - val_activation_13_loss: 0.0143\n",
      "Epoch 21/500\n",
      " - 15s - loss: 0.0190 - dense_116_loss: 0.0095 - activation_13_loss: 0.0095 - val_loss: 0.0252 - val_dense_116_loss: 0.0142 - val_activation_13_loss: 0.0143\n",
      "Epoch 22/500\n",
      " - 13s - loss: 0.0190 - dense_116_loss: 0.0095 - activation_13_loss: 0.0095 - val_loss: 0.0252 - val_dense_116_loss: 0.0141 - val_activation_13_loss: 0.0142\n",
      "Epoch 23/500\n",
      " - 14s - loss: 0.0189 - dense_116_loss: 0.0095 - activation_13_loss: 0.0095 - val_loss: 0.0252 - val_dense_116_loss: 0.0142 - val_activation_13_loss: 0.0143\n",
      "Epoch 24/500\n",
      " - 14s - loss: 0.0189 - dense_116_loss: 0.0094 - activation_13_loss: 0.0095 - val_loss: 0.0252 - val_dense_116_loss: 0.0143 - val_activation_13_loss: 0.0144\n",
      "Epoch 25/500\n",
      " - 14s - loss: 0.0189 - dense_116_loss: 0.0094 - activation_13_loss: 0.0095 - val_loss: 0.0252 - val_dense_116_loss: 0.0145 - val_activation_13_loss: 0.0146\n",
      "Epoch 26/500\n",
      " - 14s - loss: 0.0189 - dense_116_loss: 0.0094 - activation_13_loss: 0.0095 - val_loss: 0.0251 - val_dense_116_loss: 0.0140 - val_activation_13_loss: 0.0141\n",
      "Epoch 27/500\n",
      " - 14s - loss: 0.0189 - dense_116_loss: 0.0094 - activation_13_loss: 0.0095 - val_loss: 0.0257 - val_dense_116_loss: 0.0141 - val_activation_13_loss: 0.0140\n",
      "Epoch 28/500\n",
      " - 13s - loss: 0.0188 - dense_116_loss: 0.0094 - activation_13_loss: 0.0094 - val_loss: 0.0252 - val_dense_116_loss: 0.0142 - val_activation_13_loss: 0.0142\n",
      "Epoch 29/500\n",
      " - 14s - loss: 0.0188 - dense_116_loss: 0.0094 - activation_13_loss: 0.0094 - val_loss: 0.0250 - val_dense_116_loss: 0.0142 - val_activation_13_loss: 0.0142\n",
      "Epoch 30/500\n",
      " - 14s - loss: 0.0188 - dense_116_loss: 0.0094 - activation_13_loss: 0.0094 - val_loss: 0.0253 - val_dense_116_loss: 0.0140 - val_activation_13_loss: 0.0141\n",
      "Epoch 31/500\n",
      " - 14s - loss: 0.0187 - dense_116_loss: 0.0094 - activation_13_loss: 0.0094 - val_loss: 0.0253 - val_dense_116_loss: 0.0141 - val_activation_13_loss: 0.0142\n",
      "Epoch 32/500\n",
      " - 14s - loss: 0.0187 - dense_116_loss: 0.0094 - activation_13_loss: 0.0094 - val_loss: 0.0251 - val_dense_116_loss: 0.0141 - val_activation_13_loss: 0.0142\n",
      "Epoch 33/500\n",
      " - 14s - loss: 0.0187 - dense_116_loss: 0.0093 - activation_13_loss: 0.0094 - val_loss: 0.0250 - val_dense_116_loss: 0.0141 - val_activation_13_loss: 0.0142\n",
      "Epoch 34/500\n",
      " - 14s - loss: 0.0187 - dense_116_loss: 0.0093 - activation_13_loss: 0.0094 - val_loss: 0.0252 - val_dense_116_loss: 0.0141 - val_activation_13_loss: 0.0141\n",
      "Epoch 35/500\n",
      " - 14s - loss: 0.0187 - dense_116_loss: 0.0093 - activation_13_loss: 0.0094 - val_loss: 0.0250 - val_dense_116_loss: 0.0139 - val_activation_13_loss: 0.0139\n",
      "Epoch 36/500\n",
      " - 14s - loss: 0.0187 - dense_116_loss: 0.0093 - activation_13_loss: 0.0094 - val_loss: 0.0254 - val_dense_116_loss: 0.0146 - val_activation_13_loss: 0.0148\n",
      "Epoch 37/500\n",
      " - 14s - loss: 0.0186 - dense_116_loss: 0.0093 - activation_13_loss: 0.0093 - val_loss: 0.0252 - val_dense_116_loss: 0.0140 - val_activation_13_loss: 0.0140\n",
      "Epoch 38/500\n",
      " - 14s - loss: 0.0186 - dense_116_loss: 0.0093 - activation_13_loss: 0.0093 - val_loss: 0.0251 - val_dense_116_loss: 0.0145 - val_activation_13_loss: 0.0146\n",
      "Epoch 39/500\n",
      " - 14s - loss: 0.0186 - dense_116_loss: 0.0093 - activation_13_loss: 0.0093 - val_loss: 0.0252 - val_dense_116_loss: 0.0142 - val_activation_13_loss: 0.0142\n",
      "Epoch 40/500\n",
      " - 14s - loss: 0.0186 - dense_116_loss: 0.0093 - activation_13_loss: 0.0093 - val_loss: 0.0253 - val_dense_116_loss: 0.0138 - val_activation_13_loss: 0.0138\n",
      "Epoch 41/500\n",
      " - 13s - loss: 0.0186 - dense_116_loss: 0.0093 - activation_13_loss: 0.0093 - val_loss: 0.0253 - val_dense_116_loss: 0.0142 - val_activation_13_loss: 0.0143\n",
      "Epoch 42/500\n",
      " - 14s - loss: 0.0185 - dense_116_loss: 0.0092 - activation_13_loss: 0.0093 - val_loss: 0.0252 - val_dense_116_loss: 0.0143 - val_activation_13_loss: 0.0142\n",
      "Epoch 43/500\n",
      " - 15s - loss: 0.0185 - dense_116_loss: 0.0093 - activation_13_loss: 0.0093 - val_loss: 0.0252 - val_dense_116_loss: 0.0141 - val_activation_13_loss: 0.0141\n",
      "Epoch 44/500\n",
      " - 14s - loss: 0.0185 - dense_116_loss: 0.0092 - activation_13_loss: 0.0093 - val_loss: 0.0253 - val_dense_116_loss: 0.0144 - val_activation_13_loss: 0.0144\n",
      "Epoch 45/500\n",
      " - 14s - loss: 0.0185 - dense_116_loss: 0.0093 - activation_13_loss: 0.0093 - val_loss: 0.0256 - val_dense_116_loss: 0.0147 - val_activation_13_loss: 0.0147\n",
      "Epoch 46/500\n",
      " - 14s - loss: 0.0184 - dense_116_loss: 0.0092 - activation_13_loss: 0.0092 - val_loss: 0.0250 - val_dense_116_loss: 0.0141 - val_activation_13_loss: 0.0141\n",
      "Epoch 47/500\n",
      " - 14s - loss: 0.0184 - dense_116_loss: 0.0092 - activation_13_loss: 0.0092 - val_loss: 0.0251 - val_dense_116_loss: 0.0140 - val_activation_13_loss: 0.0140\n",
      "Epoch 48/500\n",
      " - 14s - loss: 0.0184 - dense_116_loss: 0.0092 - activation_13_loss: 0.0092 - val_loss: 0.0253 - val_dense_116_loss: 0.0144 - val_activation_13_loss: 0.0145\n",
      "Epoch 49/500\n",
      " - 14s - loss: 0.0184 - dense_116_loss: 0.0092 - activation_13_loss: 0.0092 - val_loss: 0.0255 - val_dense_116_loss: 0.0145 - val_activation_13_loss: 0.0143\n",
      "Epoch 50/500\n",
      " - 13s - loss: 0.0183 - dense_116_loss: 0.0091 - activation_13_loss: 0.0092 - val_loss: 0.0250 - val_dense_116_loss: 0.0141 - val_activation_13_loss: 0.0140\n",
      "Epoch 51/500\n",
      " - 14s - loss: 0.0183 - dense_116_loss: 0.0091 - activation_13_loss: 0.0092 - val_loss: 0.0252 - val_dense_116_loss: 0.0139 - val_activation_13_loss: 0.0139\n",
      "Epoch 52/500\n",
      " - 14s - loss: 0.0183 - dense_116_loss: 0.0092 - activation_13_loss: 0.0092 - val_loss: 0.0251 - val_dense_116_loss: 0.0132 - val_activation_13_loss: 0.0132\n",
      "Epoch 53/500\n",
      " - 14s - loss: 0.0183 - dense_116_loss: 0.0091 - activation_13_loss: 0.0092 - val_loss: 0.0253 - val_dense_116_loss: 0.0141 - val_activation_13_loss: 0.0141\n",
      "Epoch 54/500\n",
      " - 14s - loss: 0.0183 - dense_116_loss: 0.0091 - activation_13_loss: 0.0092 - val_loss: 0.0252 - val_dense_116_loss: 0.0142 - val_activation_13_loss: 0.0141\n",
      "Epoch 55/500\n",
      " - 14s - loss: 0.0183 - dense_116_loss: 0.0091 - activation_13_loss: 0.0092 - val_loss: 0.0253 - val_dense_116_loss: 0.0145 - val_activation_13_loss: 0.0145\n",
      "Epoch 56/500\n",
      " - 14s - loss: 0.0182 - dense_116_loss: 0.0091 - activation_13_loss: 0.0091 - val_loss: 0.0252 - val_dense_116_loss: 0.0141 - val_activation_13_loss: 0.0140\n",
      " val fold: 11 0.012466412\n",
      " val fold: 11 0.012340344\n",
      "****************************************\n",
      "Folds: [0.0119221, 0.012178002, 0.0126527725, 0.014294522, 0.013022779, 0.0130896615, 0.013162894, 0.013369378, 0.01409386, 0.012598708, 0.013023273, 0.012340344]\n",
      "Average: 0.012979024\n"
     ]
    }
   ],
   "source": [
    "name = 'nn'\n",
    "results[name] = {'crps': [], 'y_test': [], 'imp': [], 'models': []}\n",
    "\n",
    "try:\n",
    "    del sess\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sess = init_seeds(0)\n",
    "\n",
    "pred_test = []\n",
    "\n",
    "\n",
    "for index in indices:\n",
    "    if 0 and index[0] < 10:\n",
    "        continue\n",
    "    train_idx, val_idx = index[1]\n",
    "    train_players, train_inv_dmats, train_play, train_y, = (X_players_train[train_idx], \n",
    "                                                            inv_dmats_train[train_idx], \n",
    "                                                            X_play_train[train_idx], \n",
    "                                                            y_crps_train[train_idx],) \n",
    "    val_players, val_inv_dmats, val_play, val_y, = (X_players_train[val_idx], \n",
    "                                                    inv_dmats_train[val_idx], \n",
    "                                                    X_play_train[val_idx], \n",
    "                                                    y_crps_train[val_idx], )\n",
    "    w_train = w[train_idx]\n",
    "\n",
    "    n_player = 22\n",
    "    n_factor = 64\n",
    "    se_ratio = 2\n",
    "    n_loop = 1\n",
    "    n_head = 4\n",
    "    n_hidden = 2*n_factor\n",
    "    dropout = 0.25\n",
    "    n_player_cols = len(player_cols)\n",
    "    n_play_cols = len(play_cols)\n",
    "    model = get_model(n_player, n_factor, n_loop, n_head, n_hidden, se_ratio, dropout, n_player_cols, n_play_cols)\n",
    "    \n",
    "    if 1:\n",
    "        opm = keras.optimizers.Adam(lr=1e-3)\n",
    "        #opm = RAdam(warmup_proportion=0.1, min_lr=1e-5)\n",
    "        model.compile(loss='mse', optimizer=opm, metrics=[])\n",
    "        es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', \n",
    "                                           restore_best_weights=True, verbose=0, patience=21)\n",
    "        lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=0, mode='min', min_delta=0.00001)\n",
    "        history = model.fit([train_players, train_inv_dmats, train_play], [train_y, train_y], \n",
    "                        verbose=2, batch_size=256, epochs=500, \n",
    "                        callbacks=[es,lr], \n",
    "                        validation_data=[[val_players, val_inv_dmats, val_play], [val_y, val_y]],\n",
    "                        sample_weight=[w_train, w_train])\n",
    "    \n",
    "    X_ir = np.arange(199).astype('float64')\n",
    "    #print(val_players.shape, val_inv_dmats.shape)\n",
    "\n",
    "    y_pred = model.predict([val_players, val_inv_dmats, val_play])\n",
    "    y_pred = (y_pred[0]+y_pred[1])/2.\n",
    "    score0 = crps(y_mae_train[val_idx], y_pred)\n",
    "    y_pred = post_process(y_pred, yardline_train[val_idx], time_steps)\n",
    "    score1 = crps(y_mae_train[val_idx][:len(y_pred)], y_pred)\n",
    "    print(' val fold:', index[0], score0)\n",
    "    print(' val fold:', index[0], score1)\n",
    "    if score0 < score1:\n",
    "        break\n",
    "    results[name]['models'].append(model)\n",
    "    results[name]['crps'].append(score1)\n",
    "    #results[name]['imp'].append(model.feature_importances_)\n",
    "    if simulate_test:\n",
    "        y_pred = model.predict([X_players_test, inv_dmats_test, X_play_test])\n",
    "        y_pred = (y_pred[0]+y_pred[1])/2.\n",
    "        score0 = crps(y_mae_test, y_pred)\n",
    "        y_pred = post_process(y_pred, yardline_test, time_steps)\n",
    "        score1 = crps(y_mae_test[:len(y_pred)], y_pred)\n",
    "        print('test fold:', index[0], score0)\n",
    "        print('test fold:', index[0], score1)\n",
    "        pred_test.append(y_pred)\n",
    "    print('*' * 40)\n",
    "print('Folds:', results[name]['crps'])\n",
    "print('Average:', np.mean(results[name]['crps']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete our training data to make sure we don't use it anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_reorient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the code computes submission.  It performs the same data cleaning and data augmentation than was done on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "env = nflrush.make_env()\n",
    "iter_test = env.iter_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3438it [28:50,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission file has been saved!  Once you `Commit` your Notebook and it finishes running, you can submit the file to the competition from the Notebook Viewer `Output` tab.\n"
     ]
    }
   ],
   "source": [
    "for (test_df, sample_prediction_df) in tqdm(iter_test):\n",
    "    test_reorient = reorient(test_df.copy(), flip_left=True)\n",
    "    test_reorient = augment_data(test_reorient, time_steps)\n",
    "    df = test_reorient[player_cols]\n",
    "    X_players = df.values.astype('float32') \n",
    "    X_players = ss_players.transform(df)\n",
    "    X_players = X_players.reshape((-1, 22, len(player_cols)))\n",
    "    X,Y = test_reorient.X.values, test_reorient.Y.values\n",
    "    dmats = [get_dmat(X[i:i+22], Y[i:i+22]) for i in range(0, test_reorient.shape[0], 22)]\n",
    "    dmats = np.vstack(dmats).reshape((-1, 22, 22))   \n",
    "    inv_dmats = 1. / (1e-2 + dmats)**2\n",
    "    inv_dmats /= inv_dmats.sum(axis=2, keepdims=True)    \n",
    "    X_play = test_reorient[play_cols].values[::22]\n",
    "    X_play = ss_play.transform(X_play)    \n",
    "    yardline = test_reorient['YardLine_std'].values[::22]\n",
    "    y_preds = [model.predict([X_players, inv_dmats, X_play]\n",
    "                            ) for model in results[name]['models']]\n",
    "    y_preds = [(pred[0] + pred[1]) / 2. for pred in y_preds]\n",
    "    y_pred = np.mean(y_preds, axis=0)\n",
    "    y_pred = post_process(y_pred, yardline, time_steps)\n",
    "    sample_prediction_df.iloc[0, :] = y_pred.ravel()\n",
    "    env.predict(sample_prediction_df)\n",
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 199)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66, 64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reorient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 22, 22)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y = test_reorient.X.values, test_reorient.Y.values\n",
    "dmats = [get_dmat(X[i:i+22], Y[i:i+22]) for i in range(0, test_reorient.shape[0], 22)]\n",
    "dmats = np.vstack(dmats).reshape((-1, 22, 22))\n",
    "dmats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
