{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NFL Competition\n",
    "\n",
    "# Feature Engineering e Modelo de Machine Learning\n",
    "\n",
    "- Version: 1.0: usando padrao do fork: https://www.kaggle.com/bestpredict/location-eda-8eb410\n",
    "        *    Resultado: 0.012744\n",
    "        *    LB: 0.01363\n",
    "   \n",
    "- Version: 2.0: adicionado Feature Selection com LOFO Importance\n",
    "        *    Resultado: 0.012780\n",
    "        *    LB: 0.01365\n",
    "\n",
    "- Version: 3.0: adicionado novas features (apenas feature fxx + old_data + YardLine_std)\n",
    "        *    Resultado: 0.012614\n",
    "        *    LB: 0.01361\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importa os pacotes e o dataset de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Importar os principais pacotes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "import codecs\n",
    "import time\n",
    "import datetime\n",
    "import tsfresh\n",
    "import pandasql as ps\n",
    "\n",
    "# Evitar que aparece os warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Seta algumas opções no Jupyter para exibição dos datasets\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "\n",
    "# Variavel para controlar o treinamento no Kaggle\n",
    "TRAIN_OFFLINE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os pacotes de algoritmos de regressão\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Importa os pacotes de algoritmos de redes neurais (Keras)\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda,BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import Callback,EarlyStopping,ModelCheckpoint\n",
    "import keras.backend as K\n",
    "#from keras_lookahead import Lookahead\n",
    "#from keras_radam import RAdam\n",
    "\n",
    "# Importa pacotes do sklearn\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics as mtr\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})\n",
    "if TRAIN_OFFLINE:\n",
    "    train = pd.read_csv('../data/train.csv', dtype={'WindSpeed': 'object'})\n",
    "else:\n",
    "    train = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = train[['GameId','PlayId','Yards']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strtoseconds(txt):\n",
    "    txt = txt.split(':')\n",
    "    ans = int(txt[0])*60 + int(txt[1]) + int(txt[2])/60\n",
    "    return ans\n",
    "\n",
    "def strtofloat(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "def get_time(x):\n",
    "    x = x.split(\":\")\n",
    "    return int(x[0])*60 + int(x[1])\n",
    "\n",
    "def map_weather(txt):\n",
    "    ans = 1\n",
    "    if pd.isna(txt):\n",
    "        return 0\n",
    "    if 'partly' in txt:\n",
    "        ans*=0.5\n",
    "    if 'climate controlled' in txt or 'indoor' in txt:\n",
    "        return ans*3\n",
    "    if 'sunny' in txt or 'sun' in txt:\n",
    "        return ans*2\n",
    "    if 'clear' in txt:\n",
    "        return ans\n",
    "    if 'cloudy' in txt:\n",
    "        return -ans\n",
    "    if 'rain' in txt or 'rainy' in txt:\n",
    "        return -2*ans\n",
    "    if 'snow' in txt:\n",
    "        return -3*ans\n",
    "    return 0\n",
    "\n",
    "def OffensePersonnelSplit(x):\n",
    "    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0, 'QB' : 0, 'RB' : 0, 'TE' : 0, 'WR' : 0}\n",
    "    for xx in x.split(\",\"):\n",
    "        xxs = xx.split(\" \")\n",
    "        dic[xxs[-1]] = int(xxs[-2])\n",
    "    return dic\n",
    "\n",
    "def DefensePersonnelSplit(x):\n",
    "    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0}\n",
    "    for xx in x.split(\",\"):\n",
    "        xxs = xx.split(\" \")\n",
    "        dic[xxs[-1]] = int(xxs[-2])\n",
    "    return dic\n",
    "\n",
    "def orientation_to_cat(x):\n",
    "    x = np.clip(x, 0, 360 - 1)\n",
    "    try:\n",
    "        return str(int(x/15))\n",
    "    except:\n",
    "        return \"nan\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, deploy=False):\n",
    "    def new_X(x_coordinate, play_direction):\n",
    "        if play_direction == 'left':\n",
    "            return 120.0 - x_coordinate\n",
    "        else:\n",
    "            return x_coordinate\n",
    "\n",
    "    def new_line(rush_team, field_position, yardline):\n",
    "        if rush_team == field_position:\n",
    "            # offense starting at X = 0 plus the 10 yard endzone plus the line of scrimmage\n",
    "            return 10.0 + yardline\n",
    "        else:\n",
    "            # half the field plus the yards between midfield and the line of scrimmage\n",
    "            return 60.0 + (50 - yardline)\n",
    "\n",
    "    def new_orientation(angle, play_direction):\n",
    "        if play_direction == 'left':\n",
    "            new_angle = 360.0 - angle\n",
    "            if new_angle == 360.0:\n",
    "                new_angle = 0.0\n",
    "            return new_angle\n",
    "        else:\n",
    "            return angle\n",
    "\n",
    "    def euclidean_distance(x1,y1,x2,y2):\n",
    "        x_diff = (x1-x2)**2\n",
    "        y_diff = (y1-y2)**2\n",
    "\n",
    "        return np.sqrt(x_diff + y_diff)\n",
    "\n",
    "    def back_direction(orientation):\n",
    "        if orientation > 180.0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def update_yardline(df):\n",
    "        new_yardline = df[df['NflId'] == df['NflIdRusher']]\n",
    "        new_yardline['YardLine'] = new_yardline[['PossessionTeam','FieldPosition','YardLine']].apply(lambda x: new_line(x[0],x[1],x[2]), axis=1)\n",
    "        new_yardline = new_yardline[['GameId','PlayId','YardLine']]\n",
    "\n",
    "        return new_yardline\n",
    "\n",
    "    def update_orientation(df, yardline):\n",
    "        df['X'] = df[['X','PlayDirection']].apply(lambda x: new_X(x[0],x[1]), axis=1)\n",
    "        df['Orientation'] = df[['Orientation','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n",
    "        df['Dir'] = df[['Dir','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n",
    "\n",
    "        df = df.drop('YardLine', axis=1)\n",
    "        df = pd.merge(df, yardline, on=['GameId','PlayId'], how='inner')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def back_features(df):\n",
    "        carriers = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','NflIdRusher','X','Y','Orientation','Dir','YardLine']]\n",
    "        carriers['back_from_scrimmage'] = carriers['YardLine'] - carriers['X']\n",
    "        carriers['back_oriented_down_field'] = carriers['Orientation'].apply(lambda x: back_direction(x))\n",
    "        carriers['back_moving_down_field'] = carriers['Dir'].apply(lambda x: back_direction(x))\n",
    "        carriers = carriers.rename(columns={'X':'back_X',\n",
    "                                            'Y':'back_Y'})\n",
    "        carriers = carriers[['GameId','PlayId','NflIdRusher','back_X','back_Y','back_from_scrimmage','back_oriented_down_field','back_moving_down_field']]\n",
    "\n",
    "        return carriers\n",
    "\n",
    "    def features_relative_to_back(df, carriers):\n",
    "        player_distance = df[['GameId','PlayId','NflId','X','Y']]\n",
    "        player_distance = pd.merge(player_distance, carriers, on=['GameId','PlayId'], how='inner')\n",
    "        player_distance = player_distance[player_distance['NflId'] != player_distance['NflIdRusher']]\n",
    "        player_distance['dist_to_back'] = player_distance[['X','Y','back_X','back_Y']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n",
    "\n",
    "        player_distance = player_distance.groupby(['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field'])\\\n",
    "                                         .agg({'dist_to_back':['min','max','mean','std']})\\\n",
    "                                         .reset_index()\n",
    "        player_distance.columns = ['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field',\n",
    "                                   'min_dist','max_dist','mean_dist','std_dist']\n",
    "\n",
    "        return player_distance\n",
    "\n",
    "    def defense_features(df):\n",
    "        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','Y']]\n",
    "        rusher.columns = ['GameId','PlayId','RusherTeam','RusherX','RusherY']\n",
    "\n",
    "        defense = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n",
    "        defense = defense[defense['Team'] != defense['RusherTeam']][['GameId','PlayId','X','Y','RusherX','RusherY']]\n",
    "        defense['def_dist_to_back'] = defense[['X','Y','RusherX','RusherY']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n",
    "\n",
    "        defense = defense.groupby(['GameId','PlayId'])\\\n",
    "                         .agg({'def_dist_to_back':['min','max','mean','std']})\\\n",
    "                         .reset_index()\n",
    "        defense.columns = ['GameId','PlayId','def_min_dist','def_max_dist','def_mean_dist','def_std_dist']\n",
    "\n",
    "        return defense\n",
    "\n",
    "    def static_features(df):\n",
    "        \n",
    "        \n",
    "        add_new_feas = []\n",
    "\n",
    "        ## Height\n",
    "        df['PlayerHeight_dense'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n",
    "        \n",
    "        add_new_feas.append('PlayerHeight_dense')\n",
    "\n",
    "        ## Time\n",
    "        df['TimeHandoff'] = df['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "        df['TimeSnap'] = df['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "\n",
    "        df['TimeDelta'] = df.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n",
    "        df['PlayerBirthDate'] =df['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m/%d/%Y\"))\n",
    "\n",
    "        ## Age\n",
    "        seconds_in_year = 60*60*24*365.25\n",
    "        df['PlayerAge'] = df.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()/seconds_in_year, axis=1)\n",
    "        add_new_feas.append('PlayerAge')\n",
    "\n",
    "        ## WindSpeed\n",
    "        df['WindSpeed_ob'] = df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n",
    "        df['WindSpeed_ob'] = df['WindSpeed_ob'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n",
    "        df['WindSpeed_ob'] = df['WindSpeed_ob'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n",
    "        df['WindSpeed_dense'] = df['WindSpeed_ob'].apply(strtofloat)\n",
    "        add_new_feas.append('WindSpeed_dense')\n",
    "\n",
    "        ## Weather\n",
    "        df['GameWeather_process'] = df['GameWeather'].str.lower()\n",
    "        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: \"indoor\" if not pd.isna(x) and \"indoor\" in x else x)\n",
    "        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\n",
    "        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\n",
    "        df['GameWeather_process'] = df['GameWeather_process'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n",
    "        df['GameWeather_dense'] = df['GameWeather_process'].apply(map_weather)\n",
    "        add_new_feas.append('GameWeather_dense')\n",
    "        \n",
    "#         ## Rusher\n",
    "#         train['IsRusher'] = (train['NflId'] == train['NflIdRusher'])\n",
    "#         train['IsRusher_ob'] = (train['NflId'] == train['NflIdRusher']).astype(\"object\")\n",
    "#         temp = train[train[\"IsRusher\"]][[\"Team\", \"PlayId\"]].rename(columns={\"Team\":\"RusherTeam\"})\n",
    "#         train = train.merge(temp, on = \"PlayId\")\n",
    "#         train[\"IsRusherTeam\"] = train[\"Team\"] == train[\"RusherTeam\"]\n",
    "\n",
    "        ## dense -> categorical\n",
    "#         train[\"Quarter_ob\"] = train[\"Quarter\"].astype(\"object\")\n",
    "#         train[\"Down_ob\"] = train[\"Down\"].astype(\"object\")\n",
    "#         train[\"JerseyNumber_ob\"] = train[\"JerseyNumber\"].astype(\"object\")\n",
    "#         train[\"YardLine_ob\"] = train[\"YardLine\"].astype(\"object\")\n",
    "        # train[\"DefendersInTheBox_ob\"] = train[\"DefendersInTheBox\"].astype(\"object\")\n",
    "        # train[\"Week_ob\"] = train[\"Week\"].astype(\"object\")\n",
    "        # train[\"TimeDelta_ob\"] = train[\"TimeDelta\"].astype(\"object\")\n",
    "\n",
    "\n",
    "        ## Orientation and Dir\n",
    "        df[\"Orientation_ob\"] = df[\"Orientation\"].apply(lambda x : orientation_to_cat(x)).astype(\"object\")\n",
    "        df[\"Dir_ob\"] = df[\"Dir\"].apply(lambda x : orientation_to_cat(x)).astype(\"object\")\n",
    "\n",
    "        df[\"Orientation_sin\"] = df[\"Orientation\"].apply(lambda x : np.sin(x/360 * 2 * np.pi))\n",
    "        df[\"Orientation_cos\"] = df[\"Orientation\"].apply(lambda x : np.cos(x/360 * 2 * np.pi))\n",
    "        \n",
    "        df[\"Dir_sin\"] = df[\"Dir\"].apply(lambda x : np.sin(x/360 * 2 * np.pi))\n",
    "        df[\"Dir_cos\"] = df[\"Dir\"].apply(lambda x : np.cos(x/360 * 2 * np.pi))\n",
    "        \n",
    "        add_new_feas.append(\"Dir_sin\")\n",
    "        add_new_feas.append(\"Dir_cos\")\n",
    "\n",
    "        ## diff Score\n",
    "        df[\"diffScoreBeforePlay\"] = df[\"HomeScoreBeforePlay\"] - df[\"VisitorScoreBeforePlay\"]\n",
    "        add_new_feas.append(\"diffScoreBeforePlay\")\n",
    "    \n",
    "        static_features = df[df['NflId'] == df['NflIdRusher']][add_new_feas+['GameId','PlayId','X','Y','S','A','Dis','Orientation','Dir',\n",
    "                                                                             'YardLine','Quarter','Down','Distance','DefendersInTheBox',\n",
    "                                                                             'NflId','NflIdRusher','PossessionTeam','HomeTeamAbbr',\n",
    "                                                                             'VisitorTeamAbbr','PlayDirection','GameClock','Season','Team','FieldPosition']].drop_duplicates()\n",
    "        static_features.fillna(-999,inplace=True)\n",
    "\n",
    "        return static_features\n",
    "\n",
    "\n",
    "    def combine_features(relative_to_back, defense, static, deploy=deploy):\n",
    "        df = pd.merge(relative_to_back,defense,on=['GameId','PlayId'],how='inner')\n",
    "        df = pd.merge(df,static,on=['GameId','PlayId'],how='inner')\n",
    "\n",
    "        if not deploy:\n",
    "            df = pd.merge(df, outcomes, on=['GameId','PlayId'], how='inner')\n",
    "\n",
    "        return df\n",
    "    \n",
    "    yardline = update_yardline(df)\n",
    "    df = update_orientation(df, yardline)\n",
    "    back_feats = back_features(df)\n",
    "    rel_back = features_relative_to_back(df, back_feats)\n",
    "    def_feats = defense_features(df)\n",
    "    static_feats = static_features(df)\n",
    "    basetable = combine_features(rel_back, def_feats, static_feats, deploy=deploy)\n",
    "    \n",
    "    return basetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_02(t_):\n",
    "    t_['fe1'] = pd.Series(np.sqrt(np.absolute(np.square(t_.X.values) - np.square(t_.Y.values))))\n",
    "    t_['fe5'] = np.square(t_['S'].values) + 2 * t_['A'].values * t_['Dis'].values  # N\n",
    "    t_['fe7'] = np.arccos(np.clip(t_['X'].values / t_['Y'].values, -1, 1))  # N\n",
    "    t_['fe8'] = t_['S'].values / np.clip(t_['fe1'].values, 0.6, None)\n",
    "    radian_angle = (90 - t_['Dir']) * np.pi / 180.0\n",
    "    t_['fe10'] = np.abs(t_['S'] * np.cos(radian_angle))\n",
    "    t_['fe11'] = np.abs(t_['S'] * np.sin(radian_angle))\n",
    "    \n",
    "    #t_[\"is_rusher\"]          = 1.0*(t_[\"NflId\"] == t_[\"NflIdRusher\"])\n",
    "    #t_[\"is_home\"]            = t_[\"Team\"] == \"home\"\n",
    "    #t_[\"is_possession_team\"] = 1.0*(t_[\"PossessionTeam\"] == t_[\"HomeTeamAbbr\"]) - 1.0*(t_[\"PossessionTeam\"] == t_[\"VisitorTeamAbbr\"])\n",
    "    #t_[\"is_field_team\"]      = 1.0*(t_[\"FieldPosition\"] == t_[\"HomeTeamAbbr\"]) - 1.0*(t_[\"FieldPosition\"] == t_[\"VisitorTeamAbbr\"])\n",
    "    #t_[\"is_left\"]            = t_[\"PlayDirection\"] == \"left\"\n",
    "    \n",
    "    #t_[\"game_time\"]   = t_[\"GameClock\"].apply(get_time)\n",
    "    t_[\"old_data\"]    = t_[\"Season\"] == 2017\n",
    "    \n",
    "    t_['YardLine_std'] = 100 - t_['YardLine']\n",
    "    \n",
    "    return t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_basetable = create_features(train, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_basetable = create_features_02(train_basetable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basetable.drop(['NflId','NflIdRusher','PossessionTeam','HomeTeamAbbr','VisitorTeamAbbr','PlayDirection','GameClock','Season','Team','FieldPosition'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma copia do dataset para backup\n",
    "X = train_basetable.copy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Dataset for LOFO\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    target: string\n",
    "        Column name for target within df\n",
    "    features: list of strings\n",
    "        List of column names within df\n",
    "    feature_groups: dict, optional\n",
    "        Name, value dictionary of feature groups as numpy.darray or scipy.csr.scr_matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, target, features, feature_groups=None):\n",
    "        self.df = df.copy()\n",
    "        self.features = list(features)\n",
    "        self.feature_groups = feature_groups if feature_groups else dict()\n",
    "\n",
    "        self.num_rows = df.shape[0]\n",
    "        self.y = df[target].values\n",
    "\n",
    "        for feature_name, feature_matrix in self.feature_groups.items():\n",
    "            if not (isinstance(feature_matrix, np.ndarray) or isinstance(feature_matrix, ss.csr.csr_matrix)):\n",
    "                raise Exception(\"Data type {dtype} is not a valid type!\".format(dtype=type(feature_matrix)))\n",
    "\n",
    "            if feature_matrix.shape[0] != self.num_rows:\n",
    "                raise Exception(\"Expected {expected} rows but got {n} rows!\".format(expected=self.num_rows,\n",
    "                                                                                    n=feature_matrix.shape[0]))\n",
    "\n",
    "            if feature_name in self.features:\n",
    "                raise Exception(\"Feature group name '{name}' is the same with one of the features!\")\n",
    "\n",
    "    def getX(self, feature_to_remove, fit_params):\n",
    "        \"\"\"Get feature matrix and fit_params after removing a feature\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_to_remove : string\n",
    "            feature name to remove\n",
    "        fit_params : dict\n",
    "            fit parameters for the model\n",
    "        Returns\n",
    "        -------\n",
    "        X : numpy.darray or scipy.csr.scr_matrix\n",
    "            Feature matrix\n",
    "        fit_params: dict\n",
    "            Updated fit_params after feature removal\n",
    "        \"\"\"\n",
    "        feature_list = [feature for feature in self.features if feature != feature_to_remove]\n",
    "        concat_list = [self.df[feature_list].values]\n",
    "\n",
    "        for feature_name, feature_matrix in self.feature_groups.items():\n",
    "            if feature_name != feature_to_remove:\n",
    "                concat_list.append(feature_matrix)\n",
    "\n",
    "        fit_params = fit_params.copy()\n",
    "        if \"categorical_feature\" in fit_params:\n",
    "            cat_features = [f for f in fit_params[\"categorical_feature\"] if f != feature_to_remove]\n",
    "            fit_params[\"categorical_feature\"] = [ix for ix, f in enumerate(feature_list) if (f in cat_features)]\n",
    "\n",
    "        has_sparse = False\n",
    "        for feature_name, feature_matrix in self.feature_groups.items():\n",
    "            if feature_name != feature_to_remove and isinstance(feature_matrix, ss.csr.csr_matrix):\n",
    "                has_sparse = True\n",
    "\n",
    "        concat = np.hstack\n",
    "        if has_sparse:\n",
    "            concat = ss.hstack\n",
    "\n",
    "        return concat(concat_list), fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(importance_df, figsize=(8, 8)):\n",
    "    \"\"\"Plot feature importance\n",
    "    Parameters\n",
    "    ----------\n",
    "    importance_df : pandas dataframe\n",
    "        Output dataframe from LOFO/FLOFO get_importance\n",
    "    figsize : tuple\n",
    "    \"\"\"\n",
    "    importance_df = importance_df.copy()\n",
    "    importance_df[\"color\"] = (importance_df[\"importance_mean\"] > 0).map({True: 'g', False: 'r'})\n",
    "    importance_df.sort_values(\"importance_mean\", inplace=True)\n",
    "\n",
    "    importance_df.plot(x=\"feature\", y=\"importance_mean\", xerr=\"importance_std\",\n",
    "                       kind='barh', color=importance_df[\"color\"], figsize=figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import multiprocessing\n",
    "#from lofo.infer_defaults import infer_model\n",
    "\n",
    "\n",
    "class LOFOImportance:\n",
    "    \"\"\"\n",
    "    Leave One Feature Out Importance\n",
    "    Given a model and cross-validation scheme, calculates the feature importances.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: LOFO Dataset object\n",
    "    scoring: string or callable\n",
    "        Same as scoring in sklearn API\n",
    "    model: model (sklearn API), optional\n",
    "        Not trained model object\n",
    "    fit_params : dict, optional\n",
    "        fit parameters for the model\n",
    "    cv: int or iterable\n",
    "        Same as cv in sklearn API\n",
    "    n_jobs: int, optional\n",
    "        Number of jobs for parallel computation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, scoring, model=None, fit_params=None, cv=4, n_jobs=None):\n",
    "\n",
    "        self.fit_params = fit_params if fit_params else dict()\n",
    "        if model is None:\n",
    "            model, dataset.df, categoricals, dataset.y = infer_model(dataset.df, dataset.features, dataset.y, n_jobs)\n",
    "            self.fit_params[\"categorical_feature\"] = categoricals\n",
    "            n_jobs = 1\n",
    "\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.scoring = scoring\n",
    "        self.cv = cv\n",
    "        self.n_jobs = n_jobs\n",
    "        if self.n_jobs is not None and self.n_jobs > 1:\n",
    "            warning_str = (\"Warning: If your model is multithreaded, please initialise the number\"\n",
    "                           \"of jobs of LOFO to be equal to 1, otherwise you may experience performance issues.\")\n",
    "            warnings.warn(warning_str)\n",
    "\n",
    "    def _get_cv_score(self, feature_to_remove):\n",
    "        X, fit_params = self.dataset.getX(feature_to_remove=feature_to_remove, fit_params=self.fit_params)\n",
    "        y = self.dataset.y\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            cv_results = cross_validate(self.model, X, y, cv=self.cv, scoring=self.scoring, fit_params=fit_params)\n",
    "        return cv_results['test_score']\n",
    "\n",
    "    def _get_cv_score_parallel(self, feature, result_queue):\n",
    "        test_score = self._get_cv_score(feature_to_remove=feature)\n",
    "        result_queue.put((feature, test_score))\n",
    "        return test_score\n",
    "\n",
    "    def get_importance(self):\n",
    "        \"\"\"Run LOFO to get feature importances\n",
    "        Returns\n",
    "        -------\n",
    "        importance_df : pandas dataframe\n",
    "            Dataframe with feature names and corresponding importance mean and std (sorted by importance)\n",
    "        \"\"\"\n",
    "        base_cv_score = self._get_cv_score(feature_to_remove=None)\n",
    "        feature_list = self.dataset.features + list(self.dataset.feature_groups.keys())\n",
    "\n",
    "        if self.n_jobs is not None and self.n_jobs > 1:\n",
    "\n",
    "            pool = multiprocessing.Pool(self.n_jobs)\n",
    "            manager = multiprocessing.Manager()\n",
    "            result_queue = manager.Queue()\n",
    "\n",
    "            for f in feature_list:\n",
    "                pool.apply_async(self._get_cv_score_parallel, (f, result_queue))\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            lofo_cv_result = [result_queue.get() for _ in range(len(feature_list))]\n",
    "            lofo_cv_scores_normalized = np.array([base_cv_score - lofo_cv_score for _, lofo_cv_score in lofo_cv_result])\n",
    "            feature_list = [feature for feature, _ in lofo_cv_result]\n",
    "        else:\n",
    "            lofo_cv_scores = []\n",
    "            for f in tqdm_notebook(feature_list):\n",
    "                lofo_cv_scores.append(self._get_cv_score(feature_to_remove=f))\n",
    "\n",
    "            lofo_cv_scores_normalized = np.array([base_cv_score - lofo_cv_score for lofo_cv_score in lofo_cv_scores])\n",
    "\n",
    "        importance_df = pd.DataFrame()\n",
    "        importance_df[\"feature\"] = feature_list\n",
    "        importance_df[\"importance_mean\"] = lofo_cv_scores_normalized.mean(axis=1)\n",
    "        importance_df[\"importance_std\"] = lofo_cv_scores_normalized.std(axis=1)\n",
    "\n",
    "        return importance_df.sort_values(\"importance_mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedimento para verificar as features mais importantes\n",
    "# Usando LightGBM para treinamento\n",
    "#from lofo import LOFOImportance, Dataset, plot_importance\n",
    "\n",
    "n_folds = 5\n",
    "kfold_lgb = KFold(n_folds, shuffle=True)\n",
    "\n",
    "features = [x for x in X.columns if x not in ['Yards','GameId','PlayId']]\n",
    "\n",
    "params2 = {'num_leaves': 15,\n",
    "          'objective': 'mae',\n",
    "          #'learning_rate': 0.1,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"num_rounds\": 150\n",
    "          }\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(**params2)\n",
    "dataset = Dataset(df=X, target=\"Yards\", features=features)\n",
    "lofo_imp = LOFOImportance(dataset, model=model_lgb, cv=kfold_lgb, scoring=\"neg_mean_absolute_error\")\n",
    "\n",
    "importance_df = lofo_imp.get_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exibindo grafico com as features\n",
    "plot_importance(importance_df, figsize=(12, 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = importance_df.loc[importance_df['importance_mean'] > 0].feature\n",
    "best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Criar e avaliar alguns algoritmos de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Split Treino e Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um dataset somente com as colunas mais importantes conforme visto anteriormente\n",
    "new_X = X.loc[:,best_features]\n",
    "#new_X = X.drop(['GameId','PlayId','Yards'], axis=1)\n",
    "target = X.Yards\n",
    "\n",
    "y = np.zeros((target.shape[0], 199))\n",
    "for idx, target in enumerate(list(target)):\n",
    "    y[idx][99 + target] = 1\n",
    "    \n",
    "# Normalizando as variaveis do dataset de treino\n",
    "scaler = StandardScaler()\n",
    "new_X = scaler.fit_transform(new_X)\n",
    "new_X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Teste com Keras (New NN Struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRPSCallback(Callback):\n",
    "    \n",
    "    def __init__(self,validation, predict_batch_size=20, include_on_batch=False):\n",
    "        super(CRPSCallback, self).__init__()\n",
    "        self.validation = validation\n",
    "        self.predict_batch_size = predict_batch_size\n",
    "        self.include_on_batch = include_on_batch\n",
    "        \n",
    "        print('validation shape',len(self.validation))\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not ('CRPS_score_val' in self.params['metrics']):\n",
    "            self.params['metrics'].append('CRPS_score_val')\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if (self.include_on_batch):\n",
    "            logs['CRPS_score_val'] = float('-inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['CRPS_score_val'] = float('-inf')\n",
    "            \n",
    "        if (self.validation):\n",
    "            X_valid, y_valid = self.validation[0], self.validation[1]\n",
    "            y_pred = self.model.predict(X_valid)\n",
    "            y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n",
    "            y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
    "            val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * X_valid.shape[0])\n",
    "            val_s = np.round(val_s, 6)\n",
    "            logs['CRPS_score_val'] = val_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(x_tr,y_tr,x_val,y_val):\n",
    "    inp = Input(shape = (x_tr.shape[1],))\n",
    "    x = Dense(1024, input_dim=X.shape[1], activation='relu')(inp)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    out = Dense(199, activation='softmax')(x)\n",
    "    model = Model(inp,out)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[])\n",
    "    \n",
    "    #add lookahead\n",
    "#     lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n",
    "#     lookahead.inject(model) # add into model\n",
    "\n",
    "    \n",
    "    es = EarlyStopping(monitor='CRPS_score_val', \n",
    "                       mode='min',\n",
    "                       restore_best_weights=True, \n",
    "                       verbose=1, \n",
    "                       patience=10)\n",
    "\n",
    "    mc = ModelCheckpoint('best_model.h5',monitor='CRPS_score_val',mode='min',save_best_only=True, verbose=1, save_weights_only=True)\n",
    "    \n",
    "    bsz = 1024\n",
    "    steps = x_tr.shape[0]/bsz\n",
    "    \n",
    "    model.fit(x_tr, y_tr,callbacks=[CRPSCallback(validation = (x_val,y_val)),es,mc], epochs=100, batch_size=bsz,verbose=1)\n",
    "    model.load_weights(\"best_model.h5\")\n",
    "    \n",
    "    y_pred = model.predict(x_val)\n",
    "    y_valid = y_val\n",
    "    y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n",
    "    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
    "    val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * x_val.shape[0])\n",
    "    crps = np.round(val_s, 6)\n",
    "\n",
    "    return model,crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_te):\n",
    "    model_num = len(models)\n",
    "    for k,m in enumerate(models):\n",
    "        if k==0:\n",
    "            y_pred = m.predict(x_te,batch_size=1024)\n",
    "        else:\n",
    "            y_pred+=m.predict(x_te,batch_size=1024)\n",
    "            \n",
    "    y_pred = y_pred / model_num\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "models = []\n",
    "crps_csv = []\n",
    "\n",
    "s_time = time.time()\n",
    "\n",
    "for k in range(2):\n",
    "    kfold = KFold(10, random_state = 42 + k, shuffle = True)\n",
    "    for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(y)):\n",
    "        print(\"-----------\")\n",
    "        print(\"-----------\")\n",
    "        tr_x,tr_y = new_X[tr_inds],y[tr_inds]\n",
    "        val_x,val_y = new_X[val_inds],y[val_inds]\n",
    "        model,crps = get_model(tr_x,tr_y,val_x,val_y)\n",
    "        models.append(model)\n",
    "        print(\"the %d fold crps is %f\"%((k_fold+1),crps))\n",
    "        crps_csv.append(crps)\n",
    "\n",
    "#print(\"mean crps is %f\"%np.mean(crps_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean crps is %f\"%np.mean(crps_csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Realizar a submissão para o Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if  TRAIN_OFFLINE==False:\n",
    "    \n",
    "    from kaggle.competitions import nflrush\n",
    "    env = nflrush.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    df_prev = pd.DataFrame()\n",
    "    df_test = pd.DataFrame()\n",
    "\n",
    "    for (test_df, sample_prediction_df) in tqdm_notebook(iter_test):\n",
    "        basetable = create_features(test_df, deploy=True)\n",
    "        basetable = create_features_02(basetable)\n",
    "        basetable.drop(['GameId','PlayId','NflId','NflIdRusher','PossessionTeam','HomeTeamAbbr','VisitorTeamAbbr','PlayDirection','GameClock','Season','Team','FieldPosition'], axis=1, inplace=True)\n",
    "        \n",
    "        # Considerar somente as colunas do Feature Selection\n",
    "        basetable = basetable.loc[:,best_features]\n",
    "    \n",
    "        scaled_basetable = scaler.transform(basetable)\n",
    "\n",
    "        y_pred = predict(scaled_basetable)\n",
    "        y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1).tolist()[0]\n",
    "\n",
    "        preds_df = pd.DataFrame(data=[y_pred], columns=sample_prediction_df.columns)\n",
    "        \n",
    "        df_test = df_test.append(basetable)\n",
    "        df_prev = df_prev.append(preds_df)\n",
    "    \n",
    "        env.predict(preds_df)\n",
    "\n",
    "    env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
